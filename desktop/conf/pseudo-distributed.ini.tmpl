#####################################
# DEVELOPMENT EDITION
#####################################

# Hue configuration file
# ===================================
#
# For complete documentation about the contents of this file, run
#       $ <hue_root>/build/env/bin/hue config_help
#
# All .ini files under the current directory are treated equally.  Their
# contents are merged to form the Hue configuration, which can
# can be viewed on the Hue at
#       http://<hue_host>:<port>/dump_config


###########################################################################
# General configuration for core Desktop features (authentication, etc)
###########################################################################

[desktop]

  send_dbug_messages=1

  # To show database transactions, set database_logging to 1
  database_logging=0

  # Set this to a random string, the longer the better.
  # This is used for secure hashing in the session store.
  secret_key=

  # Webserver listens on this address and port
  http_host=0.0.0.0
  http_port=8000

  # Time zone name
  time_zone=America/Los_Angeles

  # Turn off debug
  django_debug_mode=1

  # Turn off backtrace for server error
  http_500_debug_mode=1

  # Server email for internal error messages
  ## django_server_email='hue@localhost.localdomain'

  # Email backend
  ## django_email_backend=django.core.mail.backends.smtp.EmailBackend

  # Set to true to use CherryPy as the webserver, set to false
  # to use Spawning as the webserver. Defaults to Spawning if
  # key is not specified.
  ## use_cherrypy_server = false

  # Webserver runs as this user
  ## server_user=hue
  ## server_group=hue

  # If set to false, runcpserver will not actually start the web server.
  # Used if Apache is being used as a WSGI container.
  ## enable_server=yes

  # Number of threads used by the CherryPy web server
  ## cherrypy_server_threads=10

  # Filename of SSL Certificate
  ## ssl_certificate=

  # Filename of SSL RSA Private Key
  ## ssl_private_key=

  # List of allowed and disallowed ciphers
  ## ssl_cipher_list=DEFAULT:!aNULL:!eNULL:!LOW:!EXPORT:!SSLv2

  # Default encoding for site data
  ## default_site_encoding=utf-8

  # Help improve Hue with anonymous usage analytics.
  # Use Google Analytics to see how many times an application or specific section of an application is used, nothing more.
  ## collect_usage=true

  ## Comma-separated list of regular expressions, which match the redirect URL.
  ## For example, to restrict to your local domain and FQDN, the following value can be used:
  ##   ^\/.*$,^http:\/\/www.mydomain.com\/.*$
  # redirect_whitelist=

  # Administrators
  # ----------------
  [[django_admins]]
    ## [[[admin1]]]
    ## name=john
    ## email=john@doe.com

  # UI customizations
  # -------------------
  [[custom]]

  # Top banner HTML code
  ## banner_top_html=

  # Configuration options for user authentication into the web application
  # ------------------------------------------------------------------------
  [[auth]]

    # Authentication backend. Common settings are:
    # - django.contrib.auth.backends.ModelBackend (entirely Django backend)
    # - desktop.auth.backend.AllowAllBackend (allows everyone)
    # - desktop.auth.backend.AllowFirstUserDjangoBackend
    #     (Default. Relies on Django and user manager, after the first login)
    # - desktop.auth.backend.LdapBackend
    # - desktop.auth.backend.PamBackend
    # - desktop.auth.backend.SpnegoDjangoBackend
    # - desktop.auth.backend.RemoteUserDjangoBackend
    # - desktop.auth.backend.OAuthBackend
    # - libsaml.backend.SAML2Backend
    ## backend=desktop.auth.backend.AllowFirstUserDjangoBackend

    # Backend to synchronize user-group membership with
    ## user_group_membership_synchronization_backend=desktop.auth.backend.LdapSynchronizationBackend

    ## pam_service=login

    # When using the desktop.auth.backend.RemoteUserDjangoBackend, this sets
    # the normalized name of the header that contains the remote user.
    # The HTTP header in the request is converted to a key by converting
    # all characters to uppercase, replacing any hyphens with underscores
    # and adding an HTTP_ prefix to the name. So, for example, if the header
    # is called Remote-User that would be configured as HTTP_REMOTE_USER
    #
    # Defaults to HTTP_REMOTE_USER
    ## remote_user_header=HTTP_REMOTE_USER

    # Ignore the case of usernames when searching for existing users.
    # Only supported in remoteUserDjangoBackend.
    ## ignore_username_case=false

    # Ignore the case of usernames when searching for existing users to authenticate with.
    # Only supported in remoteUserDjangoBackend.
    ## force_username_lowercase=false

  # Configuration options for connecting to LDAP and Active Directory
  # -------------------------------------------------------------------
  [[ldap]]

    # The search base for finding users and groups
    ## base_dn="DC=mycompany,DC=com"

    # The NT domain to connect to (only for use with Active Directory)
    ## nt_domain=mycompany.com

    # URL of the LDAP server
    ## ldap_url=ldap://auth.mycompany.com

    # A PEM-format file containing certificates for the CA's that
    # Hue will trust for authentication over TLS.
    # The certificate for the CA that signed the
    # LDAP server certificate must be included among these certificates.
    # See more here http://www.openldap.org/doc/admin24/tls.html.
    ## ldap_cert=
    ## use_start_tls=true

    # Distinguished name of the user to bind as -- not necessary if the LDAP server
    # supports anonymous searches
    ## bind_dn="CN=ServiceAccount,DC=mycompany,DC=com"

    # Password of the bind user -- not necessary if the LDAP server supports
    # anonymous searches
    ## bind_password=

    # Pattern for searching for usernames -- Use <username> for the parameter
    # For use when using LdapBackend for Hue authentication
    ## ldap_username_pattern="uid=<username>,ou=People,dc=mycompany,dc=com"

    # Create users in Hue when they try to login with their LDAP credentials
    # For use when using LdapBackend for Hue authentication
    ## create_users_on_login = true

    # Ignore the case of usernames when searching for existing users in Hue.
    ## ignore_username_case=false

    # Force usernames to lowercase when creating new users from LDAP.
    ## force_username_lowercase=false

    # Use search bind authentication.
    ## search_bind_authentication=true

    # Choose which kind of subgrouping to use: nested or suboordinate (deprecated).
    ## subgroups=suboordinate

    # Define the number of levels to search for nested members.
    ## nested_members_search_depth=10

    [[[users]]]

      # Base filter for searching for users
      ## user_filter="objectclass=*"

      # The username attribute in the LDAP schema
      ## user_name_attr=sAMAccountName

      [[[groups]]]

      # Base filter for searching for groups
      ## group_filter="objectclass=*"

      # The username attribute in the LDAP schema
      ## group_name_attr=cn

  # Configuration options for specifying the Desktop Database.  For more info,
  # see http://docs.djangoproject.com/en/1.1/ref/settings/#database-engine
  # ------------------------------------------------------------------------
  [[database]]
    # Database engine is typically one of:
    # postgresql_psycopg2, mysql, or sqlite3
    #
    # Note that for sqlite3, 'name', below is a filename;
    # for other backends, it is the database name.
    ## engine=sqlite3
    ## host=
    ## port=
    ## user=
    ## password=
    ## name=desktop/desktop.db
    ## options={}


  # Configuration options for specifying the Desktop session.
  # For more info, see https://docs.djangoproject.com/en/1.4/topics/http/sessions/
  # ------------------------------------------------------------------------
  [[session]]
    # The cookie containing the users' session ID will expire after this amount of time in seconds.
    ## ttl=60*60*24*14

    # The cookie containing the users' session ID will be secure.
    # Should only be enabled with HTTPS.
    ## secure=false

  # Configuration options for connecting to an external SMTP server
  # ------------------------------------------------------------------------
  [[smtp]]

    # The SMTP server information for email notification delivery
    host=localhost
    port=25
    user=
    password=

    # Whether to use a TLS (secure) connection when talking to the SMTP server
    tls=no

    # Default email address to use for various automated notification from Hue
    ## default_from_email=hue@localhost


  # Configuration options for Kerberos integration for secured Hadoop clusters
  # ------------------------------------------------------------------------
  [[kerberos]]

    # Path to Hue's Kerberos keytab file
    ## hue_keytab=
    # Kerberos principal name for Hue
    ## hue_principal=hue/hostname.foo.com
    # Path to kinit
    ## kinit_path=/path/to/kinit


  # Configuration options for using OAuthBackend login
  # ------------------------------------------------------------------------
  [[oauth]]
    # The Consumer key of the application
    ## consumer_key=XXXXXXXXXXXXXXXXXXXXX

    # The Consumer secret of the application
    ## consumer_secret=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

    # The Request token URL
    ## request_token_url=https://api.twitter.com/oauth/request_token

    # The Access token URL
    ## access_token_url=https://api.twitter.com/oauth/access_token

    # The Authorize URL
    ## authenticate_url=https://api.twitter.com/oauth/authorize


###########################################################################
# Settings to configure SAML
###########################################################################

[libsaml]
  # Xmlsec1 binary path. This program should be executable by the user running Hue.
  ## xmlsec_binary=/usr/local/bin/xmlsec1

  # Entity ID for Hue acting as service provider.
  # Can also accept a pattern where '<base_url>' will be replaced with server URL base.
  ## entity_id="<base_url>/saml2/metadata/"

  # Create users from SSO on login.
  ## create_users_on_login=true

  # Required attributes to ask for from IdP.
  # This requires a comma separated list.
  ## required_attributes=uid

  # Optional attributes to ask for from IdP.
  # This requires a comma separated list.
  ## optional_attributes=

  # IdP metadata in the form of a file. This is generally an XML file containing metadata that the Identity Provider generates.
  ## metadata_file=

  # Private key to encrypt metadata with.
  ## key_file=

  # Signed certificate to send along with encrypted metadata.
  ## cert_file=

  # A mapping from attributes in the response from the IdP to django user attributes.
  ## user_attribute_mapping={'uid':'username'}

  # Have Hue initiated authn requests be signed and provide a certificate.
  ## authn_requests_signed=false
  
  # Have Hue initiated logout requests be signed and provide a certificate.
  ## logout_requests_signed=false

  # Username can be sourced from 'attributes' or 'nameid'.
  ## username_source=attributes

  # Performs the logout or not.
  ## logout_enabled=true


###########################################################################
# Settings to configure your Hadoop cluster.
###########################################################################

[hadoop]

  # Configuration for HDFS NameNode
  # ------------------------------------------------------------------------
  [[hdfs_clusters]]
    # HA support by using HttpFs

    [[[default]]]
      # Enter the filesystem uri
      fs_defaultfs=hdfs://localhost:8020

      # Use WebHdfs/HttpFs as the communication mechanism. To fallback to
      # using the Thrift plugin (used in Hue 1.x), this must be uncommented
      # and explicitly set to the empty value.
      # NameNode logical name.
      ## logical_name=

      # Use WebHdfs/HttpFs as the communication mechanism.
      # This should be the web service root URL, such as
      # http://namenode:50070/webhdfs/v1
      ## webhdfs_url=

      ## security_enabled=false

      # Settings about this HDFS cluster. If you install HDFS in a
      # different location, you need to set the following.

      # Defaults to $HADOOP_HDFS_HOME or /usr/lib/hadoop-hdfs
      ## hadoop_hdfs_home=/usr/lib/hadoop-hdfs

      # Defaults to $HADOOP_BIN or /usr/bin/hadoop
      ## hadoop_bin=/usr/bin/hadoop

      # Defaults to $HADOOP_CONF_DIR or /etc/hadoop/conf
      ## hadoop_conf_dir=/etc/hadoop/conf

  # Configuration for MapReduce 0.20 JobTracker (MR1)
  # ------------------------------------------------------------------------
  [[mapred_clusters]]

    [[[default]]]
      # Enter the host on which you are running the Hadoop JobTracker
      jobtracker_host=localhost
      # The port where the JobTracker IPC listens on
      jobtracker_port=8021
      # Thrift plug-in port for the JobTracker
      ## thrift_port=9290
      # Whether to submit jobs to this cluster
      ## submit_to=True

      # NameNode logical name.
      ## logical_name=

      # Change this if your MapReduce cluster is Kerberos-secured
      ## security_enabled=false

      # Settings about this MR1 cluster. If you install MR1 in a
      # different location, you need to set the following.

      # Defaults to $HADOOP_MR1_HOME or /usr/lib/hadoop-0.20-mapreduce
      ## hadoop_mapred_home=/usr/lib/hadoop-0.20-mapreduce

      # Defaults to $HADOOP_BIN or /usr/bin/hadoop
      ## hadoop_bin=/usr/bin/hadoop

      # Defaults to $HADOOP_CONF_DIR or /etc/hadoop/conf
      ## hadoop_conf_dir=/etc/hadoop/conf

    # HA support by specifying multiple clusters
    # e.g.

    # [[[ha]]]
      # Enter the host on which you are running the failover JobTracker
      # jobtracker_host=localhost-ha


  # Configuration for Yarn
  # ------------------------------------------------------------------------
  [[yarn_clusters]]

    [[[default]]]
      # Enter the host on which you are running the ResourceManager
      resourcemanager_host=localhost
      # The port where the ResourceManager IPC listens on
      resourcemanager_port=8032
      # Whether to submit jobs to this cluster
      ## submit_to=False

      ## security_enabled=false

      # Settings about this MR2 cluster. If you install MR2 in a
      # different location, you need to set the following.

      # Defaults to $HADOOP_MR2_HOME or /usr/lib/hadoop-mapreduce
      ## hadoop_mapred_home=/usr/lib/hadoop-mapreduce

      # Defaults to $HADOOP_BIN or /usr/bin/hadoop
      ## hadoop_bin=/usr/bin/hadoop

      # Defaults to $HADOOP_CONF_DIR or /etc/hadoop/conf
      ## hadoop_conf_dir=/etc/hadoop/conf

      # URL of the ResourceManager API
      ## resourcemanager_api_url=http://localhost:8088

      # URL of the ProxyServer API
      ## proxy_api_url=http://localhost:8088

      # URL of the HistoryServer API
      history_server_api_url=http://localhost:19888

      # URL of the NodeManager API
      node_manager_api_url=http://localhost:8042


###########################################################################
# Settings to configure liboozie
###########################################################################

[liboozie]
  # The URL where the Oozie service runs on. This is required in order for
  # users to submit jobs.
  ## oozie_url=http://localhost:11000/oozie

  # Requires FQDN in oozie_url if enabled
  ## security_enabled=false

  # Location on HDFS where the workflows/coordinator are deployed when submitted.
  ## remote_deployement_dir=/user/hue/oozie/deployments


###########################################################################
# Settings to configure the Oozie app
###########################################################################

[oozie]
  # Location on local FS where the examples are stored.
  ## local_data_dir=..../examples

  # Location on local FS where the data for the examples is stored.
  ## sample_data_dir=...thirdparty/sample_data

  # Location on HDFS where the oozie examples and workflows are stored.
  ## remote_data_dir=/user/hue/oozie/workspaces

  # Share workflows and coordinators information with all users. If set to false,
  # they will be visible only to the owner and administrators.
  ## share_jobs=True

  # Maximum of Oozie workflows or coodinators to retrieve in one API call.
  ## oozie_jobs_count=100


###########################################################################
# Settings to configure Beeswax
###########################################################################

[beeswax]

  # Host where Beeswax server Thrift daemon is running.
  # If Kerberos security is enabled, the fully-qualified domain name (FQDN) is
  # required, even if the Thrift daemon is running on the same host as Hue.
  ## beeswax_server_host=<FQDN of Beeswax Server>

  # The type of Thrift interface used for contacting the backend for sending
  # queries/metadata requests.
  # Choices are 'beeswax' (default), 'hiveserver2'.
  ## server_interface=beeswax

  # Port where Beeswax Thrift server runs on.
  # Use 10000 when using the HiveServer2 interface.
  ## beeswax_server_port=8002

  # Host where internal metastore Thrift daemon is running.
  ## beeswax_meta_server_host=localhost

  # Configure the port the internal metastore daemon runs on.
  # Used only if hive.metastore.local is true.
  ## beeswax_meta_server_port=8003

  # Hive home directory
  ## hive_home_dir=/usr/lib/hive

  # Hive configuration directory, where hive-site.xml is located
  ## hive_conf_dir=/etc/hive/conf

  # Timeout in seconds for thrift calls to beeswax service
  ## beeswax_server_conn_timeout=120

  # Timeout in seconds for thrift calls to the hive metastore
  ## metastore_conn_timeout=10

  # Maximum Java heapsize (in megabytes) used by Beeswax Server.
  # Note that the setting of HADOOP_HEAPSIZE in $HADOOP_CONF_DIR/hadoop-env.sh
  # may override this setting.
  ## beeswax_server_heapsize=1000

  # Share saved queries with all users. If set to false, saved queries are
  # visible only to the owner and administrators.
  ## share_saved_queries=true

  # Time in milliseconds for Beeswax to persist queries in its cache.
  # 7*24*60*60*1000 = 1 week
  ## beeswax_running_query_lifetime=604800000L

  # Set a LIMIT clause when browsing a partitioned table.
  # A positive value will be set as the LIMIT. If 0 or negative, do not set any limit.
  ## browse_partitioned_table_limit=250


###########################################################################
# Settings to configure Pig
###########################################################################

[pig]
  # Location of piggybank.jar on local filesystem.
  ## local_sample_dir=/usr/share/hue/apps/pig/examples

  # Location piggybank.jar will be copied to in HDFS.
  ## remote_data_dir=/user/hue/pig/examples


###########################################################################
# Settings to configure Sqoop
###########################################################################

[sqoop]
  # Sqoop server URL
  ## server_url=http://localhost:12000/sqoop


###########################################################################
# Settings to configure Proxy
###########################################################################

[proxy]
  # Comma-separated list of regular expressions,
  # which match 'host:port' of requested proxy target.
  ## whitelist=(localhost|127\.0\.0\.1):(50030|50070|50060|50075)

  # Comma-separated list of regular expressions,
  # which match any prefix of 'host:port/path' of requested proxy target.
  # This does not support matching GET parameters.
  ## blacklist=()


###########################################################################
# Settings to configure Impala
###########################################################################

[impala]
  # Host of the Impala Server (one of the Impalad)
  ## server_host=localhost

  # The backend to contact for queries/metadata requests.
  # Choices are 'beeswax' or 'hiveserver2' (default).
  # 'hiveserver2' supports log, progress information, query cancelation
  # 'beeswax' requires Beeswax to run for proxying the metadata requests
  ## server_interface=hiveserver2

  # Port of the Impala Server
  # Default is 21050 as HiveServer2 Thrift interface is the default.
  # Use 21000 when using Beeswax Thrift interface.
  ## server_port=21050

  # Kerberos principal
  ## impala_principal=impala/hostname.foo.com

  # Turn on/off impersonation mechanism when talking to Impala
  ## impersonation_enabled=False


###########################################################################
# Settings to configure Hbase
###########################################################################

[hbase]
  # Comma-separated list of HBase Thrift servers for
  # clusters in the format of '(name|host:port)'.
  ## hbase_clusters=(Cluster|localhost:9090)

  # Hard limit of rows or columns per row fetched before truncating.
  ## truncate_limit = 500


###########################################################################
# Settings to configure Solr Search
###########################################################################

[search]

  # URL of the Solr Server
  ## solr_url=http://localhost:8983/solr/

  # Requires FQDN in solr_url if enabled
  ## security_enabled=false

  ## Query sent when no term is entered
  ## empty_query=*:*


###########################################################################
# Settings to configure Job Designer
###########################################################################

[jobsub]
  # Location on HDFS where the jobsub examples and templates are stored.
  ## remote_data_dir=/user/hue/jobsub

  # Location on local FS where examples and template are stored.
  ## local_data_dir=..../data

  # Location on local FS where sample data is stored
  ## sample_data_dir=...thirdparty/sample_data


###########################################################################
# Settings to configure Job Browser
###########################################################################

[jobbrowser]
  # Share submitted jobs information with all users. If set to false,
  # submitted jobs are visible only to the owner and administrators.
  ## share_jobs=true


###########################################################################
# Settings to configure the Shell application
###########################################################################

[shell]
  # The shell_buffer_amount specifies the number of bytes of output per shell
  # that the Shell app will keep in memory. If not specified, it defaults to
  # 524288 (512 MiB).
  ## shell_buffer_amount=100

  # If you run Hue against a Hadoop cluster with Kerberos security enabled, the
  # Shell app needs to acquire delegation tokens for the subprocesses to work
  # correctly. These delegation tokens are stored as temporary files in some
  # directory. You can configure this directory here. If not specified, it
  # defaults to /tmp/hue_delegation_tokens.
  ## shell_delegation_token_dir=/tmp/hue_delegation_tokens

  [[ shelltypes ]]

    # Define and configure a new shell type "pig"
    # ------------------------------------------------------------------------
    [[[ pig ]]]
      nice_name = "Pig Shell (Grunt)"
      command = "/usr/bin/pig -l /dev/null"
      help = "The command-line interpreter for Pig"

      [[[[ environment ]]]]
        # You can specify environment variables for the Pig shell
        # in this section. Note that JAVA_HOME must be configured
        # for the Pig shell to run.

        [[[[[ JAVA_HOME ]]]]]
          value = "/usr/lib/jvm/java-6-sun"

    # Define and configure a new shell type "hbase"
    # ------------------------------------------------------------------------
    [[[ hbase ]]]
      nice_name = "HBase Shell"
      command = "/usr/bin/hbase shell"
      help = "The command-line HBase client interface."

      [[[[ environment ]]]]
        # You can configure environment variables for the HBase shell
        # in this section.

    # Define and configure a new shell type "Sqoop 2"
    # ------------------------------------------------------------------------
    [[[ sqoop2 ]]]
      nice_name = "Sqoop 2 Shell"
      command = "/usr/bin/sqoop2"
      help = "The command-line Sqoop 2 client."

      [[[[ environment ]]]]
        # You can configure environment variables for the Sqoop 2 shell
        # in this section.

    # Define and configure a new shell type "bash" for testing only
    # ------------------------------------------------------------------------
    [[[ bash ]]]
      nice_name = "Bash (Test only!!!)"
      command = "/bin/bash"
      help = "A shell that does not depend on Hadoop components"


###########################################################################
# Settings for the User Admin application
###########################################################################

[useradmin]
  # The name of the default user group that users will be a member of
  ## default_user_group=default
