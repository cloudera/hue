/**
 * Autogenerated by Thrift
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 */
package org.apache.hadoop.thriftfs.api;

import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.thrift.*;
import org.apache.thrift.async.*;
import org.apache.thrift.meta_data.*;
import org.apache.thrift.transport.*;
import org.apache.thrift.protocol.*;

public class Namenode {

  /**
   * Provides an interface to a Hadoop Namenode. It is basically a Thrift
   * translation of org.apache.hadoop.hdfs.protocol.ClientProtocol.
   */
  public interface Iface extends org.apache.hadoop.thriftfs.api.HadoopServiceBase.Iface {

    /**
     * Set permissions of an existing file or directory.
     * 
     * @param ctx
     * @param path Path of the file or directory.
     * 
     * @param perms New permissions for the file or directory.
     */
    public void chmod(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Set owner of a file or directory.
     * 
     * If either parameter 'owner' or 'group' is set to null, that
     * parameter is left unchanged.
     * 
     * Parameters 'owner' and 'group' cannot be both null.
     * 
     * @param ctx
     * @param path Path to the file or directory
     * 
     * @param owner New owner.
     * 
     * @param group New group.
     */
    public void chown(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String owner, String group) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Return a list containing:
     *   (index 0) The total storage capacity of the file system (in bytes).
     *   (index 1) The total used space of the file system (in bytes).
     *   (index 2) The available storage of the file system (in bytes).
     * 
     * @param ctx
     */
    public List<Long> df(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException;

    /**
     * Enter safe mode.
     * 
     * @param ctx
     */
    public void enterSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Get a list of all blocks containing a region of a file
     * 
     * @param ctx
     * @param path Path to the file.
     * 
     * @param offset Offset of the region.
     * 
     * @param length Length of the region
     */
    public List<Block> getBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long offset, long length) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Get the preferred block size for the given file.
     * 
     * The path must exist, or common.IOException is thrown.
     * 
     * @param ctx
     * @param path Path to the file.
     */
    public long getPreferredBlockSize(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Returns whether HDFS is in safe mode or not.
     * 
     * @param ctx
     */
    public boolean isInSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Leave safe mode.
     * 
     * @param ctx
     */
    public void leaveSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Get a listing of the indicated directory.
     * 
     * @param ctx
     * @param path Path to the directory.
     */
    public List<Stat> ls(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Create a directory (or hierarchy of directories).
     * 
     * Returns false if directory did not exist and could not be created,
     * true otherwise.
     * 
     * @param ctx
     * @param path Path to the directory.
     * 
     * @param perms Access permissions of the directory.
     */
    public boolean mkdirhier(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Tells the name node to reread the hosts and exclude files.
     * 
     * @param ctx
     */
    public void refreshNodes(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Rename an item in the file system namespace.
     * 
     * Returns true  if successful, or
     *         false if the old name does not exist or if the new name already
     *               belongs to the namespace.
     * 
     * @param ctx
     * @param path Path to existing file or directory.
     * 
     * @param newPath New path.
     */
    public boolean rename(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String newPath) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Report corrupted blocks.
     * 
     * @param ctx
     * @param blocks List of corrupted blocks.
     */
    public void reportBadBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<Block> blocks) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Get information about a path in HDFS.
     * 
     * Return value will be nul if path does not exist.
     * 
     * @param ctx
     * @param path Path of the file or directory.
     */
    public Stat stat(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Get the summary of a directory's contents.
     * 
     * Note that this has runtime linear in the total number of nodes
     * in the directory tree - this can be expensive for directories
     * near the top of a big HDFS. Use with care.
     * 
     * @param ctx
     * @param Path
     */
    public ContentSummary getContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, String Path) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Get ContentSummary objects for multiple directories simultaneously. The same warnings
     * apply as for getContentSummary(...) above.
     * 
     * @param ctx
     * @param paths
     */
    public List<ContentSummary> multiGetContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<String> paths) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Set the quota for a directory.
     * 
     * Quota parameters may have three types of values:
     * 
     *    (1) 0 or more:      Quota will be set to that value.
     *    (2) QUOTA_DONT_SET: Quota will not be changed,
     *    (3) QUOTA_RESET:    Quota will be reset.
     * 
     * Any other value is a runtime error.
     * 
     * @param ctx
     * @param path Path of the directory.
     * 
     * @param namespaceQuota Limit on the number of names in the directory.
     * 
     * @param diskspaceQuota Limit on disk space occupied by all the files in the
     * directory.
     */
    public void setQuota(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long namespaceQuota, long diskspaceQuota) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Set replication factor for an existing file.
     * 
     * This call just updates the value of the replication factor. The actual
     * block replication is not expected to be performed during this method call.
     * The blocks will be populated or removed in the background as the result of
     * the routine block maintenance procedures.
     * 
     * Returns true if successful, false if file does not exist or is a
     * directory.
     * 
     * @param ctx
     * @param path Path of the file.
     * 
     * @param replication New replication factor.
     */
    public boolean setReplication(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short replication) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Delete a file or directory from the file system.
     * 
     * Any blocks belonging to the deleted files will be garbage-collected.
     * 
     * @param ctx
     * @param path Path of the file or directory.
     * 
     * @param recursive Delete a non-empty directory recursively.
     */
    public boolean unlink(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, boolean recursive) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Sets the modification and access time of a file or directory.
     * 
     * Setting *one single time paramater* to -1 means that time parameter
     * must not be set by this call.
     * 
     * Setting *both time parameters* to -1 means both of them must be set to
     * the current time.
     * 
     * @param ctx
     * @param path Path of the file or directory.
     * 
     * @param atime Access time in milliseconds since 1970-01-01 00:00 UTC
     * 
     * @param mtime Modification time in milliseconds since 1970-01-01 00:00 UTC
     */
    public void utime(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long atime, long mtime) throws org.apache.hadoop.thriftfs.api.IOException, TException;

    /**
     * Inform the namenode that a datanode process has started.
     * 
     * @param name <host name>:<port number> of the datanode
     * 
     * @param storage the storage id of the datanode
     * 
     * @param thriftPort Thrift port of the datanode
     */
    public void datanodeUp(String name, String storage, int thriftPort) throws TException;

    /**
     * Inform the namenode that a datanode process has stopped.
     * 
     * @param name <host name>:<port number> of the datanode
     * 
     * @param storage the storage id of the datanode
     * 
     * @param thriftPort Thrift port of the datanode
     */
    public void datanodeDown(String name, String storage, int thriftPort) throws TException;

    /**
     * Get an HDFS delegation token.
     * 
     * @param ctx
     * @param renewer
     */
    public org.apache.hadoop.thriftfs.api.ThriftDelegationToken getDelegationToken(org.apache.hadoop.thriftfs.api.RequestContext ctx, String renewer) throws org.apache.hadoop.thriftfs.api.IOException, TException;

  }

  public interface AsyncIface extends org.apache.hadoop.thriftfs.api.HadoopServiceBase .AsyncIface {

    public void chmod(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms, AsyncMethodCallback<AsyncClient.chmod_call> resultHandler) throws TException;

    public void chown(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String owner, String group, AsyncMethodCallback<AsyncClient.chown_call> resultHandler) throws TException;

    public void df(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<AsyncClient.df_call> resultHandler) throws TException;

    public void enterSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<AsyncClient.enterSafeMode_call> resultHandler) throws TException;

    public void getBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long offset, long length, AsyncMethodCallback<AsyncClient.getBlocks_call> resultHandler) throws TException;

    public void getPreferredBlockSize(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<AsyncClient.getPreferredBlockSize_call> resultHandler) throws TException;

    public void isInSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<AsyncClient.isInSafeMode_call> resultHandler) throws TException;

    public void leaveSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<AsyncClient.leaveSafeMode_call> resultHandler) throws TException;

    public void ls(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<AsyncClient.ls_call> resultHandler) throws TException;

    public void mkdirhier(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms, AsyncMethodCallback<AsyncClient.mkdirhier_call> resultHandler) throws TException;

    public void refreshNodes(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<AsyncClient.refreshNodes_call> resultHandler) throws TException;

    public void rename(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String newPath, AsyncMethodCallback<AsyncClient.rename_call> resultHandler) throws TException;

    public void reportBadBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<Block> blocks, AsyncMethodCallback<AsyncClient.reportBadBlocks_call> resultHandler) throws TException;

    public void stat(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<AsyncClient.stat_call> resultHandler) throws TException;

    public void getContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, String Path, AsyncMethodCallback<AsyncClient.getContentSummary_call> resultHandler) throws TException;

    public void multiGetContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<String> paths, AsyncMethodCallback<AsyncClient.multiGetContentSummary_call> resultHandler) throws TException;

    public void setQuota(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long namespaceQuota, long diskspaceQuota, AsyncMethodCallback<AsyncClient.setQuota_call> resultHandler) throws TException;

    public void setReplication(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short replication, AsyncMethodCallback<AsyncClient.setReplication_call> resultHandler) throws TException;

    public void unlink(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, boolean recursive, AsyncMethodCallback<AsyncClient.unlink_call> resultHandler) throws TException;

    public void utime(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long atime, long mtime, AsyncMethodCallback<AsyncClient.utime_call> resultHandler) throws TException;

    public void datanodeUp(String name, String storage, int thriftPort, AsyncMethodCallback<AsyncClient.datanodeUp_call> resultHandler) throws TException;

    public void datanodeDown(String name, String storage, int thriftPort, AsyncMethodCallback<AsyncClient.datanodeDown_call> resultHandler) throws TException;

    public void getDelegationToken(org.apache.hadoop.thriftfs.api.RequestContext ctx, String renewer, AsyncMethodCallback<AsyncClient.getDelegationToken_call> resultHandler) throws TException;

  }

  public static class Client extends org.apache.hadoop.thriftfs.api.HadoopServiceBase.Client implements TServiceClient, Iface {
    public static class Factory implements TServiceClientFactory<Client> {
      public Factory() {}
      public Client getClient(TProtocol prot) {
        return new Client(prot);
      }
      public Client getClient(TProtocol iprot, TProtocol oprot) {
        return new Client(iprot, oprot);
      }
    }

    public Client(TProtocol prot)
    {
      this(prot, prot);
    }

    public Client(TProtocol iprot, TProtocol oprot)
    {
      super(iprot, oprot);
    }

    public void chmod(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_chmod(ctx, path, perms);
      recv_chmod();
    }

    public void send_chmod(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("chmod", TMessageType.CALL, ++seqid_));
      chmod_args args = new chmod_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setPerms(perms);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_chmod() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "chmod failed: out of sequence response");
      }
      chmod_result result = new chmod_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public void chown(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String owner, String group) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_chown(ctx, path, owner, group);
      recv_chown();
    }

    public void send_chown(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String owner, String group) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("chown", TMessageType.CALL, ++seqid_));
      chown_args args = new chown_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setOwner(owner);
      args.setGroup(group);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_chown() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "chown failed: out of sequence response");
      }
      chown_result result = new chown_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public List<Long> df(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException
    {
      send_df(ctx);
      return recv_df();
    }

    public void send_df(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("df", TMessageType.CALL, ++seqid_));
      df_args args = new df_args();
      args.setCtx(ctx);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public List<Long> recv_df() throws TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "df failed: out of sequence response");
      }
      df_result result = new df_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "df failed: unknown result");
    }

    public void enterSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_enterSafeMode(ctx);
      recv_enterSafeMode();
    }

    public void send_enterSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("enterSafeMode", TMessageType.CALL, ++seqid_));
      enterSafeMode_args args = new enterSafeMode_args();
      args.setCtx(ctx);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_enterSafeMode() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "enterSafeMode failed: out of sequence response");
      }
      enterSafeMode_result result = new enterSafeMode_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public List<Block> getBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long offset, long length) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_getBlocks(ctx, path, offset, length);
      return recv_getBlocks();
    }

    public void send_getBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long offset, long length) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("getBlocks", TMessageType.CALL, ++seqid_));
      getBlocks_args args = new getBlocks_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setOffset(offset);
      args.setLength(length);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public List<Block> recv_getBlocks() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "getBlocks failed: out of sequence response");
      }
      getBlocks_result result = new getBlocks_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "getBlocks failed: unknown result");
    }

    public long getPreferredBlockSize(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_getPreferredBlockSize(ctx, path);
      return recv_getPreferredBlockSize();
    }

    public void send_getPreferredBlockSize(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("getPreferredBlockSize", TMessageType.CALL, ++seqid_));
      getPreferredBlockSize_args args = new getPreferredBlockSize_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public long recv_getPreferredBlockSize() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "getPreferredBlockSize failed: out of sequence response");
      }
      getPreferredBlockSize_result result = new getPreferredBlockSize_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "getPreferredBlockSize failed: unknown result");
    }

    public boolean isInSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_isInSafeMode(ctx);
      return recv_isInSafeMode();
    }

    public void send_isInSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("isInSafeMode", TMessageType.CALL, ++seqid_));
      isInSafeMode_args args = new isInSafeMode_args();
      args.setCtx(ctx);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public boolean recv_isInSafeMode() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "isInSafeMode failed: out of sequence response");
      }
      isInSafeMode_result result = new isInSafeMode_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "isInSafeMode failed: unknown result");
    }

    public void leaveSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_leaveSafeMode(ctx);
      recv_leaveSafeMode();
    }

    public void send_leaveSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("leaveSafeMode", TMessageType.CALL, ++seqid_));
      leaveSafeMode_args args = new leaveSafeMode_args();
      args.setCtx(ctx);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_leaveSafeMode() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "leaveSafeMode failed: out of sequence response");
      }
      leaveSafeMode_result result = new leaveSafeMode_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public List<Stat> ls(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_ls(ctx, path);
      return recv_ls();
    }

    public void send_ls(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("ls", TMessageType.CALL, ++seqid_));
      ls_args args = new ls_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public List<Stat> recv_ls() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "ls failed: out of sequence response");
      }
      ls_result result = new ls_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "ls failed: unknown result");
    }

    public boolean mkdirhier(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_mkdirhier(ctx, path, perms);
      return recv_mkdirhier();
    }

    public void send_mkdirhier(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("mkdirhier", TMessageType.CALL, ++seqid_));
      mkdirhier_args args = new mkdirhier_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setPerms(perms);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public boolean recv_mkdirhier() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "mkdirhier failed: out of sequence response");
      }
      mkdirhier_result result = new mkdirhier_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "mkdirhier failed: unknown result");
    }

    public void refreshNodes(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_refreshNodes(ctx);
      recv_refreshNodes();
    }

    public void send_refreshNodes(org.apache.hadoop.thriftfs.api.RequestContext ctx) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("refreshNodes", TMessageType.CALL, ++seqid_));
      refreshNodes_args args = new refreshNodes_args();
      args.setCtx(ctx);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_refreshNodes() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "refreshNodes failed: out of sequence response");
      }
      refreshNodes_result result = new refreshNodes_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public boolean rename(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String newPath) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_rename(ctx, path, newPath);
      return recv_rename();
    }

    public void send_rename(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String newPath) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("rename", TMessageType.CALL, ++seqid_));
      rename_args args = new rename_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setNewPath(newPath);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public boolean recv_rename() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "rename failed: out of sequence response");
      }
      rename_result result = new rename_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "rename failed: unknown result");
    }

    public void reportBadBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<Block> blocks) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_reportBadBlocks(ctx, blocks);
      recv_reportBadBlocks();
    }

    public void send_reportBadBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<Block> blocks) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("reportBadBlocks", TMessageType.CALL, ++seqid_));
      reportBadBlocks_args args = new reportBadBlocks_args();
      args.setCtx(ctx);
      args.setBlocks(blocks);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_reportBadBlocks() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "reportBadBlocks failed: out of sequence response");
      }
      reportBadBlocks_result result = new reportBadBlocks_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public Stat stat(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_stat(ctx, path);
      return recv_stat();
    }

    public void send_stat(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("stat", TMessageType.CALL, ++seqid_));
      stat_args args = new stat_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public Stat recv_stat() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "stat failed: out of sequence response");
      }
      stat_result result = new stat_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "stat failed: unknown result");
    }

    public ContentSummary getContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, String Path) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_getContentSummary(ctx, Path);
      return recv_getContentSummary();
    }

    public void send_getContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, String Path) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("getContentSummary", TMessageType.CALL, ++seqid_));
      getContentSummary_args args = new getContentSummary_args();
      args.setCtx(ctx);
      args.setPath(Path);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public ContentSummary recv_getContentSummary() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "getContentSummary failed: out of sequence response");
      }
      getContentSummary_result result = new getContentSummary_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "getContentSummary failed: unknown result");
    }

    public List<ContentSummary> multiGetContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<String> paths) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_multiGetContentSummary(ctx, paths);
      return recv_multiGetContentSummary();
    }

    public void send_multiGetContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<String> paths) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("multiGetContentSummary", TMessageType.CALL, ++seqid_));
      multiGetContentSummary_args args = new multiGetContentSummary_args();
      args.setCtx(ctx);
      args.setPaths(paths);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public List<ContentSummary> recv_multiGetContentSummary() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "multiGetContentSummary failed: out of sequence response");
      }
      multiGetContentSummary_result result = new multiGetContentSummary_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "multiGetContentSummary failed: unknown result");
    }

    public void setQuota(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long namespaceQuota, long diskspaceQuota) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_setQuota(ctx, path, namespaceQuota, diskspaceQuota);
      recv_setQuota();
    }

    public void send_setQuota(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long namespaceQuota, long diskspaceQuota) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("setQuota", TMessageType.CALL, ++seqid_));
      setQuota_args args = new setQuota_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setNamespaceQuota(namespaceQuota);
      args.setDiskspaceQuota(diskspaceQuota);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_setQuota() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "setQuota failed: out of sequence response");
      }
      setQuota_result result = new setQuota_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public boolean setReplication(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short replication) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_setReplication(ctx, path, replication);
      return recv_setReplication();
    }

    public void send_setReplication(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short replication) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("setReplication", TMessageType.CALL, ++seqid_));
      setReplication_args args = new setReplication_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setReplication(replication);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public boolean recv_setReplication() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "setReplication failed: out of sequence response");
      }
      setReplication_result result = new setReplication_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "setReplication failed: unknown result");
    }

    public boolean unlink(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, boolean recursive) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_unlink(ctx, path, recursive);
      return recv_unlink();
    }

    public void send_unlink(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, boolean recursive) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("unlink", TMessageType.CALL, ++seqid_));
      unlink_args args = new unlink_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setRecursive(recursive);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public boolean recv_unlink() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "unlink failed: out of sequence response");
      }
      unlink_result result = new unlink_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "unlink failed: unknown result");
    }

    public void utime(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long atime, long mtime) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_utime(ctx, path, atime, mtime);
      recv_utime();
    }

    public void send_utime(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long atime, long mtime) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("utime", TMessageType.CALL, ++seqid_));
      utime_args args = new utime_args();
      args.setCtx(ctx);
      args.setPath(path);
      args.setAtime(atime);
      args.setMtime(mtime);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_utime() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "utime failed: out of sequence response");
      }
      utime_result result = new utime_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.err != null) {
        throw result.err;
      }
      return;
    }

    public void datanodeUp(String name, String storage, int thriftPort) throws TException
    {
      send_datanodeUp(name, storage, thriftPort);
      recv_datanodeUp();
    }

    public void send_datanodeUp(String name, String storage, int thriftPort) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("datanodeUp", TMessageType.CALL, ++seqid_));
      datanodeUp_args args = new datanodeUp_args();
      args.setName(name);
      args.setStorage(storage);
      args.setThriftPort(thriftPort);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_datanodeUp() throws TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "datanodeUp failed: out of sequence response");
      }
      datanodeUp_result result = new datanodeUp_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      return;
    }

    public void datanodeDown(String name, String storage, int thriftPort) throws TException
    {
      send_datanodeDown(name, storage, thriftPort);
      recv_datanodeDown();
    }

    public void send_datanodeDown(String name, String storage, int thriftPort) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("datanodeDown", TMessageType.CALL, ++seqid_));
      datanodeDown_args args = new datanodeDown_args();
      args.setName(name);
      args.setStorage(storage);
      args.setThriftPort(thriftPort);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public void recv_datanodeDown() throws TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "datanodeDown failed: out of sequence response");
      }
      datanodeDown_result result = new datanodeDown_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      return;
    }

    public org.apache.hadoop.thriftfs.api.ThriftDelegationToken getDelegationToken(org.apache.hadoop.thriftfs.api.RequestContext ctx, String renewer) throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      send_getDelegationToken(ctx, renewer);
      return recv_getDelegationToken();
    }

    public void send_getDelegationToken(org.apache.hadoop.thriftfs.api.RequestContext ctx, String renewer) throws TException
    {
      oprot_.writeMessageBegin(new TMessage("getDelegationToken", TMessageType.CALL, ++seqid_));
      getDelegationToken_args args = new getDelegationToken_args();
      args.setCtx(ctx);
      args.setRenewer(renewer);
      args.write(oprot_);
      oprot_.writeMessageEnd();
      oprot_.getTransport().flush();
    }

    public org.apache.hadoop.thriftfs.api.ThriftDelegationToken recv_getDelegationToken() throws org.apache.hadoop.thriftfs.api.IOException, TException
    {
      TMessage msg = iprot_.readMessageBegin();
      if (msg.type == TMessageType.EXCEPTION) {
        TApplicationException x = TApplicationException.read(iprot_);
        iprot_.readMessageEnd();
        throw x;
      }
      if (msg.seqid != seqid_) {
        throw new TApplicationException(TApplicationException.BAD_SEQUENCE_ID, "getDelegationToken failed: out of sequence response");
      }
      getDelegationToken_result result = new getDelegationToken_result();
      result.read(iprot_);
      iprot_.readMessageEnd();
      if (result.isSetSuccess()) {
        return result.success;
      }
      if (result.err != null) {
        throw result.err;
      }
      throw new TApplicationException(TApplicationException.MISSING_RESULT, "getDelegationToken failed: unknown result");
    }

  }
  public static class AsyncClient extends org.apache.hadoop.thriftfs.api.HadoopServiceBase.AsyncClient implements AsyncIface {
    public static class Factory implements TAsyncClientFactory<AsyncClient> {
      private TAsyncClientManager clientManager;
      private TProtocolFactory protocolFactory;
      public Factory(TAsyncClientManager clientManager, TProtocolFactory protocolFactory) {
        this.clientManager = clientManager;
        this.protocolFactory = protocolFactory;
      }
      public AsyncClient getAsyncClient(TNonblockingTransport transport) {
        return new AsyncClient(protocolFactory, clientManager, transport);
      }
    }

    public AsyncClient(TProtocolFactory protocolFactory, TAsyncClientManager clientManager, TNonblockingTransport transport) {
      super(protocolFactory, clientManager, transport);
    }

    public void chmod(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms, AsyncMethodCallback<chmod_call> resultHandler) throws TException {
      checkReady();
      chmod_call method_call = new chmod_call(ctx, path, perms, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class chmod_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private short perms;
      public chmod_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms, AsyncMethodCallback<chmod_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.perms = perms;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("chmod", TMessageType.CALL, 0));
        chmod_args args = new chmod_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setPerms(perms);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_chmod();
      }
    }

    public void chown(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String owner, String group, AsyncMethodCallback<chown_call> resultHandler) throws TException {
      checkReady();
      chown_call method_call = new chown_call(ctx, path, owner, group, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class chown_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private String owner;
      private String group;
      public chown_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String owner, String group, AsyncMethodCallback<chown_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.owner = owner;
        this.group = group;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("chown", TMessageType.CALL, 0));
        chown_args args = new chown_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setOwner(owner);
        args.setGroup(group);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_chown();
      }
    }

    public void df(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<df_call> resultHandler) throws TException {
      checkReady();
      df_call method_call = new df_call(ctx, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class df_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      public df_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<df_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("df", TMessageType.CALL, 0));
        df_args args = new df_args();
        args.setCtx(ctx);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public List<Long> getResult() throws TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_df();
      }
    }

    public void enterSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<enterSafeMode_call> resultHandler) throws TException {
      checkReady();
      enterSafeMode_call method_call = new enterSafeMode_call(ctx, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class enterSafeMode_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      public enterSafeMode_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<enterSafeMode_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("enterSafeMode", TMessageType.CALL, 0));
        enterSafeMode_args args = new enterSafeMode_args();
        args.setCtx(ctx);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_enterSafeMode();
      }
    }

    public void getBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long offset, long length, AsyncMethodCallback<getBlocks_call> resultHandler) throws TException {
      checkReady();
      getBlocks_call method_call = new getBlocks_call(ctx, path, offset, length, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class getBlocks_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private long offset;
      private long length;
      public getBlocks_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long offset, long length, AsyncMethodCallback<getBlocks_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.offset = offset;
        this.length = length;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("getBlocks", TMessageType.CALL, 0));
        getBlocks_args args = new getBlocks_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setOffset(offset);
        args.setLength(length);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public List<Block> getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_getBlocks();
      }
    }

    public void getPreferredBlockSize(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<getPreferredBlockSize_call> resultHandler) throws TException {
      checkReady();
      getPreferredBlockSize_call method_call = new getPreferredBlockSize_call(ctx, path, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class getPreferredBlockSize_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      public getPreferredBlockSize_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<getPreferredBlockSize_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("getPreferredBlockSize", TMessageType.CALL, 0));
        getPreferredBlockSize_args args = new getPreferredBlockSize_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public long getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_getPreferredBlockSize();
      }
    }

    public void isInSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<isInSafeMode_call> resultHandler) throws TException {
      checkReady();
      isInSafeMode_call method_call = new isInSafeMode_call(ctx, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class isInSafeMode_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      public isInSafeMode_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<isInSafeMode_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("isInSafeMode", TMessageType.CALL, 0));
        isInSafeMode_args args = new isInSafeMode_args();
        args.setCtx(ctx);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public boolean getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_isInSafeMode();
      }
    }

    public void leaveSafeMode(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<leaveSafeMode_call> resultHandler) throws TException {
      checkReady();
      leaveSafeMode_call method_call = new leaveSafeMode_call(ctx, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class leaveSafeMode_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      public leaveSafeMode_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<leaveSafeMode_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("leaveSafeMode", TMessageType.CALL, 0));
        leaveSafeMode_args args = new leaveSafeMode_args();
        args.setCtx(ctx);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_leaveSafeMode();
      }
    }

    public void ls(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<ls_call> resultHandler) throws TException {
      checkReady();
      ls_call method_call = new ls_call(ctx, path, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class ls_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      public ls_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<ls_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("ls", TMessageType.CALL, 0));
        ls_args args = new ls_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public List<Stat> getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_ls();
      }
    }

    public void mkdirhier(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms, AsyncMethodCallback<mkdirhier_call> resultHandler) throws TException {
      checkReady();
      mkdirhier_call method_call = new mkdirhier_call(ctx, path, perms, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class mkdirhier_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private short perms;
      public mkdirhier_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short perms, AsyncMethodCallback<mkdirhier_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.perms = perms;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("mkdirhier", TMessageType.CALL, 0));
        mkdirhier_args args = new mkdirhier_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setPerms(perms);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public boolean getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_mkdirhier();
      }
    }

    public void refreshNodes(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<refreshNodes_call> resultHandler) throws TException {
      checkReady();
      refreshNodes_call method_call = new refreshNodes_call(ctx, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class refreshNodes_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      public refreshNodes_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, AsyncMethodCallback<refreshNodes_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("refreshNodes", TMessageType.CALL, 0));
        refreshNodes_args args = new refreshNodes_args();
        args.setCtx(ctx);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_refreshNodes();
      }
    }

    public void rename(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String newPath, AsyncMethodCallback<rename_call> resultHandler) throws TException {
      checkReady();
      rename_call method_call = new rename_call(ctx, path, newPath, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class rename_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private String newPath;
      public rename_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, String newPath, AsyncMethodCallback<rename_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.newPath = newPath;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("rename", TMessageType.CALL, 0));
        rename_args args = new rename_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setNewPath(newPath);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public boolean getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_rename();
      }
    }

    public void reportBadBlocks(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<Block> blocks, AsyncMethodCallback<reportBadBlocks_call> resultHandler) throws TException {
      checkReady();
      reportBadBlocks_call method_call = new reportBadBlocks_call(ctx, blocks, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class reportBadBlocks_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private List<Block> blocks;
      public reportBadBlocks_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<Block> blocks, AsyncMethodCallback<reportBadBlocks_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.blocks = blocks;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("reportBadBlocks", TMessageType.CALL, 0));
        reportBadBlocks_args args = new reportBadBlocks_args();
        args.setCtx(ctx);
        args.setBlocks(blocks);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_reportBadBlocks();
      }
    }

    public void stat(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<stat_call> resultHandler) throws TException {
      checkReady();
      stat_call method_call = new stat_call(ctx, path, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class stat_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      public stat_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, AsyncMethodCallback<stat_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("stat", TMessageType.CALL, 0));
        stat_args args = new stat_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public Stat getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_stat();
      }
    }

    public void getContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, String Path, AsyncMethodCallback<getContentSummary_call> resultHandler) throws TException {
      checkReady();
      getContentSummary_call method_call = new getContentSummary_call(ctx, Path, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class getContentSummary_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String Path;
      public getContentSummary_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String Path, AsyncMethodCallback<getContentSummary_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.Path = Path;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("getContentSummary", TMessageType.CALL, 0));
        getContentSummary_args args = new getContentSummary_args();
        args.setCtx(ctx);
        args.setPath(Path);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public ContentSummary getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_getContentSummary();
      }
    }

    public void multiGetContentSummary(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<String> paths, AsyncMethodCallback<multiGetContentSummary_call> resultHandler) throws TException {
      checkReady();
      multiGetContentSummary_call method_call = new multiGetContentSummary_call(ctx, paths, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class multiGetContentSummary_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private List<String> paths;
      public multiGetContentSummary_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, List<String> paths, AsyncMethodCallback<multiGetContentSummary_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.paths = paths;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("multiGetContentSummary", TMessageType.CALL, 0));
        multiGetContentSummary_args args = new multiGetContentSummary_args();
        args.setCtx(ctx);
        args.setPaths(paths);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public List<ContentSummary> getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_multiGetContentSummary();
      }
    }

    public void setQuota(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long namespaceQuota, long diskspaceQuota, AsyncMethodCallback<setQuota_call> resultHandler) throws TException {
      checkReady();
      setQuota_call method_call = new setQuota_call(ctx, path, namespaceQuota, diskspaceQuota, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class setQuota_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private long namespaceQuota;
      private long diskspaceQuota;
      public setQuota_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long namespaceQuota, long diskspaceQuota, AsyncMethodCallback<setQuota_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.namespaceQuota = namespaceQuota;
        this.diskspaceQuota = diskspaceQuota;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("setQuota", TMessageType.CALL, 0));
        setQuota_args args = new setQuota_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setNamespaceQuota(namespaceQuota);
        args.setDiskspaceQuota(diskspaceQuota);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_setQuota();
      }
    }

    public void setReplication(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short replication, AsyncMethodCallback<setReplication_call> resultHandler) throws TException {
      checkReady();
      setReplication_call method_call = new setReplication_call(ctx, path, replication, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class setReplication_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private short replication;
      public setReplication_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, short replication, AsyncMethodCallback<setReplication_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.replication = replication;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("setReplication", TMessageType.CALL, 0));
        setReplication_args args = new setReplication_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setReplication(replication);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public boolean getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_setReplication();
      }
    }

    public void unlink(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, boolean recursive, AsyncMethodCallback<unlink_call> resultHandler) throws TException {
      checkReady();
      unlink_call method_call = new unlink_call(ctx, path, recursive, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class unlink_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private boolean recursive;
      public unlink_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, boolean recursive, AsyncMethodCallback<unlink_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.recursive = recursive;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("unlink", TMessageType.CALL, 0));
        unlink_args args = new unlink_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setRecursive(recursive);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public boolean getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_unlink();
      }
    }

    public void utime(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long atime, long mtime, AsyncMethodCallback<utime_call> resultHandler) throws TException {
      checkReady();
      utime_call method_call = new utime_call(ctx, path, atime, mtime, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class utime_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String path;
      private long atime;
      private long mtime;
      public utime_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String path, long atime, long mtime, AsyncMethodCallback<utime_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.path = path;
        this.atime = atime;
        this.mtime = mtime;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("utime", TMessageType.CALL, 0));
        utime_args args = new utime_args();
        args.setCtx(ctx);
        args.setPath(path);
        args.setAtime(atime);
        args.setMtime(mtime);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_utime();
      }
    }

    public void datanodeUp(String name, String storage, int thriftPort, AsyncMethodCallback<datanodeUp_call> resultHandler) throws TException {
      checkReady();
      datanodeUp_call method_call = new datanodeUp_call(name, storage, thriftPort, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class datanodeUp_call extends TAsyncMethodCall {
      private String name;
      private String storage;
      private int thriftPort;
      public datanodeUp_call(String name, String storage, int thriftPort, AsyncMethodCallback<datanodeUp_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.name = name;
        this.storage = storage;
        this.thriftPort = thriftPort;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("datanodeUp", TMessageType.CALL, 0));
        datanodeUp_args args = new datanodeUp_args();
        args.setName(name);
        args.setStorage(storage);
        args.setThriftPort(thriftPort);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_datanodeUp();
      }
    }

    public void datanodeDown(String name, String storage, int thriftPort, AsyncMethodCallback<datanodeDown_call> resultHandler) throws TException {
      checkReady();
      datanodeDown_call method_call = new datanodeDown_call(name, storage, thriftPort, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class datanodeDown_call extends TAsyncMethodCall {
      private String name;
      private String storage;
      private int thriftPort;
      public datanodeDown_call(String name, String storage, int thriftPort, AsyncMethodCallback<datanodeDown_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.name = name;
        this.storage = storage;
        this.thriftPort = thriftPort;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("datanodeDown", TMessageType.CALL, 0));
        datanodeDown_args args = new datanodeDown_args();
        args.setName(name);
        args.setStorage(storage);
        args.setThriftPort(thriftPort);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public void getResult() throws TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        (new Client(prot)).recv_datanodeDown();
      }
    }

    public void getDelegationToken(org.apache.hadoop.thriftfs.api.RequestContext ctx, String renewer, AsyncMethodCallback<getDelegationToken_call> resultHandler) throws TException {
      checkReady();
      getDelegationToken_call method_call = new getDelegationToken_call(ctx, renewer, resultHandler, this, protocolFactory, transport);
      manager.call(method_call);
    }

    public static class getDelegationToken_call extends TAsyncMethodCall {
      private org.apache.hadoop.thriftfs.api.RequestContext ctx;
      private String renewer;
      public getDelegationToken_call(org.apache.hadoop.thriftfs.api.RequestContext ctx, String renewer, AsyncMethodCallback<getDelegationToken_call> resultHandler, TAsyncClient client, TProtocolFactory protocolFactory, TNonblockingTransport transport) throws TException {
        super(client, protocolFactory, transport, resultHandler, false);
        this.ctx = ctx;
        this.renewer = renewer;
      }

      public void write_args(TProtocol prot) throws TException {
        prot.writeMessageBegin(new TMessage("getDelegationToken", TMessageType.CALL, 0));
        getDelegationToken_args args = new getDelegationToken_args();
        args.setCtx(ctx);
        args.setRenewer(renewer);
        args.write(prot);
        prot.writeMessageEnd();
      }

      public org.apache.hadoop.thriftfs.api.ThriftDelegationToken getResult() throws org.apache.hadoop.thriftfs.api.IOException, TException {
        if (getState() != State.RESPONSE_READ) {
          throw new IllegalStateException("Method call not finished!");
        }
        TMemoryInputTransport memoryTransport = new TMemoryInputTransport(getFrameBuffer().array());
        TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);
        return (new Client(prot)).recv_getDelegationToken();
      }
    }

  }

  public static class Processor extends org.apache.hadoop.thriftfs.api.HadoopServiceBase.Processor implements TProcessor {
    private static final Logger LOGGER = LoggerFactory.getLogger(Processor.class.getName());
    public Processor(Iface iface)
    {
      super(iface);
      iface_ = iface;
      processMap_.put("chmod", new chmod());
      processMap_.put("chown", new chown());
      processMap_.put("df", new df());
      processMap_.put("enterSafeMode", new enterSafeMode());
      processMap_.put("getBlocks", new getBlocks());
      processMap_.put("getPreferredBlockSize", new getPreferredBlockSize());
      processMap_.put("isInSafeMode", new isInSafeMode());
      processMap_.put("leaveSafeMode", new leaveSafeMode());
      processMap_.put("ls", new ls());
      processMap_.put("mkdirhier", new mkdirhier());
      processMap_.put("refreshNodes", new refreshNodes());
      processMap_.put("rename", new rename());
      processMap_.put("reportBadBlocks", new reportBadBlocks());
      processMap_.put("stat", new stat());
      processMap_.put("getContentSummary", new getContentSummary());
      processMap_.put("multiGetContentSummary", new multiGetContentSummary());
      processMap_.put("setQuota", new setQuota());
      processMap_.put("setReplication", new setReplication());
      processMap_.put("unlink", new unlink());
      processMap_.put("utime", new utime());
      processMap_.put("datanodeUp", new datanodeUp());
      processMap_.put("datanodeDown", new datanodeDown());
      processMap_.put("getDelegationToken", new getDelegationToken());
    }

    private Iface iface_;

    public boolean process(TProtocol iprot, TProtocol oprot) throws TException
    {
      TMessage msg = iprot.readMessageBegin();
      ProcessFunction fn = processMap_.get(msg.name);
      if (fn == null) {
        TProtocolUtil.skip(iprot, TType.STRUCT);
        iprot.readMessageEnd();
        TApplicationException x = new TApplicationException(TApplicationException.UNKNOWN_METHOD, "Invalid method name: '"+msg.name+"'");
        oprot.writeMessageBegin(new TMessage(msg.name, TMessageType.EXCEPTION, msg.seqid));
        x.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
        return true;
      }
      fn.process(msg.seqid, iprot, oprot);
      return true;
    }

    private class chmod implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        chmod_args args = new chmod_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("chmod", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        chmod_result result = new chmod_result();
        try {
          iface_.chmod(args.ctx, args.path, args.perms);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing chmod", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing chmod");
          oprot.writeMessageBegin(new TMessage("chmod", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("chmod", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class chown implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        chown_args args = new chown_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("chown", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        chown_result result = new chown_result();
        try {
          iface_.chown(args.ctx, args.path, args.owner, args.group);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing chown", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing chown");
          oprot.writeMessageBegin(new TMessage("chown", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("chown", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class df implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        df_args args = new df_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("df", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        df_result result = new df_result();
        result.success = iface_.df(args.ctx);
        oprot.writeMessageBegin(new TMessage("df", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class enterSafeMode implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        enterSafeMode_args args = new enterSafeMode_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("enterSafeMode", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        enterSafeMode_result result = new enterSafeMode_result();
        try {
          iface_.enterSafeMode(args.ctx);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing enterSafeMode", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing enterSafeMode");
          oprot.writeMessageBegin(new TMessage("enterSafeMode", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("enterSafeMode", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class getBlocks implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        getBlocks_args args = new getBlocks_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("getBlocks", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        getBlocks_result result = new getBlocks_result();
        try {
          result.success = iface_.getBlocks(args.ctx, args.path, args.offset, args.length);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing getBlocks", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing getBlocks");
          oprot.writeMessageBegin(new TMessage("getBlocks", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("getBlocks", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class getPreferredBlockSize implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        getPreferredBlockSize_args args = new getPreferredBlockSize_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("getPreferredBlockSize", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        getPreferredBlockSize_result result = new getPreferredBlockSize_result();
        try {
          result.success = iface_.getPreferredBlockSize(args.ctx, args.path);
          result.setSuccessIsSet(true);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing getPreferredBlockSize", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing getPreferredBlockSize");
          oprot.writeMessageBegin(new TMessage("getPreferredBlockSize", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("getPreferredBlockSize", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class isInSafeMode implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        isInSafeMode_args args = new isInSafeMode_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("isInSafeMode", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        isInSafeMode_result result = new isInSafeMode_result();
        try {
          result.success = iface_.isInSafeMode(args.ctx);
          result.setSuccessIsSet(true);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing isInSafeMode", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing isInSafeMode");
          oprot.writeMessageBegin(new TMessage("isInSafeMode", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("isInSafeMode", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class leaveSafeMode implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        leaveSafeMode_args args = new leaveSafeMode_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("leaveSafeMode", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        leaveSafeMode_result result = new leaveSafeMode_result();
        try {
          iface_.leaveSafeMode(args.ctx);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing leaveSafeMode", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing leaveSafeMode");
          oprot.writeMessageBegin(new TMessage("leaveSafeMode", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("leaveSafeMode", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class ls implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        ls_args args = new ls_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("ls", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        ls_result result = new ls_result();
        try {
          result.success = iface_.ls(args.ctx, args.path);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing ls", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing ls");
          oprot.writeMessageBegin(new TMessage("ls", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("ls", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class mkdirhier implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        mkdirhier_args args = new mkdirhier_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("mkdirhier", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        mkdirhier_result result = new mkdirhier_result();
        try {
          result.success = iface_.mkdirhier(args.ctx, args.path, args.perms);
          result.setSuccessIsSet(true);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing mkdirhier", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing mkdirhier");
          oprot.writeMessageBegin(new TMessage("mkdirhier", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("mkdirhier", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class refreshNodes implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        refreshNodes_args args = new refreshNodes_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("refreshNodes", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        refreshNodes_result result = new refreshNodes_result();
        try {
          iface_.refreshNodes(args.ctx);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing refreshNodes", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing refreshNodes");
          oprot.writeMessageBegin(new TMessage("refreshNodes", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("refreshNodes", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class rename implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        rename_args args = new rename_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("rename", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        rename_result result = new rename_result();
        try {
          result.success = iface_.rename(args.ctx, args.path, args.newPath);
          result.setSuccessIsSet(true);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing rename", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing rename");
          oprot.writeMessageBegin(new TMessage("rename", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("rename", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class reportBadBlocks implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        reportBadBlocks_args args = new reportBadBlocks_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("reportBadBlocks", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        reportBadBlocks_result result = new reportBadBlocks_result();
        try {
          iface_.reportBadBlocks(args.ctx, args.blocks);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing reportBadBlocks", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing reportBadBlocks");
          oprot.writeMessageBegin(new TMessage("reportBadBlocks", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("reportBadBlocks", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class stat implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        stat_args args = new stat_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("stat", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        stat_result result = new stat_result();
        try {
          result.success = iface_.stat(args.ctx, args.path);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing stat", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing stat");
          oprot.writeMessageBegin(new TMessage("stat", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("stat", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class getContentSummary implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        getContentSummary_args args = new getContentSummary_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("getContentSummary", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        getContentSummary_result result = new getContentSummary_result();
        try {
          result.success = iface_.getContentSummary(args.ctx, args.Path);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing getContentSummary", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing getContentSummary");
          oprot.writeMessageBegin(new TMessage("getContentSummary", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("getContentSummary", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class multiGetContentSummary implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        multiGetContentSummary_args args = new multiGetContentSummary_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("multiGetContentSummary", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        multiGetContentSummary_result result = new multiGetContentSummary_result();
        try {
          result.success = iface_.multiGetContentSummary(args.ctx, args.paths);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing multiGetContentSummary", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing multiGetContentSummary");
          oprot.writeMessageBegin(new TMessage("multiGetContentSummary", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("multiGetContentSummary", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class setQuota implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        setQuota_args args = new setQuota_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("setQuota", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        setQuota_result result = new setQuota_result();
        try {
          iface_.setQuota(args.ctx, args.path, args.namespaceQuota, args.diskspaceQuota);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing setQuota", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing setQuota");
          oprot.writeMessageBegin(new TMessage("setQuota", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("setQuota", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class setReplication implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        setReplication_args args = new setReplication_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("setReplication", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        setReplication_result result = new setReplication_result();
        try {
          result.success = iface_.setReplication(args.ctx, args.path, args.replication);
          result.setSuccessIsSet(true);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing setReplication", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing setReplication");
          oprot.writeMessageBegin(new TMessage("setReplication", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("setReplication", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class unlink implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        unlink_args args = new unlink_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("unlink", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        unlink_result result = new unlink_result();
        try {
          result.success = iface_.unlink(args.ctx, args.path, args.recursive);
          result.setSuccessIsSet(true);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing unlink", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing unlink");
          oprot.writeMessageBegin(new TMessage("unlink", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("unlink", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class utime implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        utime_args args = new utime_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("utime", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        utime_result result = new utime_result();
        try {
          iface_.utime(args.ctx, args.path, args.atime, args.mtime);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing utime", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing utime");
          oprot.writeMessageBegin(new TMessage("utime", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("utime", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class datanodeUp implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        datanodeUp_args args = new datanodeUp_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("datanodeUp", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        datanodeUp_result result = new datanodeUp_result();
        iface_.datanodeUp(args.name, args.storage, args.thriftPort);
        oprot.writeMessageBegin(new TMessage("datanodeUp", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class datanodeDown implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        datanodeDown_args args = new datanodeDown_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("datanodeDown", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        datanodeDown_result result = new datanodeDown_result();
        iface_.datanodeDown(args.name, args.storage, args.thriftPort);
        oprot.writeMessageBegin(new TMessage("datanodeDown", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

    private class getDelegationToken implements ProcessFunction {
      public void process(int seqid, TProtocol iprot, TProtocol oprot) throws TException
      {
        getDelegationToken_args args = new getDelegationToken_args();
        try {
          args.read(iprot);
        } catch (TProtocolException e) {
          iprot.readMessageEnd();
          TApplicationException x = new TApplicationException(TApplicationException.PROTOCOL_ERROR, e.getMessage());
          oprot.writeMessageBegin(new TMessage("getDelegationToken", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        iprot.readMessageEnd();
        getDelegationToken_result result = new getDelegationToken_result();
        try {
          result.success = iface_.getDelegationToken(args.ctx, args.renewer);
        } catch (org.apache.hadoop.thriftfs.api.IOException err) {
          result.err = err;
        } catch (Throwable th) {
          LOGGER.error("Internal error processing getDelegationToken", th);
          TApplicationException x = new TApplicationException(TApplicationException.INTERNAL_ERROR, "Internal error processing getDelegationToken");
          oprot.writeMessageBegin(new TMessage("getDelegationToken", TMessageType.EXCEPTION, seqid));
          x.write(oprot);
          oprot.writeMessageEnd();
          oprot.getTransport().flush();
          return;
        }
        oprot.writeMessageBegin(new TMessage("getDelegationToken", TMessageType.REPLY, seqid));
        result.write(oprot);
        oprot.writeMessageEnd();
        oprot.getTransport().flush();
      }

    }

  }

  public static class chmod_args implements TBase<chmod_args, chmod_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("chmod_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField PERMS_FIELD_DESC = new TField("perms", TType.I16, (short)2);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path of the file or directory.
     */
    public String path;
    /**
     * New permissions for the file or directory.
     */
    public short perms;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path of the file or directory.
       */
      PATH((short)1, "path"),
      /**
       * New permissions for the file or directory.
       */
      PERMS((short)2, "perms");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // PERMS
            return PERMS;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __PERMS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.PERMS, new FieldMetaData("perms", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I16)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(chmod_args.class, metaDataMap);
    }

    public chmod_args() {
    }

    public chmod_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      short perms)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.perms = perms;
      setPermsIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public chmod_args(chmod_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.perms = other.perms;
    }

    public chmod_args deepCopy() {
      return new chmod_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setPermsIsSet(false);
      this.perms = 0;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public chmod_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path of the file or directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path of the file or directory.
     */
    public chmod_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * New permissions for the file or directory.
     */
    public short getPerms() {
      return this.perms;
    }

    /**
     * New permissions for the file or directory.
     */
    public chmod_args setPerms(short perms) {
      this.perms = perms;
      setPermsIsSet(true);
      return this;
    }

    public void unsetPerms() {
      __isset_bit_vector.clear(__PERMS_ISSET_ID);
    }

    /** Returns true if field perms is set (has been asigned a value) and false otherwise */
    public boolean isSetPerms() {
      return __isset_bit_vector.get(__PERMS_ISSET_ID);
    }

    public void setPermsIsSet(boolean value) {
      __isset_bit_vector.set(__PERMS_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case PERMS:
        if (value == null) {
          unsetPerms();
        } else {
          setPerms((Short)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case PERMS:
        return new Short(getPerms());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case PERMS:
        return isSetPerms();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof chmod_args)
        return this.equals((chmod_args)that);
      return false;
    }

    public boolean equals(chmod_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_perms = true;
      boolean that_present_perms = true;
      if (this_present_perms || that_present_perms) {
        if (!(this_present_perms && that_present_perms))
          return false;
        if (this.perms != that.perms)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(chmod_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      chmod_args typedOther = (chmod_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPerms()).compareTo(typedOther.isSetPerms());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPerms()) {
        lastComparison = TBaseHelper.compareTo(this.perms, typedOther.perms);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // PERMS
            if (field.type == TType.I16) {
              this.perms = iprot.readI16();
              setPermsIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(PERMS_FIELD_DESC);
      oprot.writeI16(this.perms);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("chmod_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("perms:");
      sb.append(this.perms);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class chmod_result implements TBase<chmod_result, chmod_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("chmod_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(chmod_result.class, metaDataMap);
    }

    public chmod_result() {
    }

    public chmod_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public chmod_result(chmod_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public chmod_result deepCopy() {
      return new chmod_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public chmod_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof chmod_result)
        return this.equals((chmod_result)that);
      return false;
    }

    public boolean equals(chmod_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(chmod_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      chmod_result typedOther = (chmod_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("chmod_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class chown_args implements TBase<chown_args, chown_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("chown_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField OWNER_FIELD_DESC = new TField("owner", TType.STRING, (short)2);
    private static final TField GROUP_FIELD_DESC = new TField("group", TType.STRING, (short)3);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path to the file or directory
     */
    public String path;
    /**
     * New owner.
     */
    public String owner;
    /**
     * New group.
     */
    public String group;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path to the file or directory
       */
      PATH((short)1, "path"),
      /**
       * New owner.
       */
      OWNER((short)2, "owner"),
      /**
       * New group.
       */
      GROUP((short)3, "group");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // OWNER
            return OWNER;
          case 3: // GROUP
            return GROUP;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.OWNER, new FieldMetaData("owner", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.GROUP, new FieldMetaData("group", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(chown_args.class, metaDataMap);
    }

    public chown_args() {
    }

    public chown_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      String owner,
      String group)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.owner = owner;
      this.group = group;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public chown_args(chown_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      if (other.isSetOwner()) {
        this.owner = other.owner;
      }
      if (other.isSetGroup()) {
        this.group = other.group;
      }
    }

    public chown_args deepCopy() {
      return new chown_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      this.owner = null;
      this.group = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public chown_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path to the file or directory
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path to the file or directory
     */
    public chown_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * New owner.
     */
    public String getOwner() {
      return this.owner;
    }

    /**
     * New owner.
     */
    public chown_args setOwner(String owner) {
      this.owner = owner;
      return this;
    }

    public void unsetOwner() {
      this.owner = null;
    }

    /** Returns true if field owner is set (has been asigned a value) and false otherwise */
    public boolean isSetOwner() {
      return this.owner != null;
    }

    public void setOwnerIsSet(boolean value) {
      if (!value) {
        this.owner = null;
      }
    }

    /**
     * New group.
     */
    public String getGroup() {
      return this.group;
    }

    /**
     * New group.
     */
    public chown_args setGroup(String group) {
      this.group = group;
      return this;
    }

    public void unsetGroup() {
      this.group = null;
    }

    /** Returns true if field group is set (has been asigned a value) and false otherwise */
    public boolean isSetGroup() {
      return this.group != null;
    }

    public void setGroupIsSet(boolean value) {
      if (!value) {
        this.group = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case OWNER:
        if (value == null) {
          unsetOwner();
        } else {
          setOwner((String)value);
        }
        break;

      case GROUP:
        if (value == null) {
          unsetGroup();
        } else {
          setGroup((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case OWNER:
        return getOwner();

      case GROUP:
        return getGroup();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case OWNER:
        return isSetOwner();
      case GROUP:
        return isSetGroup();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof chown_args)
        return this.equals((chown_args)that);
      return false;
    }

    public boolean equals(chown_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_owner = true && this.isSetOwner();
      boolean that_present_owner = true && that.isSetOwner();
      if (this_present_owner || that_present_owner) {
        if (!(this_present_owner && that_present_owner))
          return false;
        if (!this.owner.equals(that.owner))
          return false;
      }

      boolean this_present_group = true && this.isSetGroup();
      boolean that_present_group = true && that.isSetGroup();
      if (this_present_group || that_present_group) {
        if (!(this_present_group && that_present_group))
          return false;
        if (!this.group.equals(that.group))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(chown_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      chown_args typedOther = (chown_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetOwner()).compareTo(typedOther.isSetOwner());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetOwner()) {
        lastComparison = TBaseHelper.compareTo(this.owner, typedOther.owner);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetGroup()).compareTo(typedOther.isSetGroup());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetGroup()) {
        lastComparison = TBaseHelper.compareTo(this.group, typedOther.group);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // OWNER
            if (field.type == TType.STRING) {
              this.owner = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 3: // GROUP
            if (field.type == TType.STRING) {
              this.group = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      if (this.owner != null) {
        oprot.writeFieldBegin(OWNER_FIELD_DESC);
        oprot.writeString(this.owner);
        oprot.writeFieldEnd();
      }
      if (this.group != null) {
        oprot.writeFieldBegin(GROUP_FIELD_DESC);
        oprot.writeString(this.group);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("chown_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("owner:");
      if (this.owner == null) {
        sb.append("null");
      } else {
        sb.append(this.owner);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("group:");
      if (this.group == null) {
        sb.append("null");
      } else {
        sb.append(this.group);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class chown_result implements TBase<chown_result, chown_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("chown_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(chown_result.class, metaDataMap);
    }

    public chown_result() {
    }

    public chown_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public chown_result(chown_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public chown_result deepCopy() {
      return new chown_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public chown_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof chown_result)
        return this.equals((chown_result)that);
      return false;
    }

    public boolean equals(chown_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(chown_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      chown_result typedOther = (chown_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("chown_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class df_args implements TBase<df_args, df_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("df_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(df_args.class, metaDataMap);
    }

    public df_args() {
    }

    public df_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx)
    {
      this();
      this.ctx = ctx;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public df_args(df_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
    }

    public df_args deepCopy() {
      return new df_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public df_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof df_args)
        return this.equals((df_args)that);
      return false;
    }

    public boolean equals(df_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(df_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      df_args typedOther = (df_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("df_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class df_result implements TBase<df_result, df_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("df_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.LIST, (short)0);

    public List<Long> success;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new ListMetaData(TType.LIST, 
              new FieldValueMetaData(TType.I64))));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(df_result.class, metaDataMap);
    }

    public df_result() {
    }

    public df_result(
      List<Long> success)
    {
      this();
      this.success = success;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public df_result(df_result other) {
      if (other.isSetSuccess()) {
        List<Long> __this__success = new ArrayList<Long>();
        for (Long other_element : other.success) {
          __this__success.add(other_element);
        }
        this.success = __this__success;
      }
    }

    public df_result deepCopy() {
      return new df_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
    }

    public int getSuccessSize() {
      return (this.success == null) ? 0 : this.success.size();
    }

    public java.util.Iterator<Long> getSuccessIterator() {
      return (this.success == null) ? null : this.success.iterator();
    }

    public void addToSuccess(long elem) {
      if (this.success == null) {
        this.success = new ArrayList<Long>();
      }
      this.success.add(elem);
    }

    public List<Long> getSuccess() {
      return this.success;
    }

    public df_result setSuccess(List<Long> success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((List<Long>)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof df_result)
        return this.equals((df_result)that);
      return false;
    }

    public boolean equals(df_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(df_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      df_result typedOther = (df_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.LIST) {
              {
                TList _list4 = iprot.readListBegin();
                this.success = new ArrayList<Long>(_list4.size);
                for (int _i5 = 0; _i5 < _list4.size; ++_i5)
                {
                  long _elem6;
                  _elem6 = iprot.readI64();
                  this.success.add(_elem6);
                }
                iprot.readListEnd();
              }
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        {
          oprot.writeListBegin(new TList(TType.I64, this.success.size()));
          for (long _iter7 : this.success)
          {
            oprot.writeI64(_iter7);
          }
          oprot.writeListEnd();
        }
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("df_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class enterSafeMode_args implements TBase<enterSafeMode_args, enterSafeMode_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("enterSafeMode_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(enterSafeMode_args.class, metaDataMap);
    }

    public enterSafeMode_args() {
    }

    public enterSafeMode_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx)
    {
      this();
      this.ctx = ctx;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public enterSafeMode_args(enterSafeMode_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
    }

    public enterSafeMode_args deepCopy() {
      return new enterSafeMode_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public enterSafeMode_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof enterSafeMode_args)
        return this.equals((enterSafeMode_args)that);
      return false;
    }

    public boolean equals(enterSafeMode_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(enterSafeMode_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      enterSafeMode_args typedOther = (enterSafeMode_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("enterSafeMode_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class enterSafeMode_result implements TBase<enterSafeMode_result, enterSafeMode_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("enterSafeMode_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(enterSafeMode_result.class, metaDataMap);
    }

    public enterSafeMode_result() {
    }

    public enterSafeMode_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public enterSafeMode_result(enterSafeMode_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public enterSafeMode_result deepCopy() {
      return new enterSafeMode_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public enterSafeMode_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof enterSafeMode_result)
        return this.equals((enterSafeMode_result)that);
      return false;
    }

    public boolean equals(enterSafeMode_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(enterSafeMode_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      enterSafeMode_result typedOther = (enterSafeMode_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("enterSafeMode_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getBlocks_args implements TBase<getBlocks_args, getBlocks_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getBlocks_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField OFFSET_FIELD_DESC = new TField("offset", TType.I64, (short)2);
    private static final TField LENGTH_FIELD_DESC = new TField("length", TType.I64, (short)3);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path to the file.
     */
    public String path;
    /**
     * Offset of the region.
     */
    public long offset;
    /**
     * Length of the region
     */
    public long length;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path to the file.
       */
      PATH((short)1, "path"),
      /**
       * Offset of the region.
       */
      OFFSET((short)2, "offset"),
      /**
       * Length of the region
       */
      LENGTH((short)3, "length");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // OFFSET
            return OFFSET;
          case 3: // LENGTH
            return LENGTH;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __OFFSET_ISSET_ID = 0;
    private static final int __LENGTH_ISSET_ID = 1;
    private BitSet __isset_bit_vector = new BitSet(2);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.OFFSET, new FieldMetaData("offset", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      tmpMap.put(_Fields.LENGTH, new FieldMetaData("length", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getBlocks_args.class, metaDataMap);
    }

    public getBlocks_args() {
    }

    public getBlocks_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      long offset,
      long length)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.offset = offset;
      setOffsetIsSet(true);
      this.length = length;
      setLengthIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getBlocks_args(getBlocks_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.offset = other.offset;
      this.length = other.length;
    }

    public getBlocks_args deepCopy() {
      return new getBlocks_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setOffsetIsSet(false);
      this.offset = 0;
      setLengthIsSet(false);
      this.length = 0;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public getBlocks_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path to the file.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path to the file.
     */
    public getBlocks_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * Offset of the region.
     */
    public long getOffset() {
      return this.offset;
    }

    /**
     * Offset of the region.
     */
    public getBlocks_args setOffset(long offset) {
      this.offset = offset;
      setOffsetIsSet(true);
      return this;
    }

    public void unsetOffset() {
      __isset_bit_vector.clear(__OFFSET_ISSET_ID);
    }

    /** Returns true if field offset is set (has been asigned a value) and false otherwise */
    public boolean isSetOffset() {
      return __isset_bit_vector.get(__OFFSET_ISSET_ID);
    }

    public void setOffsetIsSet(boolean value) {
      __isset_bit_vector.set(__OFFSET_ISSET_ID, value);
    }

    /**
     * Length of the region
     */
    public long getLength() {
      return this.length;
    }

    /**
     * Length of the region
     */
    public getBlocks_args setLength(long length) {
      this.length = length;
      setLengthIsSet(true);
      return this;
    }

    public void unsetLength() {
      __isset_bit_vector.clear(__LENGTH_ISSET_ID);
    }

    /** Returns true if field length is set (has been asigned a value) and false otherwise */
    public boolean isSetLength() {
      return __isset_bit_vector.get(__LENGTH_ISSET_ID);
    }

    public void setLengthIsSet(boolean value) {
      __isset_bit_vector.set(__LENGTH_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case OFFSET:
        if (value == null) {
          unsetOffset();
        } else {
          setOffset((Long)value);
        }
        break;

      case LENGTH:
        if (value == null) {
          unsetLength();
        } else {
          setLength((Long)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case OFFSET:
        return new Long(getOffset());

      case LENGTH:
        return new Long(getLength());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case OFFSET:
        return isSetOffset();
      case LENGTH:
        return isSetLength();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getBlocks_args)
        return this.equals((getBlocks_args)that);
      return false;
    }

    public boolean equals(getBlocks_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_offset = true;
      boolean that_present_offset = true;
      if (this_present_offset || that_present_offset) {
        if (!(this_present_offset && that_present_offset))
          return false;
        if (this.offset != that.offset)
          return false;
      }

      boolean this_present_length = true;
      boolean that_present_length = true;
      if (this_present_length || that_present_length) {
        if (!(this_present_length && that_present_length))
          return false;
        if (this.length != that.length)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getBlocks_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getBlocks_args typedOther = (getBlocks_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetOffset()).compareTo(typedOther.isSetOffset());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetOffset()) {
        lastComparison = TBaseHelper.compareTo(this.offset, typedOther.offset);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetLength()).compareTo(typedOther.isSetLength());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetLength()) {
        lastComparison = TBaseHelper.compareTo(this.length, typedOther.length);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // OFFSET
            if (field.type == TType.I64) {
              this.offset = iprot.readI64();
              setOffsetIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 3: // LENGTH
            if (field.type == TType.I64) {
              this.length = iprot.readI64();
              setLengthIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(OFFSET_FIELD_DESC);
      oprot.writeI64(this.offset);
      oprot.writeFieldEnd();
      oprot.writeFieldBegin(LENGTH_FIELD_DESC);
      oprot.writeI64(this.length);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getBlocks_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("offset:");
      sb.append(this.offset);
      first = false;
      if (!first) sb.append(", ");
      sb.append("length:");
      sb.append(this.length);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getBlocks_result implements TBase<getBlocks_result, getBlocks_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getBlocks_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.LIST, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public List<Block> success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new ListMetaData(TType.LIST, 
              new StructMetaData(TType.STRUCT, Block.class))));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getBlocks_result.class, metaDataMap);
    }

    public getBlocks_result() {
    }

    public getBlocks_result(
      List<Block> success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getBlocks_result(getBlocks_result other) {
      if (other.isSetSuccess()) {
        List<Block> __this__success = new ArrayList<Block>();
        for (Block other_element : other.success) {
          __this__success.add(new Block(other_element));
        }
        this.success = __this__success;
      }
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public getBlocks_result deepCopy() {
      return new getBlocks_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
      this.err = null;
    }

    public int getSuccessSize() {
      return (this.success == null) ? 0 : this.success.size();
    }

    public java.util.Iterator<Block> getSuccessIterator() {
      return (this.success == null) ? null : this.success.iterator();
    }

    public void addToSuccess(Block elem) {
      if (this.success == null) {
        this.success = new ArrayList<Block>();
      }
      this.success.add(elem);
    }

    public List<Block> getSuccess() {
      return this.success;
    }

    public getBlocks_result setSuccess(List<Block> success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public getBlocks_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((List<Block>)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getBlocks_result)
        return this.equals((getBlocks_result)that);
      return false;
    }

    public boolean equals(getBlocks_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getBlocks_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getBlocks_result typedOther = (getBlocks_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.LIST) {
              {
                TList _list8 = iprot.readListBegin();
                this.success = new ArrayList<Block>(_list8.size);
                for (int _i9 = 0; _i9 < _list8.size; ++_i9)
                {
                  Block _elem10;
                  _elem10 = new Block();
                  _elem10.read(iprot);
                  this.success.add(_elem10);
                }
                iprot.readListEnd();
              }
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        {
          oprot.writeListBegin(new TList(TType.STRUCT, this.success.size()));
          for (Block _iter11 : this.success)
          {
            _iter11.write(oprot);
          }
          oprot.writeListEnd();
        }
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getBlocks_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getPreferredBlockSize_args implements TBase<getPreferredBlockSize_args, getPreferredBlockSize_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getPreferredBlockSize_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path to the file.
     */
    public String path;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path to the file.
       */
      PATH((short)1, "path");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getPreferredBlockSize_args.class, metaDataMap);
    }

    public getPreferredBlockSize_args() {
    }

    public getPreferredBlockSize_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path)
    {
      this();
      this.ctx = ctx;
      this.path = path;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getPreferredBlockSize_args(getPreferredBlockSize_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
    }

    public getPreferredBlockSize_args deepCopy() {
      return new getPreferredBlockSize_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public getPreferredBlockSize_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path to the file.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path to the file.
     */
    public getPreferredBlockSize_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getPreferredBlockSize_args)
        return this.equals((getPreferredBlockSize_args)that);
      return false;
    }

    public boolean equals(getPreferredBlockSize_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getPreferredBlockSize_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getPreferredBlockSize_args typedOther = (getPreferredBlockSize_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getPreferredBlockSize_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getPreferredBlockSize_result implements TBase<getPreferredBlockSize_result, getPreferredBlockSize_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getPreferredBlockSize_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.I64, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public long success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __SUCCESS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getPreferredBlockSize_result.class, metaDataMap);
    }

    public getPreferredBlockSize_result() {
    }

    public getPreferredBlockSize_result(
      long success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      setSuccessIsSet(true);
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getPreferredBlockSize_result(getPreferredBlockSize_result other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      this.success = other.success;
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public getPreferredBlockSize_result deepCopy() {
      return new getPreferredBlockSize_result(this);
    }

    @Override
    public void clear() {
      setSuccessIsSet(false);
      this.success = 0;
      this.err = null;
    }

    public long getSuccess() {
      return this.success;
    }

    public getPreferredBlockSize_result setSuccess(long success) {
      this.success = success;
      setSuccessIsSet(true);
      return this;
    }

    public void unsetSuccess() {
      __isset_bit_vector.clear(__SUCCESS_ISSET_ID);
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return __isset_bit_vector.get(__SUCCESS_ISSET_ID);
    }

    public void setSuccessIsSet(boolean value) {
      __isset_bit_vector.set(__SUCCESS_ISSET_ID, value);
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public getPreferredBlockSize_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Long)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return new Long(getSuccess());

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getPreferredBlockSize_result)
        return this.equals((getPreferredBlockSize_result)that);
      return false;
    }

    public boolean equals(getPreferredBlockSize_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true;
      boolean that_present_success = true;
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (this.success != that.success)
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getPreferredBlockSize_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getPreferredBlockSize_result typedOther = (getPreferredBlockSize_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.I64) {
              this.success = iprot.readI64();
              setSuccessIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        oprot.writeI64(this.success);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getPreferredBlockSize_result(");
      boolean first = true;

      sb.append("success:");
      sb.append(this.success);
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class isInSafeMode_args implements TBase<isInSafeMode_args, isInSafeMode_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("isInSafeMode_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(isInSafeMode_args.class, metaDataMap);
    }

    public isInSafeMode_args() {
    }

    public isInSafeMode_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx)
    {
      this();
      this.ctx = ctx;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public isInSafeMode_args(isInSafeMode_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
    }

    public isInSafeMode_args deepCopy() {
      return new isInSafeMode_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public isInSafeMode_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof isInSafeMode_args)
        return this.equals((isInSafeMode_args)that);
      return false;
    }

    public boolean equals(isInSafeMode_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(isInSafeMode_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      isInSafeMode_args typedOther = (isInSafeMode_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("isInSafeMode_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class isInSafeMode_result implements TBase<isInSafeMode_result, isInSafeMode_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("isInSafeMode_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.BOOL, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public boolean success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __SUCCESS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.BOOL)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(isInSafeMode_result.class, metaDataMap);
    }

    public isInSafeMode_result() {
    }

    public isInSafeMode_result(
      boolean success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      setSuccessIsSet(true);
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public isInSafeMode_result(isInSafeMode_result other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      this.success = other.success;
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public isInSafeMode_result deepCopy() {
      return new isInSafeMode_result(this);
    }

    @Override
    public void clear() {
      setSuccessIsSet(false);
      this.success = false;
      this.err = null;
    }

    public boolean isSuccess() {
      return this.success;
    }

    public isInSafeMode_result setSuccess(boolean success) {
      this.success = success;
      setSuccessIsSet(true);
      return this;
    }

    public void unsetSuccess() {
      __isset_bit_vector.clear(__SUCCESS_ISSET_ID);
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return __isset_bit_vector.get(__SUCCESS_ISSET_ID);
    }

    public void setSuccessIsSet(boolean value) {
      __isset_bit_vector.set(__SUCCESS_ISSET_ID, value);
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public isInSafeMode_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Boolean)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return new Boolean(isSuccess());

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof isInSafeMode_result)
        return this.equals((isInSafeMode_result)that);
      return false;
    }

    public boolean equals(isInSafeMode_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true;
      boolean that_present_success = true;
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (this.success != that.success)
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(isInSafeMode_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      isInSafeMode_result typedOther = (isInSafeMode_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.BOOL) {
              this.success = iprot.readBool();
              setSuccessIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        oprot.writeBool(this.success);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("isInSafeMode_result(");
      boolean first = true;

      sb.append("success:");
      sb.append(this.success);
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class leaveSafeMode_args implements TBase<leaveSafeMode_args, leaveSafeMode_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("leaveSafeMode_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(leaveSafeMode_args.class, metaDataMap);
    }

    public leaveSafeMode_args() {
    }

    public leaveSafeMode_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx)
    {
      this();
      this.ctx = ctx;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public leaveSafeMode_args(leaveSafeMode_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
    }

    public leaveSafeMode_args deepCopy() {
      return new leaveSafeMode_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public leaveSafeMode_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof leaveSafeMode_args)
        return this.equals((leaveSafeMode_args)that);
      return false;
    }

    public boolean equals(leaveSafeMode_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(leaveSafeMode_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      leaveSafeMode_args typedOther = (leaveSafeMode_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("leaveSafeMode_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class leaveSafeMode_result implements TBase<leaveSafeMode_result, leaveSafeMode_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("leaveSafeMode_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(leaveSafeMode_result.class, metaDataMap);
    }

    public leaveSafeMode_result() {
    }

    public leaveSafeMode_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public leaveSafeMode_result(leaveSafeMode_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public leaveSafeMode_result deepCopy() {
      return new leaveSafeMode_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public leaveSafeMode_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof leaveSafeMode_result)
        return this.equals((leaveSafeMode_result)that);
      return false;
    }

    public boolean equals(leaveSafeMode_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(leaveSafeMode_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      leaveSafeMode_result typedOther = (leaveSafeMode_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("leaveSafeMode_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class ls_args implements TBase<ls_args, ls_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("ls_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path to the directory.
     */
    public String path;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path to the directory.
       */
      PATH((short)1, "path");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(ls_args.class, metaDataMap);
    }

    public ls_args() {
    }

    public ls_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path)
    {
      this();
      this.ctx = ctx;
      this.path = path;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public ls_args(ls_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
    }

    public ls_args deepCopy() {
      return new ls_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public ls_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path to the directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path to the directory.
     */
    public ls_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof ls_args)
        return this.equals((ls_args)that);
      return false;
    }

    public boolean equals(ls_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(ls_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      ls_args typedOther = (ls_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("ls_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class ls_result implements TBase<ls_result, ls_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("ls_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.LIST, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public List<Stat> success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new ListMetaData(TType.LIST, 
              new StructMetaData(TType.STRUCT, Stat.class))));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(ls_result.class, metaDataMap);
    }

    public ls_result() {
    }

    public ls_result(
      List<Stat> success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public ls_result(ls_result other) {
      if (other.isSetSuccess()) {
        List<Stat> __this__success = new ArrayList<Stat>();
        for (Stat other_element : other.success) {
          __this__success.add(new Stat(other_element));
        }
        this.success = __this__success;
      }
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public ls_result deepCopy() {
      return new ls_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
      this.err = null;
    }

    public int getSuccessSize() {
      return (this.success == null) ? 0 : this.success.size();
    }

    public java.util.Iterator<Stat> getSuccessIterator() {
      return (this.success == null) ? null : this.success.iterator();
    }

    public void addToSuccess(Stat elem) {
      if (this.success == null) {
        this.success = new ArrayList<Stat>();
      }
      this.success.add(elem);
    }

    public List<Stat> getSuccess() {
      return this.success;
    }

    public ls_result setSuccess(List<Stat> success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public ls_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((List<Stat>)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof ls_result)
        return this.equals((ls_result)that);
      return false;
    }

    public boolean equals(ls_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(ls_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      ls_result typedOther = (ls_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.LIST) {
              {
                TList _list12 = iprot.readListBegin();
                this.success = new ArrayList<Stat>(_list12.size);
                for (int _i13 = 0; _i13 < _list12.size; ++_i13)
                {
                  Stat _elem14;
                  _elem14 = new Stat();
                  _elem14.read(iprot);
                  this.success.add(_elem14);
                }
                iprot.readListEnd();
              }
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        {
          oprot.writeListBegin(new TList(TType.STRUCT, this.success.size()));
          for (Stat _iter15 : this.success)
          {
            _iter15.write(oprot);
          }
          oprot.writeListEnd();
        }
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("ls_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class mkdirhier_args implements TBase<mkdirhier_args, mkdirhier_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("mkdirhier_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField PERMS_FIELD_DESC = new TField("perms", TType.I16, (short)2);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path to the directory.
     */
    public String path;
    /**
     * Access permissions of the directory.
     */
    public short perms;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path to the directory.
       */
      PATH((short)1, "path"),
      /**
       * Access permissions of the directory.
       */
      PERMS((short)2, "perms");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // PERMS
            return PERMS;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __PERMS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.PERMS, new FieldMetaData("perms", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I16)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(mkdirhier_args.class, metaDataMap);
    }

    public mkdirhier_args() {
    }

    public mkdirhier_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      short perms)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.perms = perms;
      setPermsIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public mkdirhier_args(mkdirhier_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.perms = other.perms;
    }

    public mkdirhier_args deepCopy() {
      return new mkdirhier_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setPermsIsSet(false);
      this.perms = 0;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public mkdirhier_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path to the directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path to the directory.
     */
    public mkdirhier_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * Access permissions of the directory.
     */
    public short getPerms() {
      return this.perms;
    }

    /**
     * Access permissions of the directory.
     */
    public mkdirhier_args setPerms(short perms) {
      this.perms = perms;
      setPermsIsSet(true);
      return this;
    }

    public void unsetPerms() {
      __isset_bit_vector.clear(__PERMS_ISSET_ID);
    }

    /** Returns true if field perms is set (has been asigned a value) and false otherwise */
    public boolean isSetPerms() {
      return __isset_bit_vector.get(__PERMS_ISSET_ID);
    }

    public void setPermsIsSet(boolean value) {
      __isset_bit_vector.set(__PERMS_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case PERMS:
        if (value == null) {
          unsetPerms();
        } else {
          setPerms((Short)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case PERMS:
        return new Short(getPerms());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case PERMS:
        return isSetPerms();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof mkdirhier_args)
        return this.equals((mkdirhier_args)that);
      return false;
    }

    public boolean equals(mkdirhier_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_perms = true;
      boolean that_present_perms = true;
      if (this_present_perms || that_present_perms) {
        if (!(this_present_perms && that_present_perms))
          return false;
        if (this.perms != that.perms)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(mkdirhier_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      mkdirhier_args typedOther = (mkdirhier_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPerms()).compareTo(typedOther.isSetPerms());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPerms()) {
        lastComparison = TBaseHelper.compareTo(this.perms, typedOther.perms);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // PERMS
            if (field.type == TType.I16) {
              this.perms = iprot.readI16();
              setPermsIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(PERMS_FIELD_DESC);
      oprot.writeI16(this.perms);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("mkdirhier_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("perms:");
      sb.append(this.perms);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class mkdirhier_result implements TBase<mkdirhier_result, mkdirhier_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("mkdirhier_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.BOOL, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public boolean success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __SUCCESS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.BOOL)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(mkdirhier_result.class, metaDataMap);
    }

    public mkdirhier_result() {
    }

    public mkdirhier_result(
      boolean success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      setSuccessIsSet(true);
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public mkdirhier_result(mkdirhier_result other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      this.success = other.success;
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public mkdirhier_result deepCopy() {
      return new mkdirhier_result(this);
    }

    @Override
    public void clear() {
      setSuccessIsSet(false);
      this.success = false;
      this.err = null;
    }

    public boolean isSuccess() {
      return this.success;
    }

    public mkdirhier_result setSuccess(boolean success) {
      this.success = success;
      setSuccessIsSet(true);
      return this;
    }

    public void unsetSuccess() {
      __isset_bit_vector.clear(__SUCCESS_ISSET_ID);
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return __isset_bit_vector.get(__SUCCESS_ISSET_ID);
    }

    public void setSuccessIsSet(boolean value) {
      __isset_bit_vector.set(__SUCCESS_ISSET_ID, value);
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public mkdirhier_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Boolean)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return new Boolean(isSuccess());

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof mkdirhier_result)
        return this.equals((mkdirhier_result)that);
      return false;
    }

    public boolean equals(mkdirhier_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true;
      boolean that_present_success = true;
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (this.success != that.success)
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(mkdirhier_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      mkdirhier_result typedOther = (mkdirhier_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.BOOL) {
              this.success = iprot.readBool();
              setSuccessIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        oprot.writeBool(this.success);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("mkdirhier_result(");
      boolean first = true;

      sb.append("success:");
      sb.append(this.success);
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class refreshNodes_args implements TBase<refreshNodes_args, refreshNodes_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("refreshNodes_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(refreshNodes_args.class, metaDataMap);
    }

    public refreshNodes_args() {
    }

    public refreshNodes_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx)
    {
      this();
      this.ctx = ctx;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public refreshNodes_args(refreshNodes_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
    }

    public refreshNodes_args deepCopy() {
      return new refreshNodes_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public refreshNodes_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof refreshNodes_args)
        return this.equals((refreshNodes_args)that);
      return false;
    }

    public boolean equals(refreshNodes_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(refreshNodes_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      refreshNodes_args typedOther = (refreshNodes_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("refreshNodes_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class refreshNodes_result implements TBase<refreshNodes_result, refreshNodes_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("refreshNodes_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(refreshNodes_result.class, metaDataMap);
    }

    public refreshNodes_result() {
    }

    public refreshNodes_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public refreshNodes_result(refreshNodes_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public refreshNodes_result deepCopy() {
      return new refreshNodes_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public refreshNodes_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof refreshNodes_result)
        return this.equals((refreshNodes_result)that);
      return false;
    }

    public boolean equals(refreshNodes_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(refreshNodes_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      refreshNodes_result typedOther = (refreshNodes_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("refreshNodes_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class rename_args implements TBase<rename_args, rename_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("rename_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField NEW_PATH_FIELD_DESC = new TField("newPath", TType.STRING, (short)2);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path to existing file or directory.
     */
    public String path;
    /**
     * New path.
     */
    public String newPath;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path to existing file or directory.
       */
      PATH((short)1, "path"),
      /**
       * New path.
       */
      NEW_PATH((short)2, "newPath");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // NEW_PATH
            return NEW_PATH;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.NEW_PATH, new FieldMetaData("newPath", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(rename_args.class, metaDataMap);
    }

    public rename_args() {
    }

    public rename_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      String newPath)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.newPath = newPath;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public rename_args(rename_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      if (other.isSetNewPath()) {
        this.newPath = other.newPath;
      }
    }

    public rename_args deepCopy() {
      return new rename_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      this.newPath = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public rename_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path to existing file or directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path to existing file or directory.
     */
    public rename_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * New path.
     */
    public String getNewPath() {
      return this.newPath;
    }

    /**
     * New path.
     */
    public rename_args setNewPath(String newPath) {
      this.newPath = newPath;
      return this;
    }

    public void unsetNewPath() {
      this.newPath = null;
    }

    /** Returns true if field newPath is set (has been asigned a value) and false otherwise */
    public boolean isSetNewPath() {
      return this.newPath != null;
    }

    public void setNewPathIsSet(boolean value) {
      if (!value) {
        this.newPath = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case NEW_PATH:
        if (value == null) {
          unsetNewPath();
        } else {
          setNewPath((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case NEW_PATH:
        return getNewPath();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case NEW_PATH:
        return isSetNewPath();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof rename_args)
        return this.equals((rename_args)that);
      return false;
    }

    public boolean equals(rename_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_newPath = true && this.isSetNewPath();
      boolean that_present_newPath = true && that.isSetNewPath();
      if (this_present_newPath || that_present_newPath) {
        if (!(this_present_newPath && that_present_newPath))
          return false;
        if (!this.newPath.equals(that.newPath))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(rename_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      rename_args typedOther = (rename_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetNewPath()).compareTo(typedOther.isSetNewPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetNewPath()) {
        lastComparison = TBaseHelper.compareTo(this.newPath, typedOther.newPath);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // NEW_PATH
            if (field.type == TType.STRING) {
              this.newPath = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      if (this.newPath != null) {
        oprot.writeFieldBegin(NEW_PATH_FIELD_DESC);
        oprot.writeString(this.newPath);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("rename_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("newPath:");
      if (this.newPath == null) {
        sb.append("null");
      } else {
        sb.append(this.newPath);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class rename_result implements TBase<rename_result, rename_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("rename_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.BOOL, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public boolean success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __SUCCESS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.BOOL)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(rename_result.class, metaDataMap);
    }

    public rename_result() {
    }

    public rename_result(
      boolean success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      setSuccessIsSet(true);
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public rename_result(rename_result other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      this.success = other.success;
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public rename_result deepCopy() {
      return new rename_result(this);
    }

    @Override
    public void clear() {
      setSuccessIsSet(false);
      this.success = false;
      this.err = null;
    }

    public boolean isSuccess() {
      return this.success;
    }

    public rename_result setSuccess(boolean success) {
      this.success = success;
      setSuccessIsSet(true);
      return this;
    }

    public void unsetSuccess() {
      __isset_bit_vector.clear(__SUCCESS_ISSET_ID);
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return __isset_bit_vector.get(__SUCCESS_ISSET_ID);
    }

    public void setSuccessIsSet(boolean value) {
      __isset_bit_vector.set(__SUCCESS_ISSET_ID, value);
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public rename_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Boolean)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return new Boolean(isSuccess());

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof rename_result)
        return this.equals((rename_result)that);
      return false;
    }

    public boolean equals(rename_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true;
      boolean that_present_success = true;
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (this.success != that.success)
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(rename_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      rename_result typedOther = (rename_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.BOOL) {
              this.success = iprot.readBool();
              setSuccessIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        oprot.writeBool(this.success);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("rename_result(");
      boolean first = true;

      sb.append("success:");
      sb.append(this.success);
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class reportBadBlocks_args implements TBase<reportBadBlocks_args, reportBadBlocks_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("reportBadBlocks_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField BLOCKS_FIELD_DESC = new TField("blocks", TType.LIST, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * List of corrupted blocks.
     */
    public List<Block> blocks;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * List of corrupted blocks.
       */
      BLOCKS((short)1, "blocks");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // BLOCKS
            return BLOCKS;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.BLOCKS, new FieldMetaData("blocks", TFieldRequirementType.DEFAULT, 
          new ListMetaData(TType.LIST, 
              new StructMetaData(TType.STRUCT, Block.class))));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(reportBadBlocks_args.class, metaDataMap);
    }

    public reportBadBlocks_args() {
    }

    public reportBadBlocks_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      List<Block> blocks)
    {
      this();
      this.ctx = ctx;
      this.blocks = blocks;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public reportBadBlocks_args(reportBadBlocks_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetBlocks()) {
        List<Block> __this__blocks = new ArrayList<Block>();
        for (Block other_element : other.blocks) {
          __this__blocks.add(new Block(other_element));
        }
        this.blocks = __this__blocks;
      }
    }

    public reportBadBlocks_args deepCopy() {
      return new reportBadBlocks_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.blocks = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public reportBadBlocks_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public int getBlocksSize() {
      return (this.blocks == null) ? 0 : this.blocks.size();
    }

    public java.util.Iterator<Block> getBlocksIterator() {
      return (this.blocks == null) ? null : this.blocks.iterator();
    }

    public void addToBlocks(Block elem) {
      if (this.blocks == null) {
        this.blocks = new ArrayList<Block>();
      }
      this.blocks.add(elem);
    }

    /**
     * List of corrupted blocks.
     */
    public List<Block> getBlocks() {
      return this.blocks;
    }

    /**
     * List of corrupted blocks.
     */
    public reportBadBlocks_args setBlocks(List<Block> blocks) {
      this.blocks = blocks;
      return this;
    }

    public void unsetBlocks() {
      this.blocks = null;
    }

    /** Returns true if field blocks is set (has been asigned a value) and false otherwise */
    public boolean isSetBlocks() {
      return this.blocks != null;
    }

    public void setBlocksIsSet(boolean value) {
      if (!value) {
        this.blocks = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case BLOCKS:
        if (value == null) {
          unsetBlocks();
        } else {
          setBlocks((List<Block>)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case BLOCKS:
        return getBlocks();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case BLOCKS:
        return isSetBlocks();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof reportBadBlocks_args)
        return this.equals((reportBadBlocks_args)that);
      return false;
    }

    public boolean equals(reportBadBlocks_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_blocks = true && this.isSetBlocks();
      boolean that_present_blocks = true && that.isSetBlocks();
      if (this_present_blocks || that_present_blocks) {
        if (!(this_present_blocks && that_present_blocks))
          return false;
        if (!this.blocks.equals(that.blocks))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(reportBadBlocks_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      reportBadBlocks_args typedOther = (reportBadBlocks_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetBlocks()).compareTo(typedOther.isSetBlocks());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetBlocks()) {
        lastComparison = TBaseHelper.compareTo(this.blocks, typedOther.blocks);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // BLOCKS
            if (field.type == TType.LIST) {
              {
                TList _list16 = iprot.readListBegin();
                this.blocks = new ArrayList<Block>(_list16.size);
                for (int _i17 = 0; _i17 < _list16.size; ++_i17)
                {
                  Block _elem18;
                  _elem18 = new Block();
                  _elem18.read(iprot);
                  this.blocks.add(_elem18);
                }
                iprot.readListEnd();
              }
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.blocks != null) {
        oprot.writeFieldBegin(BLOCKS_FIELD_DESC);
        {
          oprot.writeListBegin(new TList(TType.STRUCT, this.blocks.size()));
          for (Block _iter19 : this.blocks)
          {
            _iter19.write(oprot);
          }
          oprot.writeListEnd();
        }
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("reportBadBlocks_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("blocks:");
      if (this.blocks == null) {
        sb.append("null");
      } else {
        sb.append(this.blocks);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class reportBadBlocks_result implements TBase<reportBadBlocks_result, reportBadBlocks_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("reportBadBlocks_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(reportBadBlocks_result.class, metaDataMap);
    }

    public reportBadBlocks_result() {
    }

    public reportBadBlocks_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public reportBadBlocks_result(reportBadBlocks_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public reportBadBlocks_result deepCopy() {
      return new reportBadBlocks_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public reportBadBlocks_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof reportBadBlocks_result)
        return this.equals((reportBadBlocks_result)that);
      return false;
    }

    public boolean equals(reportBadBlocks_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(reportBadBlocks_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      reportBadBlocks_result typedOther = (reportBadBlocks_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("reportBadBlocks_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class stat_args implements TBase<stat_args, stat_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("stat_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path of the file or directory.
     */
    public String path;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path of the file or directory.
       */
      PATH((short)1, "path");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(stat_args.class, metaDataMap);
    }

    public stat_args() {
    }

    public stat_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path)
    {
      this();
      this.ctx = ctx;
      this.path = path;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public stat_args(stat_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
    }

    public stat_args deepCopy() {
      return new stat_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public stat_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path of the file or directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path of the file or directory.
     */
    public stat_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof stat_args)
        return this.equals((stat_args)that);
      return false;
    }

    public boolean equals(stat_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(stat_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      stat_args typedOther = (stat_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("stat_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class stat_result implements TBase<stat_result, stat_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("stat_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.STRUCT, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public Stat success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, Stat.class)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(stat_result.class, metaDataMap);
    }

    public stat_result() {
    }

    public stat_result(
      Stat success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public stat_result(stat_result other) {
      if (other.isSetSuccess()) {
        this.success = new Stat(other.success);
      }
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public stat_result deepCopy() {
      return new stat_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
      this.err = null;
    }

    public Stat getSuccess() {
      return this.success;
    }

    public stat_result setSuccess(Stat success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public stat_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Stat)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof stat_result)
        return this.equals((stat_result)that);
      return false;
    }

    public boolean equals(stat_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(stat_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      stat_result typedOther = (stat_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.STRUCT) {
              this.success = new Stat();
              this.success.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        this.success.write(oprot);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("stat_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getContentSummary_args implements TBase<getContentSummary_args, getContentSummary_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getContentSummary_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("Path", TType.STRING, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    public String Path;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      PATH((short)1, "Path");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("Path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getContentSummary_args.class, metaDataMap);
    }

    public getContentSummary_args() {
    }

    public getContentSummary_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String Path)
    {
      this();
      this.ctx = ctx;
      this.Path = Path;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getContentSummary_args(getContentSummary_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.Path = other.Path;
      }
    }

    public getContentSummary_args deepCopy() {
      return new getContentSummary_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.Path = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public getContentSummary_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public String getPath() {
      return this.Path;
    }

    public getContentSummary_args setPath(String Path) {
      this.Path = Path;
      return this;
    }

    public void unsetPath() {
      this.Path = null;
    }

    /** Returns true if field Path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.Path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.Path = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getContentSummary_args)
        return this.equals((getContentSummary_args)that);
      return false;
    }

    public boolean equals(getContentSummary_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_Path = true && this.isSetPath();
      boolean that_present_Path = true && that.isSetPath();
      if (this_present_Path || that_present_Path) {
        if (!(this_present_Path && that_present_Path))
          return false;
        if (!this.Path.equals(that.Path))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getContentSummary_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getContentSummary_args typedOther = (getContentSummary_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.Path, typedOther.Path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.Path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.Path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.Path);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getContentSummary_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("Path:");
      if (this.Path == null) {
        sb.append("null");
      } else {
        sb.append(this.Path);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getContentSummary_result implements TBase<getContentSummary_result, getContentSummary_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getContentSummary_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.STRUCT, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public ContentSummary success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, ContentSummary.class)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getContentSummary_result.class, metaDataMap);
    }

    public getContentSummary_result() {
    }

    public getContentSummary_result(
      ContentSummary success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getContentSummary_result(getContentSummary_result other) {
      if (other.isSetSuccess()) {
        this.success = new ContentSummary(other.success);
      }
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public getContentSummary_result deepCopy() {
      return new getContentSummary_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
      this.err = null;
    }

    public ContentSummary getSuccess() {
      return this.success;
    }

    public getContentSummary_result setSuccess(ContentSummary success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public getContentSummary_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((ContentSummary)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getContentSummary_result)
        return this.equals((getContentSummary_result)that);
      return false;
    }

    public boolean equals(getContentSummary_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getContentSummary_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getContentSummary_result typedOther = (getContentSummary_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.STRUCT) {
              this.success = new ContentSummary();
              this.success.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        this.success.write(oprot);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getContentSummary_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class multiGetContentSummary_args implements TBase<multiGetContentSummary_args, multiGetContentSummary_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("multiGetContentSummary_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATHS_FIELD_DESC = new TField("paths", TType.LIST, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    public List<String> paths;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      PATHS((short)1, "paths");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATHS
            return PATHS;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATHS, new FieldMetaData("paths", TFieldRequirementType.DEFAULT, 
          new ListMetaData(TType.LIST, 
              new FieldValueMetaData(TType.STRING))));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(multiGetContentSummary_args.class, metaDataMap);
    }

    public multiGetContentSummary_args() {
    }

    public multiGetContentSummary_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      List<String> paths)
    {
      this();
      this.ctx = ctx;
      this.paths = paths;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public multiGetContentSummary_args(multiGetContentSummary_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPaths()) {
        List<String> __this__paths = new ArrayList<String>();
        for (String other_element : other.paths) {
          __this__paths.add(other_element);
        }
        this.paths = __this__paths;
      }
    }

    public multiGetContentSummary_args deepCopy() {
      return new multiGetContentSummary_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.paths = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public multiGetContentSummary_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public int getPathsSize() {
      return (this.paths == null) ? 0 : this.paths.size();
    }

    public java.util.Iterator<String> getPathsIterator() {
      return (this.paths == null) ? null : this.paths.iterator();
    }

    public void addToPaths(String elem) {
      if (this.paths == null) {
        this.paths = new ArrayList<String>();
      }
      this.paths.add(elem);
    }

    public List<String> getPaths() {
      return this.paths;
    }

    public multiGetContentSummary_args setPaths(List<String> paths) {
      this.paths = paths;
      return this;
    }

    public void unsetPaths() {
      this.paths = null;
    }

    /** Returns true if field paths is set (has been asigned a value) and false otherwise */
    public boolean isSetPaths() {
      return this.paths != null;
    }

    public void setPathsIsSet(boolean value) {
      if (!value) {
        this.paths = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATHS:
        if (value == null) {
          unsetPaths();
        } else {
          setPaths((List<String>)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATHS:
        return getPaths();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATHS:
        return isSetPaths();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof multiGetContentSummary_args)
        return this.equals((multiGetContentSummary_args)that);
      return false;
    }

    public boolean equals(multiGetContentSummary_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_paths = true && this.isSetPaths();
      boolean that_present_paths = true && that.isSetPaths();
      if (this_present_paths || that_present_paths) {
        if (!(this_present_paths && that_present_paths))
          return false;
        if (!this.paths.equals(that.paths))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(multiGetContentSummary_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      multiGetContentSummary_args typedOther = (multiGetContentSummary_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPaths()).compareTo(typedOther.isSetPaths());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPaths()) {
        lastComparison = TBaseHelper.compareTo(this.paths, typedOther.paths);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATHS
            if (field.type == TType.LIST) {
              {
                TList _list20 = iprot.readListBegin();
                this.paths = new ArrayList<String>(_list20.size);
                for (int _i21 = 0; _i21 < _list20.size; ++_i21)
                {
                  String _elem22;
                  _elem22 = iprot.readString();
                  this.paths.add(_elem22);
                }
                iprot.readListEnd();
              }
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.paths != null) {
        oprot.writeFieldBegin(PATHS_FIELD_DESC);
        {
          oprot.writeListBegin(new TList(TType.STRING, this.paths.size()));
          for (String _iter23 : this.paths)
          {
            oprot.writeString(_iter23);
          }
          oprot.writeListEnd();
        }
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("multiGetContentSummary_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("paths:");
      if (this.paths == null) {
        sb.append("null");
      } else {
        sb.append(this.paths);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class multiGetContentSummary_result implements TBase<multiGetContentSummary_result, multiGetContentSummary_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("multiGetContentSummary_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.LIST, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public List<ContentSummary> success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new ListMetaData(TType.LIST, 
              new StructMetaData(TType.STRUCT, ContentSummary.class))));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(multiGetContentSummary_result.class, metaDataMap);
    }

    public multiGetContentSummary_result() {
    }

    public multiGetContentSummary_result(
      List<ContentSummary> success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public multiGetContentSummary_result(multiGetContentSummary_result other) {
      if (other.isSetSuccess()) {
        List<ContentSummary> __this__success = new ArrayList<ContentSummary>();
        for (ContentSummary other_element : other.success) {
          __this__success.add(new ContentSummary(other_element));
        }
        this.success = __this__success;
      }
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public multiGetContentSummary_result deepCopy() {
      return new multiGetContentSummary_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
      this.err = null;
    }

    public int getSuccessSize() {
      return (this.success == null) ? 0 : this.success.size();
    }

    public java.util.Iterator<ContentSummary> getSuccessIterator() {
      return (this.success == null) ? null : this.success.iterator();
    }

    public void addToSuccess(ContentSummary elem) {
      if (this.success == null) {
        this.success = new ArrayList<ContentSummary>();
      }
      this.success.add(elem);
    }

    public List<ContentSummary> getSuccess() {
      return this.success;
    }

    public multiGetContentSummary_result setSuccess(List<ContentSummary> success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public multiGetContentSummary_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((List<ContentSummary>)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof multiGetContentSummary_result)
        return this.equals((multiGetContentSummary_result)that);
      return false;
    }

    public boolean equals(multiGetContentSummary_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(multiGetContentSummary_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      multiGetContentSummary_result typedOther = (multiGetContentSummary_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.LIST) {
              {
                TList _list24 = iprot.readListBegin();
                this.success = new ArrayList<ContentSummary>(_list24.size);
                for (int _i25 = 0; _i25 < _list24.size; ++_i25)
                {
                  ContentSummary _elem26;
                  _elem26 = new ContentSummary();
                  _elem26.read(iprot);
                  this.success.add(_elem26);
                }
                iprot.readListEnd();
              }
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        {
          oprot.writeListBegin(new TList(TType.STRUCT, this.success.size()));
          for (ContentSummary _iter27 : this.success)
          {
            _iter27.write(oprot);
          }
          oprot.writeListEnd();
        }
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("multiGetContentSummary_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class setQuota_args implements TBase<setQuota_args, setQuota_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("setQuota_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField NAMESPACE_QUOTA_FIELD_DESC = new TField("namespaceQuota", TType.I64, (short)2);
    private static final TField DISKSPACE_QUOTA_FIELD_DESC = new TField("diskspaceQuota", TType.I64, (short)3);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path of the directory.
     */
    public String path;
    /**
     * Limit on the number of names in the directory.
     */
    public long namespaceQuota;
    /**
     * Limit on disk space occupied by all the files in the
     * directory.
     */
    public long diskspaceQuota;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path of the directory.
       */
      PATH((short)1, "path"),
      /**
       * Limit on the number of names in the directory.
       */
      NAMESPACE_QUOTA((short)2, "namespaceQuota"),
      /**
       * Limit on disk space occupied by all the files in the
       * directory.
       */
      DISKSPACE_QUOTA((short)3, "diskspaceQuota");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // NAMESPACE_QUOTA
            return NAMESPACE_QUOTA;
          case 3: // DISKSPACE_QUOTA
            return DISKSPACE_QUOTA;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __NAMESPACEQUOTA_ISSET_ID = 0;
    private static final int __DISKSPACEQUOTA_ISSET_ID = 1;
    private BitSet __isset_bit_vector = new BitSet(2);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.NAMESPACE_QUOTA, new FieldMetaData("namespaceQuota", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      tmpMap.put(_Fields.DISKSPACE_QUOTA, new FieldMetaData("diskspaceQuota", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(setQuota_args.class, metaDataMap);
    }

    public setQuota_args() {
    }

    public setQuota_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      long namespaceQuota,
      long diskspaceQuota)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.namespaceQuota = namespaceQuota;
      setNamespaceQuotaIsSet(true);
      this.diskspaceQuota = diskspaceQuota;
      setDiskspaceQuotaIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public setQuota_args(setQuota_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.namespaceQuota = other.namespaceQuota;
      this.diskspaceQuota = other.diskspaceQuota;
    }

    public setQuota_args deepCopy() {
      return new setQuota_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setNamespaceQuotaIsSet(false);
      this.namespaceQuota = 0;
      setDiskspaceQuotaIsSet(false);
      this.diskspaceQuota = 0;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public setQuota_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path of the directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path of the directory.
     */
    public setQuota_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * Limit on the number of names in the directory.
     */
    public long getNamespaceQuota() {
      return this.namespaceQuota;
    }

    /**
     * Limit on the number of names in the directory.
     */
    public setQuota_args setNamespaceQuota(long namespaceQuota) {
      this.namespaceQuota = namespaceQuota;
      setNamespaceQuotaIsSet(true);
      return this;
    }

    public void unsetNamespaceQuota() {
      __isset_bit_vector.clear(__NAMESPACEQUOTA_ISSET_ID);
    }

    /** Returns true if field namespaceQuota is set (has been asigned a value) and false otherwise */
    public boolean isSetNamespaceQuota() {
      return __isset_bit_vector.get(__NAMESPACEQUOTA_ISSET_ID);
    }

    public void setNamespaceQuotaIsSet(boolean value) {
      __isset_bit_vector.set(__NAMESPACEQUOTA_ISSET_ID, value);
    }

    /**
     * Limit on disk space occupied by all the files in the
     * directory.
     */
    public long getDiskspaceQuota() {
      return this.diskspaceQuota;
    }

    /**
     * Limit on disk space occupied by all the files in the
     * directory.
     */
    public setQuota_args setDiskspaceQuota(long diskspaceQuota) {
      this.diskspaceQuota = diskspaceQuota;
      setDiskspaceQuotaIsSet(true);
      return this;
    }

    public void unsetDiskspaceQuota() {
      __isset_bit_vector.clear(__DISKSPACEQUOTA_ISSET_ID);
    }

    /** Returns true if field diskspaceQuota is set (has been asigned a value) and false otherwise */
    public boolean isSetDiskspaceQuota() {
      return __isset_bit_vector.get(__DISKSPACEQUOTA_ISSET_ID);
    }

    public void setDiskspaceQuotaIsSet(boolean value) {
      __isset_bit_vector.set(__DISKSPACEQUOTA_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case NAMESPACE_QUOTA:
        if (value == null) {
          unsetNamespaceQuota();
        } else {
          setNamespaceQuota((Long)value);
        }
        break;

      case DISKSPACE_QUOTA:
        if (value == null) {
          unsetDiskspaceQuota();
        } else {
          setDiskspaceQuota((Long)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case NAMESPACE_QUOTA:
        return new Long(getNamespaceQuota());

      case DISKSPACE_QUOTA:
        return new Long(getDiskspaceQuota());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case NAMESPACE_QUOTA:
        return isSetNamespaceQuota();
      case DISKSPACE_QUOTA:
        return isSetDiskspaceQuota();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof setQuota_args)
        return this.equals((setQuota_args)that);
      return false;
    }

    public boolean equals(setQuota_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_namespaceQuota = true;
      boolean that_present_namespaceQuota = true;
      if (this_present_namespaceQuota || that_present_namespaceQuota) {
        if (!(this_present_namespaceQuota && that_present_namespaceQuota))
          return false;
        if (this.namespaceQuota != that.namespaceQuota)
          return false;
      }

      boolean this_present_diskspaceQuota = true;
      boolean that_present_diskspaceQuota = true;
      if (this_present_diskspaceQuota || that_present_diskspaceQuota) {
        if (!(this_present_diskspaceQuota && that_present_diskspaceQuota))
          return false;
        if (this.diskspaceQuota != that.diskspaceQuota)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(setQuota_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      setQuota_args typedOther = (setQuota_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetNamespaceQuota()).compareTo(typedOther.isSetNamespaceQuota());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetNamespaceQuota()) {
        lastComparison = TBaseHelper.compareTo(this.namespaceQuota, typedOther.namespaceQuota);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetDiskspaceQuota()).compareTo(typedOther.isSetDiskspaceQuota());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetDiskspaceQuota()) {
        lastComparison = TBaseHelper.compareTo(this.diskspaceQuota, typedOther.diskspaceQuota);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // NAMESPACE_QUOTA
            if (field.type == TType.I64) {
              this.namespaceQuota = iprot.readI64();
              setNamespaceQuotaIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 3: // DISKSPACE_QUOTA
            if (field.type == TType.I64) {
              this.diskspaceQuota = iprot.readI64();
              setDiskspaceQuotaIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(NAMESPACE_QUOTA_FIELD_DESC);
      oprot.writeI64(this.namespaceQuota);
      oprot.writeFieldEnd();
      oprot.writeFieldBegin(DISKSPACE_QUOTA_FIELD_DESC);
      oprot.writeI64(this.diskspaceQuota);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("setQuota_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("namespaceQuota:");
      sb.append(this.namespaceQuota);
      first = false;
      if (!first) sb.append(", ");
      sb.append("diskspaceQuota:");
      sb.append(this.diskspaceQuota);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class setQuota_result implements TBase<setQuota_result, setQuota_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("setQuota_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(setQuota_result.class, metaDataMap);
    }

    public setQuota_result() {
    }

    public setQuota_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public setQuota_result(setQuota_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public setQuota_result deepCopy() {
      return new setQuota_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public setQuota_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof setQuota_result)
        return this.equals((setQuota_result)that);
      return false;
    }

    public boolean equals(setQuota_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(setQuota_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      setQuota_result typedOther = (setQuota_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("setQuota_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class setReplication_args implements TBase<setReplication_args, setReplication_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("setReplication_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField REPLICATION_FIELD_DESC = new TField("replication", TType.I16, (short)2);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path of the file.
     */
    public String path;
    /**
     * New replication factor.
     */
    public short replication;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path of the file.
       */
      PATH((short)1, "path"),
      /**
       * New replication factor.
       */
      REPLICATION((short)2, "replication");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // REPLICATION
            return REPLICATION;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __REPLICATION_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.REPLICATION, new FieldMetaData("replication", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I16)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(setReplication_args.class, metaDataMap);
    }

    public setReplication_args() {
    }

    public setReplication_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      short replication)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.replication = replication;
      setReplicationIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public setReplication_args(setReplication_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.replication = other.replication;
    }

    public setReplication_args deepCopy() {
      return new setReplication_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setReplicationIsSet(false);
      this.replication = 0;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public setReplication_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path of the file.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path of the file.
     */
    public setReplication_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * New replication factor.
     */
    public short getReplication() {
      return this.replication;
    }

    /**
     * New replication factor.
     */
    public setReplication_args setReplication(short replication) {
      this.replication = replication;
      setReplicationIsSet(true);
      return this;
    }

    public void unsetReplication() {
      __isset_bit_vector.clear(__REPLICATION_ISSET_ID);
    }

    /** Returns true if field replication is set (has been asigned a value) and false otherwise */
    public boolean isSetReplication() {
      return __isset_bit_vector.get(__REPLICATION_ISSET_ID);
    }

    public void setReplicationIsSet(boolean value) {
      __isset_bit_vector.set(__REPLICATION_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case REPLICATION:
        if (value == null) {
          unsetReplication();
        } else {
          setReplication((Short)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case REPLICATION:
        return new Short(getReplication());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case REPLICATION:
        return isSetReplication();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof setReplication_args)
        return this.equals((setReplication_args)that);
      return false;
    }

    public boolean equals(setReplication_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_replication = true;
      boolean that_present_replication = true;
      if (this_present_replication || that_present_replication) {
        if (!(this_present_replication && that_present_replication))
          return false;
        if (this.replication != that.replication)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(setReplication_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      setReplication_args typedOther = (setReplication_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetReplication()).compareTo(typedOther.isSetReplication());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetReplication()) {
        lastComparison = TBaseHelper.compareTo(this.replication, typedOther.replication);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // REPLICATION
            if (field.type == TType.I16) {
              this.replication = iprot.readI16();
              setReplicationIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(REPLICATION_FIELD_DESC);
      oprot.writeI16(this.replication);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("setReplication_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("replication:");
      sb.append(this.replication);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class setReplication_result implements TBase<setReplication_result, setReplication_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("setReplication_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.BOOL, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public boolean success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __SUCCESS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.BOOL)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(setReplication_result.class, metaDataMap);
    }

    public setReplication_result() {
    }

    public setReplication_result(
      boolean success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      setSuccessIsSet(true);
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public setReplication_result(setReplication_result other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      this.success = other.success;
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public setReplication_result deepCopy() {
      return new setReplication_result(this);
    }

    @Override
    public void clear() {
      setSuccessIsSet(false);
      this.success = false;
      this.err = null;
    }

    public boolean isSuccess() {
      return this.success;
    }

    public setReplication_result setSuccess(boolean success) {
      this.success = success;
      setSuccessIsSet(true);
      return this;
    }

    public void unsetSuccess() {
      __isset_bit_vector.clear(__SUCCESS_ISSET_ID);
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return __isset_bit_vector.get(__SUCCESS_ISSET_ID);
    }

    public void setSuccessIsSet(boolean value) {
      __isset_bit_vector.set(__SUCCESS_ISSET_ID, value);
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public setReplication_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Boolean)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return new Boolean(isSuccess());

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof setReplication_result)
        return this.equals((setReplication_result)that);
      return false;
    }

    public boolean equals(setReplication_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true;
      boolean that_present_success = true;
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (this.success != that.success)
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(setReplication_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      setReplication_result typedOther = (setReplication_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.BOOL) {
              this.success = iprot.readBool();
              setSuccessIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        oprot.writeBool(this.success);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("setReplication_result(");
      boolean first = true;

      sb.append("success:");
      sb.append(this.success);
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class unlink_args implements TBase<unlink_args, unlink_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("unlink_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField RECURSIVE_FIELD_DESC = new TField("recursive", TType.BOOL, (short)2);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path of the file or directory.
     */
    public String path;
    /**
     * Delete a non-empty directory recursively.
     */
    public boolean recursive;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path of the file or directory.
       */
      PATH((short)1, "path"),
      /**
       * Delete a non-empty directory recursively.
       */
      RECURSIVE((short)2, "recursive");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // RECURSIVE
            return RECURSIVE;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __RECURSIVE_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.RECURSIVE, new FieldMetaData("recursive", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.BOOL)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(unlink_args.class, metaDataMap);
    }

    public unlink_args() {
    }

    public unlink_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      boolean recursive)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.recursive = recursive;
      setRecursiveIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public unlink_args(unlink_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.recursive = other.recursive;
    }

    public unlink_args deepCopy() {
      return new unlink_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setRecursiveIsSet(false);
      this.recursive = false;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public unlink_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path of the file or directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path of the file or directory.
     */
    public unlink_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * Delete a non-empty directory recursively.
     */
    public boolean isRecursive() {
      return this.recursive;
    }

    /**
     * Delete a non-empty directory recursively.
     */
    public unlink_args setRecursive(boolean recursive) {
      this.recursive = recursive;
      setRecursiveIsSet(true);
      return this;
    }

    public void unsetRecursive() {
      __isset_bit_vector.clear(__RECURSIVE_ISSET_ID);
    }

    /** Returns true if field recursive is set (has been asigned a value) and false otherwise */
    public boolean isSetRecursive() {
      return __isset_bit_vector.get(__RECURSIVE_ISSET_ID);
    }

    public void setRecursiveIsSet(boolean value) {
      __isset_bit_vector.set(__RECURSIVE_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case RECURSIVE:
        if (value == null) {
          unsetRecursive();
        } else {
          setRecursive((Boolean)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case RECURSIVE:
        return new Boolean(isRecursive());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case RECURSIVE:
        return isSetRecursive();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof unlink_args)
        return this.equals((unlink_args)that);
      return false;
    }

    public boolean equals(unlink_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_recursive = true;
      boolean that_present_recursive = true;
      if (this_present_recursive || that_present_recursive) {
        if (!(this_present_recursive && that_present_recursive))
          return false;
        if (this.recursive != that.recursive)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(unlink_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      unlink_args typedOther = (unlink_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetRecursive()).compareTo(typedOther.isSetRecursive());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetRecursive()) {
        lastComparison = TBaseHelper.compareTo(this.recursive, typedOther.recursive);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // RECURSIVE
            if (field.type == TType.BOOL) {
              this.recursive = iprot.readBool();
              setRecursiveIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(RECURSIVE_FIELD_DESC);
      oprot.writeBool(this.recursive);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("unlink_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("recursive:");
      sb.append(this.recursive);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class unlink_result implements TBase<unlink_result, unlink_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("unlink_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.BOOL, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public boolean success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __SUCCESS_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.BOOL)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(unlink_result.class, metaDataMap);
    }

    public unlink_result() {
    }

    public unlink_result(
      boolean success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      setSuccessIsSet(true);
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public unlink_result(unlink_result other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      this.success = other.success;
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public unlink_result deepCopy() {
      return new unlink_result(this);
    }

    @Override
    public void clear() {
      setSuccessIsSet(false);
      this.success = false;
      this.err = null;
    }

    public boolean isSuccess() {
      return this.success;
    }

    public unlink_result setSuccess(boolean success) {
      this.success = success;
      setSuccessIsSet(true);
      return this;
    }

    public void unsetSuccess() {
      __isset_bit_vector.clear(__SUCCESS_ISSET_ID);
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return __isset_bit_vector.get(__SUCCESS_ISSET_ID);
    }

    public void setSuccessIsSet(boolean value) {
      __isset_bit_vector.set(__SUCCESS_ISSET_ID, value);
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public unlink_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((Boolean)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return new Boolean(isSuccess());

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof unlink_result)
        return this.equals((unlink_result)that);
      return false;
    }

    public boolean equals(unlink_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true;
      boolean that_present_success = true;
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (this.success != that.success)
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(unlink_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      unlink_result typedOther = (unlink_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.BOOL) {
              this.success = iprot.readBool();
              setSuccessIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        oprot.writeBool(this.success);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("unlink_result(");
      boolean first = true;

      sb.append("success:");
      sb.append(this.success);
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class utime_args implements TBase<utime_args, utime_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("utime_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField PATH_FIELD_DESC = new TField("path", TType.STRING, (short)1);
    private static final TField ATIME_FIELD_DESC = new TField("atime", TType.I64, (short)2);
    private static final TField MTIME_FIELD_DESC = new TField("mtime", TType.I64, (short)3);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    /**
     * Path of the file or directory.
     */
    public String path;
    /**
     * Access time in milliseconds since 1970-01-01 00:00 UTC
     */
    public long atime;
    /**
     * Modification time in milliseconds since 1970-01-01 00:00 UTC
     */
    public long mtime;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      /**
       * Path of the file or directory.
       */
      PATH((short)1, "path"),
      /**
       * Access time in milliseconds since 1970-01-01 00:00 UTC
       */
      ATIME((short)2, "atime"),
      /**
       * Modification time in milliseconds since 1970-01-01 00:00 UTC
       */
      MTIME((short)3, "mtime");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // PATH
            return PATH;
          case 2: // ATIME
            return ATIME;
          case 3: // MTIME
            return MTIME;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __ATIME_ISSET_ID = 0;
    private static final int __MTIME_ISSET_ID = 1;
    private BitSet __isset_bit_vector = new BitSet(2);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.PATH, new FieldMetaData("path", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.ATIME, new FieldMetaData("atime", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      tmpMap.put(_Fields.MTIME, new FieldMetaData("mtime", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I64)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(utime_args.class, metaDataMap);
    }

    public utime_args() {
    }

    public utime_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String path,
      long atime,
      long mtime)
    {
      this();
      this.ctx = ctx;
      this.path = path;
      this.atime = atime;
      setAtimeIsSet(true);
      this.mtime = mtime;
      setMtimeIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public utime_args(utime_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetPath()) {
        this.path = other.path;
      }
      this.atime = other.atime;
      this.mtime = other.mtime;
    }

    public utime_args deepCopy() {
      return new utime_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.path = null;
      setAtimeIsSet(false);
      this.atime = 0;
      setMtimeIsSet(false);
      this.mtime = 0;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public utime_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    /**
     * Path of the file or directory.
     */
    public String getPath() {
      return this.path;
    }

    /**
     * Path of the file or directory.
     */
    public utime_args setPath(String path) {
      this.path = path;
      return this;
    }

    public void unsetPath() {
      this.path = null;
    }

    /** Returns true if field path is set (has been asigned a value) and false otherwise */
    public boolean isSetPath() {
      return this.path != null;
    }

    public void setPathIsSet(boolean value) {
      if (!value) {
        this.path = null;
      }
    }

    /**
     * Access time in milliseconds since 1970-01-01 00:00 UTC
     */
    public long getAtime() {
      return this.atime;
    }

    /**
     * Access time in milliseconds since 1970-01-01 00:00 UTC
     */
    public utime_args setAtime(long atime) {
      this.atime = atime;
      setAtimeIsSet(true);
      return this;
    }

    public void unsetAtime() {
      __isset_bit_vector.clear(__ATIME_ISSET_ID);
    }

    /** Returns true if field atime is set (has been asigned a value) and false otherwise */
    public boolean isSetAtime() {
      return __isset_bit_vector.get(__ATIME_ISSET_ID);
    }

    public void setAtimeIsSet(boolean value) {
      __isset_bit_vector.set(__ATIME_ISSET_ID, value);
    }

    /**
     * Modification time in milliseconds since 1970-01-01 00:00 UTC
     */
    public long getMtime() {
      return this.mtime;
    }

    /**
     * Modification time in milliseconds since 1970-01-01 00:00 UTC
     */
    public utime_args setMtime(long mtime) {
      this.mtime = mtime;
      setMtimeIsSet(true);
      return this;
    }

    public void unsetMtime() {
      __isset_bit_vector.clear(__MTIME_ISSET_ID);
    }

    /** Returns true if field mtime is set (has been asigned a value) and false otherwise */
    public boolean isSetMtime() {
      return __isset_bit_vector.get(__MTIME_ISSET_ID);
    }

    public void setMtimeIsSet(boolean value) {
      __isset_bit_vector.set(__MTIME_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case PATH:
        if (value == null) {
          unsetPath();
        } else {
          setPath((String)value);
        }
        break;

      case ATIME:
        if (value == null) {
          unsetAtime();
        } else {
          setAtime((Long)value);
        }
        break;

      case MTIME:
        if (value == null) {
          unsetMtime();
        } else {
          setMtime((Long)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case PATH:
        return getPath();

      case ATIME:
        return new Long(getAtime());

      case MTIME:
        return new Long(getMtime());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case PATH:
        return isSetPath();
      case ATIME:
        return isSetAtime();
      case MTIME:
        return isSetMtime();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof utime_args)
        return this.equals((utime_args)that);
      return false;
    }

    public boolean equals(utime_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_path = true && this.isSetPath();
      boolean that_present_path = true && that.isSetPath();
      if (this_present_path || that_present_path) {
        if (!(this_present_path && that_present_path))
          return false;
        if (!this.path.equals(that.path))
          return false;
      }

      boolean this_present_atime = true;
      boolean that_present_atime = true;
      if (this_present_atime || that_present_atime) {
        if (!(this_present_atime && that_present_atime))
          return false;
        if (this.atime != that.atime)
          return false;
      }

      boolean this_present_mtime = true;
      boolean that_present_mtime = true;
      if (this_present_mtime || that_present_mtime) {
        if (!(this_present_mtime && that_present_mtime))
          return false;
        if (this.mtime != that.mtime)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(utime_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      utime_args typedOther = (utime_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetPath()).compareTo(typedOther.isSetPath());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetPath()) {
        lastComparison = TBaseHelper.compareTo(this.path, typedOther.path);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetAtime()).compareTo(typedOther.isSetAtime());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetAtime()) {
        lastComparison = TBaseHelper.compareTo(this.atime, typedOther.atime);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetMtime()).compareTo(typedOther.isSetMtime());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetMtime()) {
        lastComparison = TBaseHelper.compareTo(this.mtime, typedOther.mtime);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // PATH
            if (field.type == TType.STRING) {
              this.path = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // ATIME
            if (field.type == TType.I64) {
              this.atime = iprot.readI64();
              setAtimeIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 3: // MTIME
            if (field.type == TType.I64) {
              this.mtime = iprot.readI64();
              setMtimeIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.path != null) {
        oprot.writeFieldBegin(PATH_FIELD_DESC);
        oprot.writeString(this.path);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(ATIME_FIELD_DESC);
      oprot.writeI64(this.atime);
      oprot.writeFieldEnd();
      oprot.writeFieldBegin(MTIME_FIELD_DESC);
      oprot.writeI64(this.mtime);
      oprot.writeFieldEnd();
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("utime_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("path:");
      if (this.path == null) {
        sb.append("null");
      } else {
        sb.append(this.path);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("atime:");
      sb.append(this.atime);
      first = false;
      if (!first) sb.append(", ");
      sb.append("mtime:");
      sb.append(this.mtime);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class utime_result implements TBase<utime_result, utime_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("utime_result");

    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(utime_result.class, metaDataMap);
    }

    public utime_result() {
    }

    public utime_result(
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public utime_result(utime_result other) {
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public utime_result deepCopy() {
      return new utime_result(this);
    }

    @Override
    public void clear() {
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public utime_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof utime_result)
        return this.equals((utime_result)that);
      return false;
    }

    public boolean equals(utime_result that) {
      if (that == null)
        return false;

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(utime_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      utime_result typedOther = (utime_result)other;

      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("utime_result(");
      boolean first = true;

      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class datanodeUp_args implements TBase<datanodeUp_args, datanodeUp_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("datanodeUp_args");

    private static final TField NAME_FIELD_DESC = new TField("name", TType.STRING, (short)1);
    private static final TField STORAGE_FIELD_DESC = new TField("storage", TType.STRING, (short)2);
    private static final TField THRIFT_PORT_FIELD_DESC = new TField("thriftPort", TType.I32, (short)3);

    /**
     * <host name>:<port number> of the datanode
     */
    public String name;
    /**
     * the storage id of the datanode
     */
    public String storage;
    /**
     * Thrift port of the datanode
     */
    public int thriftPort;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      /**
       * <host name>:<port number> of the datanode
       */
      NAME((short)1, "name"),
      /**
       * the storage id of the datanode
       */
      STORAGE((short)2, "storage"),
      /**
       * Thrift port of the datanode
       */
      THRIFT_PORT((short)3, "thriftPort");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // NAME
            return NAME;
          case 2: // STORAGE
            return STORAGE;
          case 3: // THRIFT_PORT
            return THRIFT_PORT;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __THRIFTPORT_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.NAME, new FieldMetaData("name", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.STORAGE, new FieldMetaData("storage", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.THRIFT_PORT, new FieldMetaData("thriftPort", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I32)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(datanodeUp_args.class, metaDataMap);
    }

    public datanodeUp_args() {
    }

    public datanodeUp_args(
      String name,
      String storage,
      int thriftPort)
    {
      this();
      this.name = name;
      this.storage = storage;
      this.thriftPort = thriftPort;
      setThriftPortIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public datanodeUp_args(datanodeUp_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetName()) {
        this.name = other.name;
      }
      if (other.isSetStorage()) {
        this.storage = other.storage;
      }
      this.thriftPort = other.thriftPort;
    }

    public datanodeUp_args deepCopy() {
      return new datanodeUp_args(this);
    }

    @Override
    public void clear() {
      this.name = null;
      this.storage = null;
      setThriftPortIsSet(false);
      this.thriftPort = 0;
    }

    /**
     * <host name>:<port number> of the datanode
     */
    public String getName() {
      return this.name;
    }

    /**
     * <host name>:<port number> of the datanode
     */
    public datanodeUp_args setName(String name) {
      this.name = name;
      return this;
    }

    public void unsetName() {
      this.name = null;
    }

    /** Returns true if field name is set (has been asigned a value) and false otherwise */
    public boolean isSetName() {
      return this.name != null;
    }

    public void setNameIsSet(boolean value) {
      if (!value) {
        this.name = null;
      }
    }

    /**
     * the storage id of the datanode
     */
    public String getStorage() {
      return this.storage;
    }

    /**
     * the storage id of the datanode
     */
    public datanodeUp_args setStorage(String storage) {
      this.storage = storage;
      return this;
    }

    public void unsetStorage() {
      this.storage = null;
    }

    /** Returns true if field storage is set (has been asigned a value) and false otherwise */
    public boolean isSetStorage() {
      return this.storage != null;
    }

    public void setStorageIsSet(boolean value) {
      if (!value) {
        this.storage = null;
      }
    }

    /**
     * Thrift port of the datanode
     */
    public int getThriftPort() {
      return this.thriftPort;
    }

    /**
     * Thrift port of the datanode
     */
    public datanodeUp_args setThriftPort(int thriftPort) {
      this.thriftPort = thriftPort;
      setThriftPortIsSet(true);
      return this;
    }

    public void unsetThriftPort() {
      __isset_bit_vector.clear(__THRIFTPORT_ISSET_ID);
    }

    /** Returns true if field thriftPort is set (has been asigned a value) and false otherwise */
    public boolean isSetThriftPort() {
      return __isset_bit_vector.get(__THRIFTPORT_ISSET_ID);
    }

    public void setThriftPortIsSet(boolean value) {
      __isset_bit_vector.set(__THRIFTPORT_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case NAME:
        if (value == null) {
          unsetName();
        } else {
          setName((String)value);
        }
        break;

      case STORAGE:
        if (value == null) {
          unsetStorage();
        } else {
          setStorage((String)value);
        }
        break;

      case THRIFT_PORT:
        if (value == null) {
          unsetThriftPort();
        } else {
          setThriftPort((Integer)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case NAME:
        return getName();

      case STORAGE:
        return getStorage();

      case THRIFT_PORT:
        return new Integer(getThriftPort());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case NAME:
        return isSetName();
      case STORAGE:
        return isSetStorage();
      case THRIFT_PORT:
        return isSetThriftPort();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof datanodeUp_args)
        return this.equals((datanodeUp_args)that);
      return false;
    }

    public boolean equals(datanodeUp_args that) {
      if (that == null)
        return false;

      boolean this_present_name = true && this.isSetName();
      boolean that_present_name = true && that.isSetName();
      if (this_present_name || that_present_name) {
        if (!(this_present_name && that_present_name))
          return false;
        if (!this.name.equals(that.name))
          return false;
      }

      boolean this_present_storage = true && this.isSetStorage();
      boolean that_present_storage = true && that.isSetStorage();
      if (this_present_storage || that_present_storage) {
        if (!(this_present_storage && that_present_storage))
          return false;
        if (!this.storage.equals(that.storage))
          return false;
      }

      boolean this_present_thriftPort = true;
      boolean that_present_thriftPort = true;
      if (this_present_thriftPort || that_present_thriftPort) {
        if (!(this_present_thriftPort && that_present_thriftPort))
          return false;
        if (this.thriftPort != that.thriftPort)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(datanodeUp_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      datanodeUp_args typedOther = (datanodeUp_args)other;

      lastComparison = Boolean.valueOf(isSetName()).compareTo(typedOther.isSetName());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetName()) {
        lastComparison = TBaseHelper.compareTo(this.name, typedOther.name);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetStorage()).compareTo(typedOther.isSetStorage());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetStorage()) {
        lastComparison = TBaseHelper.compareTo(this.storage, typedOther.storage);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetThriftPort()).compareTo(typedOther.isSetThriftPort());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetThriftPort()) {
        lastComparison = TBaseHelper.compareTo(this.thriftPort, typedOther.thriftPort);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // NAME
            if (field.type == TType.STRING) {
              this.name = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // STORAGE
            if (field.type == TType.STRING) {
              this.storage = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 3: // THRIFT_PORT
            if (field.type == TType.I32) {
              this.thriftPort = iprot.readI32();
              setThriftPortIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.name != null) {
        oprot.writeFieldBegin(NAME_FIELD_DESC);
        oprot.writeString(this.name);
        oprot.writeFieldEnd();
      }
      if (this.storage != null) {
        oprot.writeFieldBegin(STORAGE_FIELD_DESC);
        oprot.writeString(this.storage);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(THRIFT_PORT_FIELD_DESC);
      oprot.writeI32(this.thriftPort);
      oprot.writeFieldEnd();
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("datanodeUp_args(");
      boolean first = true;

      sb.append("name:");
      if (this.name == null) {
        sb.append("null");
      } else {
        sb.append(this.name);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("storage:");
      if (this.storage == null) {
        sb.append("null");
      } else {
        sb.append(this.storage);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("thriftPort:");
      sb.append(this.thriftPort);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class datanodeUp_result implements TBase<datanodeUp_result, datanodeUp_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("datanodeUp_result");



    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
;

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }
    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(datanodeUp_result.class, metaDataMap);
    }

    public datanodeUp_result() {
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public datanodeUp_result(datanodeUp_result other) {
    }

    public datanodeUp_result deepCopy() {
      return new datanodeUp_result(this);
    }

    @Override
    public void clear() {
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof datanodeUp_result)
        return this.equals((datanodeUp_result)that);
      return false;
    }

    public boolean equals(datanodeUp_result that) {
      if (that == null)
        return false;

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(datanodeUp_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      datanodeUp_result typedOther = (datanodeUp_result)other;

      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("datanodeUp_result(");
      boolean first = true;

      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class datanodeDown_args implements TBase<datanodeDown_args, datanodeDown_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("datanodeDown_args");

    private static final TField NAME_FIELD_DESC = new TField("name", TType.STRING, (short)1);
    private static final TField STORAGE_FIELD_DESC = new TField("storage", TType.STRING, (short)2);
    private static final TField THRIFT_PORT_FIELD_DESC = new TField("thriftPort", TType.I32, (short)3);

    /**
     * <host name>:<port number> of the datanode
     */
    public String name;
    /**
     * the storage id of the datanode
     */
    public String storage;
    /**
     * Thrift port of the datanode
     */
    public int thriftPort;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      /**
       * <host name>:<port number> of the datanode
       */
      NAME((short)1, "name"),
      /**
       * the storage id of the datanode
       */
      STORAGE((short)2, "storage"),
      /**
       * Thrift port of the datanode
       */
      THRIFT_PORT((short)3, "thriftPort");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 1: // NAME
            return NAME;
          case 2: // STORAGE
            return STORAGE;
          case 3: // THRIFT_PORT
            return THRIFT_PORT;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments
    private static final int __THRIFTPORT_ISSET_ID = 0;
    private BitSet __isset_bit_vector = new BitSet(1);

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.NAME, new FieldMetaData("name", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.STORAGE, new FieldMetaData("storage", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      tmpMap.put(_Fields.THRIFT_PORT, new FieldMetaData("thriftPort", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.I32)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(datanodeDown_args.class, metaDataMap);
    }

    public datanodeDown_args() {
    }

    public datanodeDown_args(
      String name,
      String storage,
      int thriftPort)
    {
      this();
      this.name = name;
      this.storage = storage;
      this.thriftPort = thriftPort;
      setThriftPortIsSet(true);
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public datanodeDown_args(datanodeDown_args other) {
      __isset_bit_vector.clear();
      __isset_bit_vector.or(other.__isset_bit_vector);
      if (other.isSetName()) {
        this.name = other.name;
      }
      if (other.isSetStorage()) {
        this.storage = other.storage;
      }
      this.thriftPort = other.thriftPort;
    }

    public datanodeDown_args deepCopy() {
      return new datanodeDown_args(this);
    }

    @Override
    public void clear() {
      this.name = null;
      this.storage = null;
      setThriftPortIsSet(false);
      this.thriftPort = 0;
    }

    /**
     * <host name>:<port number> of the datanode
     */
    public String getName() {
      return this.name;
    }

    /**
     * <host name>:<port number> of the datanode
     */
    public datanodeDown_args setName(String name) {
      this.name = name;
      return this;
    }

    public void unsetName() {
      this.name = null;
    }

    /** Returns true if field name is set (has been asigned a value) and false otherwise */
    public boolean isSetName() {
      return this.name != null;
    }

    public void setNameIsSet(boolean value) {
      if (!value) {
        this.name = null;
      }
    }

    /**
     * the storage id of the datanode
     */
    public String getStorage() {
      return this.storage;
    }

    /**
     * the storage id of the datanode
     */
    public datanodeDown_args setStorage(String storage) {
      this.storage = storage;
      return this;
    }

    public void unsetStorage() {
      this.storage = null;
    }

    /** Returns true if field storage is set (has been asigned a value) and false otherwise */
    public boolean isSetStorage() {
      return this.storage != null;
    }

    public void setStorageIsSet(boolean value) {
      if (!value) {
        this.storage = null;
      }
    }

    /**
     * Thrift port of the datanode
     */
    public int getThriftPort() {
      return this.thriftPort;
    }

    /**
     * Thrift port of the datanode
     */
    public datanodeDown_args setThriftPort(int thriftPort) {
      this.thriftPort = thriftPort;
      setThriftPortIsSet(true);
      return this;
    }

    public void unsetThriftPort() {
      __isset_bit_vector.clear(__THRIFTPORT_ISSET_ID);
    }

    /** Returns true if field thriftPort is set (has been asigned a value) and false otherwise */
    public boolean isSetThriftPort() {
      return __isset_bit_vector.get(__THRIFTPORT_ISSET_ID);
    }

    public void setThriftPortIsSet(boolean value) {
      __isset_bit_vector.set(__THRIFTPORT_ISSET_ID, value);
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case NAME:
        if (value == null) {
          unsetName();
        } else {
          setName((String)value);
        }
        break;

      case STORAGE:
        if (value == null) {
          unsetStorage();
        } else {
          setStorage((String)value);
        }
        break;

      case THRIFT_PORT:
        if (value == null) {
          unsetThriftPort();
        } else {
          setThriftPort((Integer)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case NAME:
        return getName();

      case STORAGE:
        return getStorage();

      case THRIFT_PORT:
        return new Integer(getThriftPort());

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case NAME:
        return isSetName();
      case STORAGE:
        return isSetStorage();
      case THRIFT_PORT:
        return isSetThriftPort();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof datanodeDown_args)
        return this.equals((datanodeDown_args)that);
      return false;
    }

    public boolean equals(datanodeDown_args that) {
      if (that == null)
        return false;

      boolean this_present_name = true && this.isSetName();
      boolean that_present_name = true && that.isSetName();
      if (this_present_name || that_present_name) {
        if (!(this_present_name && that_present_name))
          return false;
        if (!this.name.equals(that.name))
          return false;
      }

      boolean this_present_storage = true && this.isSetStorage();
      boolean that_present_storage = true && that.isSetStorage();
      if (this_present_storage || that_present_storage) {
        if (!(this_present_storage && that_present_storage))
          return false;
        if (!this.storage.equals(that.storage))
          return false;
      }

      boolean this_present_thriftPort = true;
      boolean that_present_thriftPort = true;
      if (this_present_thriftPort || that_present_thriftPort) {
        if (!(this_present_thriftPort && that_present_thriftPort))
          return false;
        if (this.thriftPort != that.thriftPort)
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(datanodeDown_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      datanodeDown_args typedOther = (datanodeDown_args)other;

      lastComparison = Boolean.valueOf(isSetName()).compareTo(typedOther.isSetName());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetName()) {
        lastComparison = TBaseHelper.compareTo(this.name, typedOther.name);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetStorage()).compareTo(typedOther.isSetStorage());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetStorage()) {
        lastComparison = TBaseHelper.compareTo(this.storage, typedOther.storage);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetThriftPort()).compareTo(typedOther.isSetThriftPort());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetThriftPort()) {
        lastComparison = TBaseHelper.compareTo(this.thriftPort, typedOther.thriftPort);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 1: // NAME
            if (field.type == TType.STRING) {
              this.name = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 2: // STORAGE
            if (field.type == TType.STRING) {
              this.storage = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 3: // THRIFT_PORT
            if (field.type == TType.I32) {
              this.thriftPort = iprot.readI32();
              setThriftPortIsSet(true);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.name != null) {
        oprot.writeFieldBegin(NAME_FIELD_DESC);
        oprot.writeString(this.name);
        oprot.writeFieldEnd();
      }
      if (this.storage != null) {
        oprot.writeFieldBegin(STORAGE_FIELD_DESC);
        oprot.writeString(this.storage);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldBegin(THRIFT_PORT_FIELD_DESC);
      oprot.writeI32(this.thriftPort);
      oprot.writeFieldEnd();
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("datanodeDown_args(");
      boolean first = true;

      sb.append("name:");
      if (this.name == null) {
        sb.append("null");
      } else {
        sb.append(this.name);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("storage:");
      if (this.storage == null) {
        sb.append("null");
      } else {
        sb.append(this.storage);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("thriftPort:");
      sb.append(this.thriftPort);
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class datanodeDown_result implements TBase<datanodeDown_result, datanodeDown_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("datanodeDown_result");



    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
;

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }
    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(datanodeDown_result.class, metaDataMap);
    }

    public datanodeDown_result() {
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public datanodeDown_result(datanodeDown_result other) {
    }

    public datanodeDown_result deepCopy() {
      return new datanodeDown_result(this);
    }

    @Override
    public void clear() {
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof datanodeDown_result)
        return this.equals((datanodeDown_result)that);
      return false;
    }

    public boolean equals(datanodeDown_result that) {
      if (that == null)
        return false;

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(datanodeDown_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      datanodeDown_result typedOther = (datanodeDown_result)other;

      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("datanodeDown_result(");
      boolean first = true;

      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getDelegationToken_args implements TBase<getDelegationToken_args, getDelegationToken_args._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getDelegationToken_args");

    private static final TField CTX_FIELD_DESC = new TField("ctx", TType.STRUCT, (short)10);
    private static final TField RENEWER_FIELD_DESC = new TField("renewer", TType.STRING, (short)1);

    public org.apache.hadoop.thriftfs.api.RequestContext ctx;
    public String renewer;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      CTX((short)10, "ctx"),
      RENEWER((short)1, "renewer");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 10: // CTX
            return CTX;
          case 1: // RENEWER
            return RENEWER;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.CTX, new FieldMetaData("ctx", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.RequestContext.class)));
      tmpMap.put(_Fields.RENEWER, new FieldMetaData("renewer", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRING)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getDelegationToken_args.class, metaDataMap);
    }

    public getDelegationToken_args() {
    }

    public getDelegationToken_args(
      org.apache.hadoop.thriftfs.api.RequestContext ctx,
      String renewer)
    {
      this();
      this.ctx = ctx;
      this.renewer = renewer;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getDelegationToken_args(getDelegationToken_args other) {
      if (other.isSetCtx()) {
        this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext(other.ctx);
      }
      if (other.isSetRenewer()) {
        this.renewer = other.renewer;
      }
    }

    public getDelegationToken_args deepCopy() {
      return new getDelegationToken_args(this);
    }

    @Override
    public void clear() {
      this.ctx = null;
      this.renewer = null;
    }

    public org.apache.hadoop.thriftfs.api.RequestContext getCtx() {
      return this.ctx;
    }

    public getDelegationToken_args setCtx(org.apache.hadoop.thriftfs.api.RequestContext ctx) {
      this.ctx = ctx;
      return this;
    }

    public void unsetCtx() {
      this.ctx = null;
    }

    /** Returns true if field ctx is set (has been asigned a value) and false otherwise */
    public boolean isSetCtx() {
      return this.ctx != null;
    }

    public void setCtxIsSet(boolean value) {
      if (!value) {
        this.ctx = null;
      }
    }

    public String getRenewer() {
      return this.renewer;
    }

    public getDelegationToken_args setRenewer(String renewer) {
      this.renewer = renewer;
      return this;
    }

    public void unsetRenewer() {
      this.renewer = null;
    }

    /** Returns true if field renewer is set (has been asigned a value) and false otherwise */
    public boolean isSetRenewer() {
      return this.renewer != null;
    }

    public void setRenewerIsSet(boolean value) {
      if (!value) {
        this.renewer = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case CTX:
        if (value == null) {
          unsetCtx();
        } else {
          setCtx((org.apache.hadoop.thriftfs.api.RequestContext)value);
        }
        break;

      case RENEWER:
        if (value == null) {
          unsetRenewer();
        } else {
          setRenewer((String)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case CTX:
        return getCtx();

      case RENEWER:
        return getRenewer();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case CTX:
        return isSetCtx();
      case RENEWER:
        return isSetRenewer();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getDelegationToken_args)
        return this.equals((getDelegationToken_args)that);
      return false;
    }

    public boolean equals(getDelegationToken_args that) {
      if (that == null)
        return false;

      boolean this_present_ctx = true && this.isSetCtx();
      boolean that_present_ctx = true && that.isSetCtx();
      if (this_present_ctx || that_present_ctx) {
        if (!(this_present_ctx && that_present_ctx))
          return false;
        if (!this.ctx.equals(that.ctx))
          return false;
      }

      boolean this_present_renewer = true && this.isSetRenewer();
      boolean that_present_renewer = true && that.isSetRenewer();
      if (this_present_renewer || that_present_renewer) {
        if (!(this_present_renewer && that_present_renewer))
          return false;
        if (!this.renewer.equals(that.renewer))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getDelegationToken_args other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getDelegationToken_args typedOther = (getDelegationToken_args)other;

      lastComparison = Boolean.valueOf(isSetCtx()).compareTo(typedOther.isSetCtx());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetCtx()) {
        lastComparison = TBaseHelper.compareTo(this.ctx, typedOther.ctx);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetRenewer()).compareTo(typedOther.isSetRenewer());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetRenewer()) {
        lastComparison = TBaseHelper.compareTo(this.renewer, typedOther.renewer);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 10: // CTX
            if (field.type == TType.STRUCT) {
              this.ctx = new org.apache.hadoop.thriftfs.api.RequestContext();
              this.ctx.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // RENEWER
            if (field.type == TType.STRING) {
              this.renewer = iprot.readString();
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (this.renewer != null) {
        oprot.writeFieldBegin(RENEWER_FIELD_DESC);
        oprot.writeString(this.renewer);
        oprot.writeFieldEnd();
      }
      if (this.ctx != null) {
        oprot.writeFieldBegin(CTX_FIELD_DESC);
        this.ctx.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getDelegationToken_args(");
      boolean first = true;

      sb.append("ctx:");
      if (this.ctx == null) {
        sb.append("null");
      } else {
        sb.append(this.ctx);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("renewer:");
      if (this.renewer == null) {
        sb.append("null");
      } else {
        sb.append(this.renewer);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

  public static class getDelegationToken_result implements TBase<getDelegationToken_result, getDelegationToken_result._Fields>, java.io.Serializable, Cloneable   {
    private static final TStruct STRUCT_DESC = new TStruct("getDelegationToken_result");

    private static final TField SUCCESS_FIELD_DESC = new TField("success", TType.STRUCT, (short)0);
    private static final TField ERR_FIELD_DESC = new TField("err", TType.STRUCT, (short)1);

    public org.apache.hadoop.thriftfs.api.ThriftDelegationToken success;
    public org.apache.hadoop.thriftfs.api.IOException err;

    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
    public enum _Fields implements TFieldIdEnum {
      SUCCESS((short)0, "success"),
      ERR((short)1, "err");

      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

      static {
        for (_Fields field : EnumSet.allOf(_Fields.class)) {
          byName.put(field.getFieldName(), field);
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, or null if its not found.
       */
      public static _Fields findByThriftId(int fieldId) {
        switch(fieldId) {
          case 0: // SUCCESS
            return SUCCESS;
          case 1: // ERR
            return ERR;
          default:
            return null;
        }
      }

      /**
       * Find the _Fields constant that matches fieldId, throwing an exception
       * if it is not found.
       */
      public static _Fields findByThriftIdOrThrow(int fieldId) {
        _Fields fields = findByThriftId(fieldId);
        if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
        return fields;
      }

      /**
       * Find the _Fields constant that matches name, or null if its not found.
       */
      public static _Fields findByName(String name) {
        return byName.get(name);
      }

      private final short _thriftId;
      private final String _fieldName;

      _Fields(short thriftId, String fieldName) {
        _thriftId = thriftId;
        _fieldName = fieldName;
      }

      public short getThriftFieldId() {
        return _thriftId;
      }

      public String getFieldName() {
        return _fieldName;
      }
    }

    // isset id assignments

    public static final Map<_Fields, FieldMetaData> metaDataMap;
    static {
      Map<_Fields, FieldMetaData> tmpMap = new EnumMap<_Fields, FieldMetaData>(_Fields.class);
      tmpMap.put(_Fields.SUCCESS, new FieldMetaData("success", TFieldRequirementType.DEFAULT, 
          new StructMetaData(TType.STRUCT, org.apache.hadoop.thriftfs.api.ThriftDelegationToken.class)));
      tmpMap.put(_Fields.ERR, new FieldMetaData("err", TFieldRequirementType.DEFAULT, 
          new FieldValueMetaData(TType.STRUCT)));
      metaDataMap = Collections.unmodifiableMap(tmpMap);
      FieldMetaData.addStructMetaDataMap(getDelegationToken_result.class, metaDataMap);
    }

    public getDelegationToken_result() {
    }

    public getDelegationToken_result(
      org.apache.hadoop.thriftfs.api.ThriftDelegationToken success,
      org.apache.hadoop.thriftfs.api.IOException err)
    {
      this();
      this.success = success;
      this.err = err;
    }

    /**
     * Performs a deep copy on <i>other</i>.
     */
    public getDelegationToken_result(getDelegationToken_result other) {
      if (other.isSetSuccess()) {
        this.success = new org.apache.hadoop.thriftfs.api.ThriftDelegationToken(other.success);
      }
      if (other.isSetErr()) {
        this.err = new org.apache.hadoop.thriftfs.api.IOException(other.err);
      }
    }

    public getDelegationToken_result deepCopy() {
      return new getDelegationToken_result(this);
    }

    @Override
    public void clear() {
      this.success = null;
      this.err = null;
    }

    public org.apache.hadoop.thriftfs.api.ThriftDelegationToken getSuccess() {
      return this.success;
    }

    public getDelegationToken_result setSuccess(org.apache.hadoop.thriftfs.api.ThriftDelegationToken success) {
      this.success = success;
      return this;
    }

    public void unsetSuccess() {
      this.success = null;
    }

    /** Returns true if field success is set (has been asigned a value) and false otherwise */
    public boolean isSetSuccess() {
      return this.success != null;
    }

    public void setSuccessIsSet(boolean value) {
      if (!value) {
        this.success = null;
      }
    }

    public org.apache.hadoop.thriftfs.api.IOException getErr() {
      return this.err;
    }

    public getDelegationToken_result setErr(org.apache.hadoop.thriftfs.api.IOException err) {
      this.err = err;
      return this;
    }

    public void unsetErr() {
      this.err = null;
    }

    /** Returns true if field err is set (has been asigned a value) and false otherwise */
    public boolean isSetErr() {
      return this.err != null;
    }

    public void setErrIsSet(boolean value) {
      if (!value) {
        this.err = null;
      }
    }

    public void setFieldValue(_Fields field, Object value) {
      switch (field) {
      case SUCCESS:
        if (value == null) {
          unsetSuccess();
        } else {
          setSuccess((org.apache.hadoop.thriftfs.api.ThriftDelegationToken)value);
        }
        break;

      case ERR:
        if (value == null) {
          unsetErr();
        } else {
          setErr((org.apache.hadoop.thriftfs.api.IOException)value);
        }
        break;

      }
    }

    public Object getFieldValue(_Fields field) {
      switch (field) {
      case SUCCESS:
        return getSuccess();

      case ERR:
        return getErr();

      }
      throw new IllegalStateException();
    }

    /** Returns true if field corresponding to fieldID is set (has been asigned a value) and false otherwise */
    public boolean isSet(_Fields field) {
      if (field == null) {
        throw new IllegalArgumentException();
      }

      switch (field) {
      case SUCCESS:
        return isSetSuccess();
      case ERR:
        return isSetErr();
      }
      throw new IllegalStateException();
    }

    @Override
    public boolean equals(Object that) {
      if (that == null)
        return false;
      if (that instanceof getDelegationToken_result)
        return this.equals((getDelegationToken_result)that);
      return false;
    }

    public boolean equals(getDelegationToken_result that) {
      if (that == null)
        return false;

      boolean this_present_success = true && this.isSetSuccess();
      boolean that_present_success = true && that.isSetSuccess();
      if (this_present_success || that_present_success) {
        if (!(this_present_success && that_present_success))
          return false;
        if (!this.success.equals(that.success))
          return false;
      }

      boolean this_present_err = true && this.isSetErr();
      boolean that_present_err = true && that.isSetErr();
      if (this_present_err || that_present_err) {
        if (!(this_present_err && that_present_err))
          return false;
        if (!this.err.equals(that.err))
          return false;
      }

      return true;
    }

    @Override
    public int hashCode() {
      return 0;
    }

    public int compareTo(getDelegationToken_result other) {
      if (!getClass().equals(other.getClass())) {
        return getClass().getName().compareTo(other.getClass().getName());
      }

      int lastComparison = 0;
      getDelegationToken_result typedOther = (getDelegationToken_result)other;

      lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(typedOther.isSetSuccess());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetSuccess()) {
        lastComparison = TBaseHelper.compareTo(this.success, typedOther.success);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      lastComparison = Boolean.valueOf(isSetErr()).compareTo(typedOther.isSetErr());
      if (lastComparison != 0) {
        return lastComparison;
      }
      if (isSetErr()) {
        lastComparison = TBaseHelper.compareTo(this.err, typedOther.err);
        if (lastComparison != 0) {
          return lastComparison;
        }
      }
      return 0;
    }

    public _Fields fieldForId(int fieldId) {
      return _Fields.findByThriftId(fieldId);
    }

    public void read(TProtocol iprot) throws TException {
      TField field;
      iprot.readStructBegin();
      while (true)
      {
        field = iprot.readFieldBegin();
        if (field.type == TType.STOP) { 
          break;
        }
        switch (field.id) {
          case 0: // SUCCESS
            if (field.type == TType.STRUCT) {
              this.success = new org.apache.hadoop.thriftfs.api.ThriftDelegationToken();
              this.success.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          case 1: // ERR
            if (field.type == TType.STRUCT) {
              this.err = new org.apache.hadoop.thriftfs.api.IOException();
              this.err.read(iprot);
            } else { 
              TProtocolUtil.skip(iprot, field.type);
            }
            break;
          default:
            TProtocolUtil.skip(iprot, field.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      validate();
    }

    public void write(TProtocol oprot) throws TException {
      oprot.writeStructBegin(STRUCT_DESC);

      if (this.isSetSuccess()) {
        oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
        this.success.write(oprot);
        oprot.writeFieldEnd();
      } else if (this.isSetErr()) {
        oprot.writeFieldBegin(ERR_FIELD_DESC);
        this.err.write(oprot);
        oprot.writeFieldEnd();
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

    @Override
    public String toString() {
      StringBuilder sb = new StringBuilder("getDelegationToken_result(");
      boolean first = true;

      sb.append("success:");
      if (this.success == null) {
        sb.append("null");
      } else {
        sb.append(this.success);
      }
      first = false;
      if (!first) sb.append(", ");
      sb.append("err:");
      if (this.err == null) {
        sb.append("null");
      } else {
        sb.append(this.err);
      }
      first = false;
      sb.append(")");
      return sb.toString();
    }

    public void validate() throws TException {
      // check for required fields
    }

  }

}
