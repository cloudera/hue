{"body":"<div><div><div class=\"hue-doc-title\">TIMESTAMP Data Type</div><div><p>\n      A data type used in <span class=\"hue-doc-codeph\">CREATE TABLE</span> and <span class=\"hue-doc-codeph\">ALTER TABLE</span>\n      statements, representing a point in time.\n    </p><p><b>Syntax:</b></p><p>\n      In the column definition of a <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement:\n    </p><div class=\"hue-doc-codeblock\"><span class=\"hue-doc-varname\">column_name</span> TIMESTAMP</div><p><b>Range:</b> Allowed date values range from 1400-01-01 to 9999-12-31; this range is\n      different from the Hive <span class=\"hue-doc-codeph\">TIMESTAMP</span> type. Internally, the resolution of the\n      time portion of a <span class=\"hue-doc-codeph\">TIMESTAMP</span> value is in nanoseconds.\n    </p><p><b>INTERVAL expressions:</b></p><p>\n      You can perform date arithmetic by adding or subtracting a specified number of time units,\n      using the <span class=\"hue-doc-codeph\">INTERVAL</span> keyword and the <span class=\"hue-doc-codeph\">+</span> and\n      <span class=\"hue-doc-codeph\">-</span> operators or <span class=\"hue-doc-codeph\">date_add()</span> and\n      <span class=\"hue-doc-codeph\">date_sub()</span> functions. You can specify units as <span class=\"hue-doc-codeph\">YEAR[S]</span>,\n      <span class=\"hue-doc-codeph\">MONTH[S]</span>, <span class=\"hue-doc-codeph\">WEEK[S]</span>, <span class=\"hue-doc-codeph\">DAY[S]</span>,\n      <span class=\"hue-doc-codeph\">HOUR[S]</span>, <span class=\"hue-doc-codeph\">MINUTE[S]</span>, <span class=\"hue-doc-codeph\">SECOND[S]</span>,\n      <span class=\"hue-doc-codeph\">MILLISECOND[S]</span>, <span class=\"hue-doc-codeph\">MICROSECOND[S]</span>, and\n      <span class=\"hue-doc-codeph\">NANOSECOND[S]</span>. You can only specify one time unit in each interval\n      expression, for example <span class=\"hue-doc-codeph\">INTERVAL 3 DAYS</span> or <span class=\"hue-doc-codeph\">INTERVAL 25\n      HOURS</span>, but you can produce any granularity by adding together successive\n      <span class=\"hue-doc-codeph\">INTERVAL</span> values, such as <span class=\"hue-doc-codeph\"><span class=\"hue-doc-varname\">timestamp_value</span> +\n      INTERVAL 3 WEEKS - INTERVAL 1 DAY + INTERVAL 10 MICROSECONDS</span>.\n    </p><p>\n      For example:\n    </p><div class=\"hue-doc-codeblock\">select now() + interval 1 day;\nselect date_sub(now(), interval 5 minutes);\ninsert into auction_details\n  select auction_id, auction_start_time, auction_start_time + interval 2 days + interval 12 hours\n  from new_auctions;</div><p><b>Time zones:</b></p><p>\n      By default, Impala does not store timestamps using the local timezone, to avoid undesired\n      results from unexpected time zone issues. Timestamps are stored and interpreted relative\n      to UTC, both when written to or read from data files, or when converted to or from Unix\n      time values through functions such as <span class=\"hue-doc-codeph\">from_unixtime()</span> or\n      <span class=\"hue-doc-codeph\">unix_timestamp()</span>. To convert such a <span class=\"hue-doc-codeph\">TIMESTAMP</span> value to\n      one that represents the date and time in a specific time zone, convert the original value\n      with the <span class=\"hue-doc-codeph\">from_utc_timestamp()</span> function.\n    </p><p>\n      Because Impala does not assume that <span class=\"hue-doc-codeph\">TIMESTAMP</span> values are in any\n      particular time zone, you must be conscious of the time zone aspects of data that you\n      query, insert, or convert.\n    </p><p>\n      For consistency with Unix system calls, the <span class=\"hue-doc-codeph\">TIMESTAMP</span> returned by the\n      <span class=\"hue-doc-codeph\">now()</span> function represents the local time in the system time zone, rather\n      than in UTC. To store values relative to the current time in a portable way, convert any\n      <span class=\"hue-doc-codeph\">now()</span> return values using the <span class=\"hue-doc-codeph\">to_utc_timestamp()</span>\n      function first. For example, the following example shows that the current time in\n      California (where this Impala cluster is located) is shortly after 2 PM. If that value was\n      written to a data file, and shipped off to a distant server to be analyzed alongside other\n      data from far-flung locations, the dates and times would not match up precisely because of\n      time zone differences. Therefore, the <span class=\"hue-doc-codeph\">to_utc_timestamp()</span> function\n      converts it using a common reference point, the UTC time zone (descended from the old\n      Greenwich Mean Time standard). The <span class=\"hue-doc-codeph\">'PDT'</span> argument indicates that the\n      original value is from the Pacific time zone with Daylight Saving Time in effect. When\n      servers in all geographic locations run the same transformation on any local date and time\n      values (with the appropriate time zone argument), the stored data uses a consistent\n      representation. Impala queries can use functions such as <span class=\"hue-doc-codeph\">EXTRACT()</span>,\n      <span class=\"hue-doc-codeph\">MIN()</span>, <span class=\"hue-doc-codeph\">AVG()</span>, and so on to do time-series analysis on\n      those timestamps.\n    </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; select now();\n+-------------------------------+\n| now()                         |\n+-------------------------------+\n| 2015-04-09 14:07:46.580465000 |\n+-------------------------------+\n[localhost:21000] &gt; select to_utc_timestamp(now(), 'PDT');\n+--------------------------------+\n| to_utc_timestamp(now(), 'pdt') |\n+--------------------------------+\n| 2015-04-09 21:08:07.664547000  |\n+--------------------------------+\n</div><p>\n      The converse function, <span class=\"hue-doc-codeph\">from_utc_timestamp()</span>, lets you take stored\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span> data or calculated results and convert back to local date and\n      time for processing on the application side. The following example shows how you might\n      represent some future date (such as the ending date and time of an auction) in UTC, and\n      then convert back to local time when convenient for reporting or other processing. The\n      final query in the example tests whether this arbitrary UTC date and time has passed yet,\n      by converting it back to the local time zone and comparing it against the current date and\n      time.\n    </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; select to_utc_timestamp(now() + interval 2 weeks, 'PDT');\n+---------------------------------------------------+\n| to_utc_timestamp(now() + interval 2 weeks, 'pdt') |\n+---------------------------------------------------+\n| 2015-04-23 21:08:34.152923000                     |\n+---------------------------------------------------+\n[localhost:21000] &gt; select from_utc_timestamp('2015-04-23 21:08:34.152923000','PDT');\n+------------------------------------------------------------+\n| from_utc_timestamp('2015-04-23 21:08:34.152923000', 'pdt') |\n+------------------------------------------------------------+\n| 2015-04-23 14:08:34.152923000                              |\n+------------------------------------------------------------+\n[localhost:21000] &gt; select from_utc_timestamp('2015-04-23 21:08:34.152923000','PDT') &lt; now();\n+--------------------------------------------------------------------+\n| from_utc_timestamp('2015-04-23 21:08:34.152923000', 'pdt') &lt; now() |\n+--------------------------------------------------------------------+\n| false                                                              |\n+--------------------------------------------------------------------+\n</div><p>\n      If you have data files written by Hive, those <span class=\"hue-doc-codeph\">TIMESTAMP</span> values represent\n      the local timezone of the host where the data was written, potentially leading to\n      inconsistent results when processed by Impala. To avoid compatibility problems or having\n      to code workarounds, you can specify one or both of these <span class=\"hue-doc-cmdname\">impalad</span>\n      startup flags: <span class=\"hue-doc-codeph\">--use_local_tz_for_unix_timestamp_conversions=true</span><span class=\"hue-doc-codeph\">-convert_legacy_hive_parquet_utc_timestamps=true</span>. Although\n      <span class=\"hue-doc-codeph\">-convert_legacy_hive_parquet_utc_timestamps</span> is turned off by default to\n      avoid performance overhead, where practical turn it on when processing\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span> columns in Parquet files written by Hive, to avoid unexpected\n      behavior.\n    </p><p>\n      The <span class=\"hue-doc-codeph\">--use_local_tz_for_unix_timestamp_conversions</span> setting affects\n      conversions from <span class=\"hue-doc-codeph\">TIMESTAMP</span> to <span class=\"hue-doc-codeph\">BIGINT</span>, or from\n      <span class=\"hue-doc-codeph\">BIGINT</span> to <span class=\"hue-doc-codeph\">TIMESTAMP</span>. By default, Impala treats all\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span> values as UTC, to simplify analysis of time-series data from\n      different geographic regions. When you enable the\n      <span class=\"hue-doc-codeph\">--use_local_tz_for_unix_timestamp_conversions</span> setting, these operations\n      treat the input values as if they are in the local tie zone of the host doing the\n      processing. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_datetime_functions.xml\" data-doc-anchor-id=\"datetime_functions\">Impala Date and Time Functions</a>\n      for the list of functions affected by the\n      <span class=\"hue-doc-codeph\">--use_local_tz_for_unix_timestamp_conversions</span> setting.\n    </p><p>\n      The following sequence of examples shows how the interpretation of\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span> values in Parquet tables is affected by the setting of the\n      <span class=\"hue-doc-codeph\">-convert_legacy_hive_parquet_utc_timestamps</span> setting.\n    </p><p>\n      Regardless of the <span class=\"hue-doc-codeph\">-convert_legacy_hive_parquet_utc_timestamps</span> setting,\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span> columns in text tables can be written and read interchangeably\n      by Impala and Hive:\n    </p><div class=\"hue-doc-codeblock\">Impala DDL and queries for text table:\n\n[localhost:21000] &gt; create table t1 (x timestamp);\n[localhost:21000] &gt; insert into t1 values (now()), (now() + interval 1 day);\n[localhost:21000] &gt; select x from t1;\n+-------------------------------+\n| x                             |\n+-------------------------------+\n| 2015-04-07 15:43:02.892403000 |\n| 2015-04-08 15:43:02.892403000 |\n+-------------------------------+\n[localhost:21000] &gt; select to_utc_timestamp(x, 'PDT') from t1;\n+-------------------------------+\n| to_utc_timestamp(x, 'pdt')    |\n+-------------------------------+\n| 2015-04-07 22:43:02.892403000 |\n| 2015-04-08 22:43:02.892403000 |\n+-------------------------------+\n\nHive query for text table:\n\nhive&gt; select * from t1;\nOK\n2015-04-07 15:43:02.892403\n2015-04-08 15:43:02.892403\nTime taken: 1.245 seconds, Fetched: 2 row(s)\n</div><p>\n      When the table uses Parquet format, Impala expects any time zone adjustment to be applied\n      prior to writing, while <span class=\"hue-doc-codeph\">TIMESTAMP</span> values written by Hive are adjusted to\n      be in the UTC time zone. When Hive queries Parquet data files that it wrote, it adjusts\n      the <span class=\"hue-doc-codeph\">TIMESTAMP</span> values back to the local time zone, while Impala does no\n      conversion. Hive does no time zone conversion when it queries Impala-written Parquet\n      files.\n    </p><div class=\"hue-doc-codeblock\">Impala DDL and queries for Parquet table:\n\n[localhost:21000] &gt; create table p1 stored as parquet as select x from t1;\n+-------------------+\n| summary           |\n+-------------------+\n| Inserted 2 row(s) |\n+-------------------+\n[localhost:21000] &gt; select x from p1;\n+-------------------------------+\n| x                             |\n+-------------------------------+\n| 2015-04-07 15:43:02.892403000 |\n| 2015-04-08 15:43:02.892403000 |\n+-------------------------------+\n\nHive DDL and queries for Parquet table:\n\nhive&gt; create table h1 (x timestamp) stored as parquet;\nOK\nhive&gt; insert into h1 select * from p1;\n...\nOK\nTime taken: 35.573 seconds\nhive&gt; select x from p1;\nOK\n2015-04-07 15:43:02.892403\n2015-04-08 15:43:02.892403\nTime taken: 0.324 seconds, Fetched: 2 row(s)\nhive&gt; select x from h1;\nOK\n2015-04-07 15:43:02.892403\n2015-04-08 15:43:02.892403\nTime taken: 0.197 seconds, Fetched: 2 row(s)\n</div><p>\n      The discrepancy arises when Impala queries the Hive-created Parquet table. The underlying\n      values in the <span class=\"hue-doc-codeph\">TIMESTAMP</span> column are different from the ones written by\n      Impala, even though they were copied from one table to another by an <span class=\"hue-doc-codeph\">INSERT ...\n      SELECT</span> statement in Hive. Hive did an implicit conversion from the local time\n      zone to UTC as it wrote the values to Parquet.\n    </p><div class=\"hue-doc-codeblock\">Impala query for TIMESTAMP values from Impala-written and Hive-written data:\n\n[localhost:21000] &gt; select * from p1;\n+-------------------------------+\n| x                             |\n+-------------------------------+\n| 2015-04-07 15:43:02.892403000 |\n| 2015-04-08 15:43:02.892403000 |\n+-------------------------------+\nFetched 2 row(s) in 0.29s\n[localhost:21000] &gt; select * from h1;\n+-------------------------------+\n| x                             |\n+-------------------------------+\n| 2015-04-07 22:43:02.892403000 |\n| 2015-04-08 22:43:02.892403000 |\n+-------------------------------+\nFetched 2 row(s) in 0.41s\n\nUnderlying integer values for Impala-written and Hive-written data:\n\n[localhost:21000] &gt; select cast(x as bigint) from p1;\n+-------------------+\n| cast(x as bigint) |\n+-------------------+\n| 1428421382        |\n| 1428507782        |\n+-------------------+\nFetched 2 row(s) in 0.38s\n[localhost:21000] &gt; select cast(x as bigint) from h1;\n+-------------------+\n| cast(x as bigint) |\n+-------------------+\n| 1428446582        |\n| 1428532982        |\n+-------------------+\nFetched 2 row(s) in 0.20s\n</div><p>\n      When the <span class=\"hue-doc-codeph\">-convert_legacy_hive_parquet_utc_timestamps</span> setting is enabled,\n      Impala recognizes the Parquet data files written by Hive, and applies the same\n      UTC-to-local-timezone conversion logic during the query as Hive uses, making the contents\n      of the Impala-written <span class=\"hue-doc-codeph\">P1</span> table and the Hive-written <span class=\"hue-doc-codeph\">H1</span>\n      table appear identical, whether represented as <span class=\"hue-doc-codeph\">TIMESTAMP</span> values or the\n      underlying <span class=\"hue-doc-codeph\">BIGINT</span> integers:\n    </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; select x from p1;\n+-------------------------------+\n| x                             |\n+-------------------------------+\n| 2015-04-07 15:43:02.892403000 |\n| 2015-04-08 15:43:02.892403000 |\n+-------------------------------+\nFetched 2 row(s) in 0.37s\n[localhost:21000] &gt; select x from h1;\n+-------------------------------+\n| x                             |\n+-------------------------------+\n| 2015-04-07 15:43:02.892403000 |\n| 2015-04-08 15:43:02.892403000 |\n+-------------------------------+\nFetched 2 row(s) in 0.19s\n[localhost:21000] &gt; select cast(x as bigint) from p1;\n+-------------------+\n| cast(x as bigint) |\n+-------------------+\n| 1428446582        |\n| 1428532982        |\n+-------------------+\nFetched 2 row(s) in 0.29s\n[localhost:21000] &gt; select cast(x as bigint) from h1;\n+-------------------+\n| cast(x as bigint) |\n+-------------------+\n| 1428446582        |\n| 1428532982        |\n+-------------------+\nFetched 2 row(s) in 0.22s\n</div><p><b>Conversions:</b></p><p>\n        Impala automatically converts <span class=\"hue-doc-codeph\">STRING</span> literals of the\n        correct format into <span class=\"hue-doc-codeph\">TIMESTAMP</span> values. Timestamp values\n        are accepted in the format <span class=\"hue-doc-codeph\">\"yyyy-MM-dd HH:mm:ss.SSSSSS\"</span>,\n        and can consist of just the date, or just the time, with or without the\n        fractional second portion. For example, you can specify <span class=\"hue-doc-codeph\">TIMESTAMP</span>\n        values such as <span class=\"hue-doc-codeph\">'1966-07-30'</span>, <span class=\"hue-doc-codeph\">'08:30:00'</span>,\n        or <span class=\"hue-doc-codeph\">'1985-09-25 17:45:30.005'</span>.\n      </p><p><span class=\"hue-doc-ph\">\n          Casting an integer or floating-point value <span class=\"hue-doc-codeph\">N</span> to\n        <span class=\"hue-doc-codeph\">TIMESTAMP</span> produces a value that is <span class=\"hue-doc-codeph\">N</span> seconds past the start of the epoch\n        date (January 1, 1970). By default, the result value represents a date and time in the UTC time zone.\n        If the setting <span class=\"hue-doc-codeph\">--use_local_tz_for_unix_timestamp_conversions=true</span> is in effect,\n        the resulting <span class=\"hue-doc-codeph\">TIMESTAMP</span> represents a date and time in the local time zone.\n        </span></p><p>\n      In Impala 1.3 and higher, the <span class=\"hue-doc-codeph\">FROM_UNIXTIME()</span> and\n      <span class=\"hue-doc-codeph\">UNIX_TIMESTAMP()</span> functions allow a wider range of format strings, with\n      more flexibility in element order, repetition of letter placeholders, and separator\n      characters. In Impala 2.3 and higher, the\n      <span class=\"hue-doc-codeph\">UNIX_TIMESTAMP()</span> function also allows a numeric timezone offset to be\n      specified as part of the input string. See\n      <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_datetime_functions.xml\" data-doc-anchor-id=\"datetime_functions\">Impala Date and Time Functions</a> for details.\n    </p><p>\n        In Impala 2.2.0 and higher, built-in functions that accept or return integers representing <span class=\"hue-doc-codeph\">TIMESTAMP</span> values\n        use the <span class=\"hue-doc-codeph\">BIGINT</span> type for parameters and return values, rather than <span class=\"hue-doc-codeph\">INT</span>.\n        This change lets the date and time functions avoid an overflow error that would otherwise occur\n        on January 19th, 2038 (known as the\n        <a class=\"hue-doc-external-link\" href=\"http://en.wikipedia.org/wiki/Year_2038_problem\" target=\"_blank\"><q>Year 2038 problem</q> or <q>Y2K38 problem</q></a>).\n        This change affects the <span class=\"hue-doc-codeph\">from_unixtime()</span> and <span class=\"hue-doc-codeph\">unix_timestamp()</span> functions.\n        You might need to change application code that interacts with these functions, change the types of\n        columns that store the return values, or add <span class=\"hue-doc-codeph\">CAST()</span> calls to SQL statements that\n        call these functions.\n      </p><p><b>Partitioning:</b></p><p>\n      Although you cannot use a <span class=\"hue-doc-codeph\">TIMESTAMP</span> column as a partition key, you can\n      extract the individual years, months, days, hours, and so on and partition based on those\n      columns. Because the partition key column values are represented in HDFS directory names,\n      rather than as fields in the data files themselves, you can also keep the original\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span> values if desired, without duplicating data or wasting storage\n      space. See <a class=\"hue-doc-external-link\" href=\"https://www.cloudera.com/documentation/enterprise/latest/topics/impala_partitioning.html#partition_key_columns\" target=\"_blank\">Partition Key Columns</a> for more\n      details on partitioning with date and time values.\n    </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; create table timeline (event string) partitioned by (happened timestamp);\nERROR: AnalysisException: Type 'TIMESTAMP' is not supported as partition-column type in column: happened\n</div><p><b>NULL considerations:</b> Casting any unrecognized <span class=\"hue-doc-codeph\">STRING</span> value to this type produces a\n        <span class=\"hue-doc-codeph\">NULL</span> value.\n      </p><p><b>Partitioning:</b> Because this type potentially has so many distinct values, it is often not a sensible\n        choice for a partition key column. For example, events 1 millisecond apart would be stored in different\n        partitions. Consider using the <span class=\"hue-doc-codeph\">TRUNC()</span> function to condense the number of distinct values,\n        and partition on a new column with the truncated values.\n      </p><p><b>HBase considerations:</b> This data type is fully compatible with HBase tables.\n      </p><p><b>Parquet considerations:</b> This type is fully compatible with Parquet tables.\n      </p><p><b>Text table considerations:</b> Values of this type are potentially larger in text tables than in tables\n        using Parquet or other binary formats.\n      </p><p><b>Internal details:</b> Represented in memory as a 16-byte value.\n      </p><p><b>Added in:</b> Available in all versions of Impala.\n      </p><p><b>Column statistics considerations:</b> Because this type has a fixed size, the maximum and average size\n        fields are always filled in for column statistics, even before you run the <span class=\"hue-doc-codeph\">COMPUTE STATS</span>\n        statement.\n      </p><p><b>Sqoop considerations:</b></p><p> If you use Sqoop to\n        convert RDBMS data to Parquet, be careful with interpreting any\n        resulting values from <span class=\"hue-doc-codeph\">DATE</span>, <span class=\"hue-doc-codeph\">DATETIME</span>,\n        or <span class=\"hue-doc-codeph\">TIMESTAMP</span> columns. The underlying values are\n        represented as the Parquet <span class=\"hue-doc-codeph\">INT64</span> type, which is\n        represented as <span class=\"hue-doc-codeph\">BIGINT</span> in the Impala table. The Parquet\n        values represent the time in milliseconds, while Impala interprets\n          <span class=\"hue-doc-codeph\">BIGINT</span> as the time in seconds. Therefore, if you have\n        a <span class=\"hue-doc-codeph\">BIGINT</span> column in a Parquet table that was imported\n        this way from Sqoop, divide the values by 1000 when interpreting as the\n          <span class=\"hue-doc-codeph\">TIMESTAMP</span> type.</p><p><b>Restrictions:</b></p><p>\n      If you cast a <span class=\"hue-doc-codeph\">STRING</span> with an unrecognized format to a\n      <span class=\"hue-doc-codeph\">TIMESTAMP</span>, the result is <span class=\"hue-doc-codeph\">NULL</span> rather than an error. Make\n      sure to test your data pipeline to be sure any textual date and time values are in a\n      format that Impala <span class=\"hue-doc-codeph\">TIMESTAMP</span> can recognize.\n    </p><p>\n        Currently, Avro tables cannot contain <span class=\"hue-doc-codeph\">TIMESTAMP</span> columns. If you need to store date and\n        time values in Avro tables, as a workaround you can use a <span class=\"hue-doc-codeph\">STRING</span> representation of the\n        values, convert the values to <span class=\"hue-doc-codeph\">BIGINT</span> with the <span class=\"hue-doc-codeph\">UNIX_TIMESTAMP()</span> function,\n        or create separate numeric columns for individual date and time fields using the <span class=\"hue-doc-codeph\">EXTRACT()</span>\n        function.\n      </p><p><b>Kudu considerations:</b></p><p>\n        In Impala 2.9 and higher, you can include <span class=\"hue-doc-codeph\">TIMESTAMP</span>\n        columns in Kudu tables, instead of representing the date and time as a <span class=\"hue-doc-codeph\">BIGINT</span>\n        value. The behavior of <span class=\"hue-doc-codeph\">TIMESTAMP</span> for Kudu tables has some special considerations:\n\n        <ul><li><p>\n              Any nanoseconds in the original 96-bit value produced by Impala are not stored, because\n              Kudu represents date/time columns using 64-bit values. The nanosecond portion of the value\n              is rounded, not truncated. Therefore, a <span class=\"hue-doc-codeph\">TIMESTAMP</span> value\n              that you store in a Kudu table might not be bit-for-bit identical to the value returned by a query.\n            </p></li><li><p>\n              The conversion between the Impala 96-bit representation and the Kudu 64-bit representation\n              introduces some performance overhead when reading or writing <span class=\"hue-doc-codeph\">TIMESTAMP</span>\n              columns. You can minimize the overhead during writes by performing inserts through the\n              Kudu API. Because the overhead during reads applies to each query, you might continue to\n              use a <span class=\"hue-doc-codeph\">BIGINT</span> column to represent date/time values  in performance-critical\n              applications.\n            </p></li><li><p>\n              The Impala <span class=\"hue-doc-codeph\">TIMESTAMP</span> type has a narrower range for years than the underlying\n              Kudu data type. Impala can represent years 1400-9999. If year values outside this range\n              are written to a Kudu table by a non-Impala client, Impala returns <span class=\"hue-doc-codeph\">NULL</span>\n              by default when reading those <span class=\"hue-doc-codeph\">TIMESTAMP</span> values during a query. Or, if the\n              <span class=\"hue-doc-codeph\">ABORT_ON_ERROR</span> query option is enabled, the query fails when it encounters\n              a value with an out-of-range year.\n            </p></li></ul></p><p><b>Examples:</b></p><p>\n      The following examples demonstrate using <span class=\"hue-doc-codeph\">TIMESTAMP</span> values with built-in\n      functions:\n    </p><div class=\"hue-doc-codeblock\">select cast('1966-07-30' as timestamp);\nselect cast('1985-09-25 17:45:30.005' as timestamp);\nselect cast('08:30:00' as timestamp);\nselect hour('1970-01-01 15:30:00');         -- Succeeds, returns 15.\nselect hour('1970-01-01 15:30');            -- Returns NULL because seconds field required.\nselect hour('1970-01-01 27:30:00');         -- Returns NULL because hour value out of range.\nselect dayofweek('2004-06-13');             -- Returns 1, representing Sunday.\nselect dayname('2004-06-13');               -- Returns 'Sunday'.\nselect date_add('2004-06-13', 365);         -- Returns 2005-06-13 with zeros for hh:mm:ss fields.\nselect day('2004-06-13');                   -- Returns 13.\nselect datediff('1989-12-31','1984-09-01'); -- How many days between these 2 dates?\nselect now();                               -- Returns current date and time in local timezone.\n</div><p>\n      The following examples demonstrate using <span class=\"hue-doc-codeph\">TIMESTAMP</span> values with\n      HDFS-backed tables:\n    </p><div class=\"hue-doc-codeblock\">create table dates_and_times (t timestamp);\ninsert into dates_and_times values\n  ('1966-07-30'), ('1985-09-25 17:45:30.005'), ('08:30:00'), (now());\n</div><p>\n      The following examples demonstrate using <span class=\"hue-doc-codeph\">TIMESTAMP</span> values with Kudu\n      tables:\n    </p><div class=\"hue-doc-codeblock\">create table timestamp_t (x int primary key, s string, t timestamp, b bigint)\n  partition by hash (x) partitions 16\n  stored as kudu;\n\n-- The default value of now() has microsecond precision, so the final 3 digits\n-- representing nanoseconds are all zero.\ninsert into timestamp_t values (1, cast(now() as string), now(), unix_timestamp(now()));\n\n-- Values with 1-499 nanoseconds are rounded down in the Kudu TIMESTAMP column.\ninsert into timestamp_t values (2, cast(now() + interval 100 nanoseconds as string), now() + interval 100 nanoseconds, unix_timestamp(now() + interval 100 nanoseconds));\ninsert into timestamp_t values (3, cast(now() + interval 499 nanoseconds as string), now() + interval 499 nanoseconds, unix_timestamp(now() + interval 499 nanoseconds));\n\n-- Values with 500-999 nanoseconds are rounded up in the Kudu TIMESTAMP column.\ninsert into timestamp_t values (4, cast(now() + interval 500 nanoseconds as string), now() + interval 500 nanoseconds, unix_timestamp(now() + interval 500 nanoseconds));\ninsert into timestamp_t values (5, cast(now() + interval 501 nanoseconds as string), now() + interval 501 nanoseconds, unix_timestamp(now() + interval 501 nanoseconds));\n\n-- The string representation shows how underlying Impala TIMESTAMP can have nanosecond precision.\n-- The TIMESTAMP column shows how timestamps in a Kudu table are rounded to microsecond precision.\n-- The BIGINT column represents seconds past the epoch and so if not affected much by nanoseconds.\nselect s, t, b from timestamp_t order by t;\n+-------------------------------+-------------------------------+------------+\n| s                             | t                             | b          |\n+-------------------------------+-------------------------------+------------+\n| 2017-05-31 15:30:05.107157000 | 2017-05-31 15:30:05.107157000 | 1496244605 |\n| 2017-05-31 15:30:28.868151100 | 2017-05-31 15:30:28.868151000 | 1496244628 |\n| 2017-05-31 15:34:33.674692499 | 2017-05-31 15:34:33.674692000 | 1496244873 |\n| 2017-05-31 15:35:04.769166500 | 2017-05-31 15:35:04.769167000 | 1496244904 |\n| 2017-05-31 15:35:33.033082501 | 2017-05-31 15:35:33.033083000 | 1496244933 |\n+-------------------------------+-------------------------------+------------+\n</div><p><b>Related information:</b></p><ul><li><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_literals.xml\" data-doc-anchor-id=\"timestamp_literals\">Timestamp Literals</a>.\n      </li><li>\n        To convert to or from different date formats, or perform date arithmetic, use the date\n        and time functions described in\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_datetime_functions.xml\" data-doc-anchor-id=\"datetime_functions\">Impala Date and Time Functions</a>. In\n        particular, the <span class=\"hue-doc-codeph\">from_unixtime()</span> function requires a case-sensitive\n        format string such as <span class=\"hue-doc-codeph\">\"yyyy-MM-dd HH:mm:ss.SSSS\"</span>, matching one of the\n        allowed variations of a <span class=\"hue-doc-codeph\">TIMESTAMP</span> value (date plus time, only date,\n        only time, optional fractional seconds).\n      </li><li>\n        See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_langref_unsupported.xml\" data-doc-anchor-id=\"langref_hiveql_delta\">SQL Differences Between Impala and Hive</a> for\n        details about differences in <span class=\"hue-doc-codeph\">TIMESTAMP</span> handling between Impala and\n        Hive.\n      </li></ul></div></div></div>","title":"TIMESTAMP Data Type"}