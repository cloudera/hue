{"body":"<div><div id=\"avro\"><div class=\"hue-doc-title\">Using the Avro File Format with Impala Tables</div><div><p> Impala supports using tables whose data files use the Avro\n      file format. Impala can query Avro tables. In Impala 1.4.0 and higher,\n      Impala can create Avro tables, but cannot insert data into them. For\n      insert operations, use Hive, then switch back to Impala to run queries. </p><table><div class=\"hue-doc-title\">Avro Format Support in Impala</div><thead><tr><td>\n              File Type\n            </td><td>\n              Format\n            </td><td>\n              Compression Codecs\n            </td><td>\n              Impala Can CREATE?\n            </td><td>\n              Impala Can INSERT?\n            </td></tr></thead><tbody><tr id=\"avro_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_avro.xml\" data-doc-anchor-id=\"avro\">Avro</a></td><td>\n              Structured\n            </td><td>\n              Snappy, gzip, deflate\n            </td><td>\n              Yes, in Impala 1.4.0 and higher. In lower versions, create the table using Hive.\n            </td><td>\n              No. Import data by using <span class=\"hue-doc-codeph\">LOAD DATA</span> on data files already in the\n              right format, or use <span class=\"hue-doc-codeph\">INSERT</span> in Hive followed by <span class=\"hue-doc-codeph\">REFRESH\n              <span class=\"hue-doc-varname\">table_name</span></span> in Impala.\n            </td></tr></tbody></table><p/></div><div id=\"avro_create_table\"><div class=\"hue-doc-title\">Creating Avro Tables</div><div><p>\n        To create a new table using the Avro file format, issue the <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement through\n        Impala with the <span class=\"hue-doc-codeph\">STORED AS AVRO</span> clause, or through Hive. If you create the table through\n        Impala, you must include column definitions that match the fields specified in the Avro schema. With Hive,\n        you can omit the columns and just specify the Avro schema.\n      </p><p>\n        In Impala 2.3 and higher, the <span class=\"hue-doc-codeph\">CREATE TABLE</span> for Avro tables can include\n        SQL-style column definitions rather than specifying Avro notation through the <span class=\"hue-doc-codeph\">TBLPROPERTIES</span>\n        clause. Impala issues warning messages if there are any mismatches between the types specified in the\n        SQL column definitions and the underlying types; for example, any <span class=\"hue-doc-codeph\">TINYINT</span> or\n        <span class=\"hue-doc-codeph\">SMALLINT</span> columns are treated as <span class=\"hue-doc-codeph\">INT</span> in the underlying Avro files,\n        and therefore are displayed as <span class=\"hue-doc-codeph\">INT</span> in any <span class=\"hue-doc-codeph\">DESCRIBE</span> or\n        <span class=\"hue-doc-codeph\">SHOW CREATE TABLE</span> output.\n      </p><div class=\"hue-doc-note\"><p id=\"avro_no_timestamp\">\n        Currently, Avro tables cannot contain <span class=\"hue-doc-codeph\">TIMESTAMP</span> columns. If you need to\n        store date and time values in Avro tables, as a workaround you can use a\n        <span class=\"hue-doc-codeph\">STRING</span> representation of the values, convert the values to\n        <span class=\"hue-doc-codeph\">BIGINT</span> with the <span class=\"hue-doc-codeph\">UNIX_TIMESTAMP()</span> function, or create\n        separate numeric columns for individual date and time fields using the\n        <span class=\"hue-doc-codeph\">EXTRACT()</span> function.\n      </p></div><p>\n        The following examples demonstrate creating an Avro table in Impala, using either an inline column\n        specification or one taken from a JSON file stored in HDFS:\n      </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; CREATE TABLE avro_only_sql_columns\n                  &gt; (\n                  &gt;   id INT,\n                  &gt;   bool_col BOOLEAN,\n                  &gt;   tinyint_col TINYINT, /* Gets promoted to INT */\n                  &gt;   smallint_col SMALLINT, /* Gets promoted to INT */\n                  &gt;   int_col INT,\n                  &gt;   bigint_col BIGINT,\n                  &gt;   float_col FLOAT,\n                  &gt;   double_col DOUBLE,\n                  &gt;   date_string_col STRING,\n                  &gt;   string_col STRING\n                  &gt; )\n                  &gt; STORED AS AVRO;\n\n[localhost:21000] &gt; CREATE TABLE impala_avro_table\n                  &gt; (bool_col BOOLEAN, int_col INT, long_col BIGINT, float_col FLOAT, double_col DOUBLE, string_col STRING, nullable_int INT)\n                  &gt; STORED AS AVRO\n                  &gt; TBLPROPERTIES ('avro.schema.literal'='{\n                  &gt;    \"name\": \"my_record\",\n                  &gt;    \"type\": \"record\",\n                  &gt;    \"fields\": [\n                  &gt;       {\"name\":\"bool_col\", \"type\":\"boolean\"},\n                  &gt;       {\"name\":\"int_col\", \"type\":\"int\"},\n                  &gt;       {\"name\":\"long_col\", \"type\":\"long\"},\n                  &gt;       {\"name\":\"float_col\", \"type\":\"float\"},\n                  &gt;       {\"name\":\"double_col\", \"type\":\"double\"},\n                  &gt;       {\"name\":\"string_col\", \"type\":\"string\"},\n                  &gt;       {\"name\": \"nullable_int\", \"type\": [\"null\", \"int\"]}]}');\n\n[localhost:21000] &gt; CREATE TABLE avro_examples_of_all_types (\n                  &gt;     id INT,\n                  &gt;     bool_col BOOLEAN,\n                  &gt;     tinyint_col TINYINT,\n                  &gt;     smallint_col SMALLINT,\n                  &gt;     int_col INT,\n                  &gt;     bigint_col BIGINT,\n                  &gt;     float_col FLOAT,\n                  &gt;     double_col DOUBLE,\n                  &gt;     date_string_col STRING,\n                  &gt;     string_col STRING\n                  &gt;   )\n                  &gt;   STORED AS AVRO\n                  &gt;   TBLPROPERTIES ('avro.schema.url'='hdfs://localhost:8020/avro_schemas/alltypes.json');\n</div><p>\n        The following example demonstrates creating an Avro table in Hive:\n      </p><div class=\"hue-doc-codeblock\">hive&gt; CREATE TABLE hive_avro_table\n    &gt; ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\n    &gt; STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\n    &gt; OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\n    &gt; TBLPROPERTIES ('avro.schema.literal'='{\n    &gt;    \"name\": \"my_record\",\n    &gt;    \"type\": \"record\",\n    &gt;    \"fields\": [\n    &gt;       {\"name\":\"bool_col\", \"type\":\"boolean\"},\n    &gt;       {\"name\":\"int_col\", \"type\":\"int\"},\n    &gt;       {\"name\":\"long_col\", \"type\":\"long\"},\n    &gt;       {\"name\":\"float_col\", \"type\":\"float\"},\n    &gt;       {\"name\":\"double_col\", \"type\":\"double\"},\n    &gt;       {\"name\":\"string_col\", \"type\":\"string\"},\n    &gt;       {\"name\": \"nullable_int\", \"type\": [\"null\", \"int\"]}]}');\n</div><p>\n        Each field of the record becomes a column of the table. Note that any other information, such as the record\n        name, is ignored.\n      </p><div class=\"hue-doc-note\">        For nullable Avro columns, make sure to put the <span class=\"hue-doc-codeph\">\"null\"</span> entry before the actual type name.\n        In Impala, all columns are nullable; Impala currently does not have a <span class=\"hue-doc-codeph\">NOT NULL</span> clause. Any\n        non-nullable property is only enforced on the Avro side.\n      </div><p>\n        Most column types map directly from Avro to Impala under the same names. These are the exceptions and\n        special cases to consider:\n      </p><ul><li>\n          The <span class=\"hue-doc-codeph\">DECIMAL</span> type is defined in Avro as a <span class=\"hue-doc-codeph\">BYTE</span> type with the\n          <span class=\"hue-doc-codeph\">logicalType</span> property set to <span class=\"hue-doc-codeph\">\"decimal\"</span> and a specified precision and\n          scale.\n        </li><li>\n          The Avro <span class=\"hue-doc-codeph\">long</span> type maps to <span class=\"hue-doc-codeph\">BIGINT</span> in Impala.\n        </li></ul><p>\n        If you create the table through Hive, switch back to <span class=\"hue-doc-cmdname\">impala-shell</span> and issue an\n        <span class=\"hue-doc-codeph\">INVALIDATE METADATA <span class=\"hue-doc-varname\">table_name</span></span> statement. Then you can run queries for\n        that table through <span class=\"hue-doc-cmdname\">impala-shell</span>.\n      </p><p>\n        In rare instances, a mismatch could occur between the Avro schema and the column definitions in the\n        metastore database. In Impala 2.3 and higher, Impala checks for such inconsistencies during\n        a <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement and each time it loads the metadata for a table (for example,\n        after <span class=\"hue-doc-codeph\">INVALIDATE METADATA</span>). Impala uses the following rules to determine how to treat\n        mismatching columns, a process known as <span class=\"hue-doc-term\">schema reconciliation</span>:\n        <ul><li>\n          If there is a mismatch in the number of columns, Impala uses the column\n          definitions from the Avro schema.\n        </li><li>\n          If there is a mismatch in column name or type, Impala uses the column definition from the Avro schema.\n          Because a <span class=\"hue-doc-codeph\">CHAR</span> or <span class=\"hue-doc-codeph\">VARCHAR</span> column in Impala maps to an Avro <span class=\"hue-doc-codeph\">STRING</span>,\n          this case is not considered a mismatch and the column is preserved as <span class=\"hue-doc-codeph\">CHAR</span> or <span class=\"hue-doc-codeph\">VARCHAR</span>\n          in the reconciled schema. <span class=\"hue-doc-ph\">Prior to Impala 2.7 the column\n          name and comment for such <span class=\"hue-doc-codeph\">CHAR</span> and <span class=\"hue-doc-codeph\">VARCHAR</span> columns was also taken from the SQL column definition.\n          In Impala 2.7 and higher, the column name and comment from the Avro schema file take precedence for such columns,\n          and only the <span class=\"hue-doc-codeph\">CHAR</span> or <span class=\"hue-doc-codeph\">VARCHAR</span> type is preserved from the SQL column definition.</span></li><li>\n          An Impala <span class=\"hue-doc-codeph\">TIMESTAMP</span> column definition maps to an Avro <span class=\"hue-doc-codeph\">STRING</span> and is presented as a <span class=\"hue-doc-codeph\">STRING</span>\n          in the reconciled schema, because Avro has no binary <span class=\"hue-doc-codeph\">TIMESTAMP</span> representation.\n          As a result, no Avro table can have a <span class=\"hue-doc-codeph\">TIMESTAMP</span> column; this restriction is the same as\n          in earlier Impala releases.\n        </li></ul></p><p id=\"complex_types_unsupported_filetype\"><b>Complex type considerations:</b> Although you can create tables in this file format\n        using the complex types (<span class=\"hue-doc-codeph\">ARRAY</span>, <span class=\"hue-doc-codeph\">STRUCT</span>, and\n        <span class=\"hue-doc-codeph\">MAP</span>) available in Impala 2.3 and higher,\n        currently, Impala can query these types only in Parquet tables. <span class=\"hue-doc-ph\">\n        The one exception to the preceding rule is <span class=\"hue-doc-codeph\">COUNT(*)</span> queries on RCFile\n        tables that include complex types. Such queries are allowed in\n        Impala 2.6 and higher. </span></p></div></div><div id=\"avro_map_table\"><div class=\"hue-doc-title\">Using a Hive-Created Avro Table in Impala</div><div><p>\n        If you have an Avro table created through Hive, you can use it in Impala as long as it contains only\n        Impala-compatible data types. It cannot contain:\n        <ul><li>\n            Complex types: <span class=\"hue-doc-codeph\">array</span>, <span class=\"hue-doc-codeph\">map</span>, <span class=\"hue-doc-codeph\">record</span>,\n            <span class=\"hue-doc-codeph\">struct</span>, <span class=\"hue-doc-codeph\">union</span> other than\n            <span class=\"hue-doc-codeph\">[<span class=\"hue-doc-varname\">supported_type</span>,null]</span> or\n            <span class=\"hue-doc-codeph\">[null,<span class=\"hue-doc-varname\">supported_type</span>]</span></li><li>\n            The Avro-specific types <span class=\"hue-doc-codeph\">enum</span>, <span class=\"hue-doc-codeph\">bytes</span>, and <span class=\"hue-doc-codeph\">fixed</span></li><li>\n            Any scalar type other than those listed in <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_datatypes.xml\" data-doc-anchor-id=\"datatypes\">Data Types</a></li></ul>\n        Because Impala and Hive share the same metastore database, Impala can directly access the table definitions\n        and data for tables that were created in Hive.\n      </p><p>\n        If you create an Avro table in Hive, issue an <span class=\"hue-doc-codeph\">INVALIDATE METADATA</span> the next time you\n        connect to Impala through <span class=\"hue-doc-cmdname\">impala-shell</span>. This is a one-time operation to make Impala\n        aware of the new table. You can issue the statement while connected to any Impala node, and the catalog\n        service broadcasts the change to all other Impala nodes.\n      </p><p>\n        If you load new data into an Avro table through Hive, either through a Hive <span class=\"hue-doc-codeph\">LOAD DATA</span> or\n        <span class=\"hue-doc-codeph\">INSERT</span> statement, or by manually copying or moving files into the data directory for the\n        table, issue a <span class=\"hue-doc-codeph\">REFRESH <span class=\"hue-doc-varname\">table_name</span></span> statement the next time you connect\n        to Impala through <span class=\"hue-doc-cmdname\">impala-shell</span>. You can issue the statement while connected to any\n        Impala node, and the catalog service broadcasts the change to all other Impala nodes. If you issue the\n        <span class=\"hue-doc-codeph\">LOAD DATA</span> statement through Impala, you do not need a <span class=\"hue-doc-codeph\">REFRESH</span> afterward.\n      </p><p>\n        Impala only supports fields of type <span class=\"hue-doc-codeph\">boolean</span>, <span class=\"hue-doc-codeph\">int</span>, <span class=\"hue-doc-codeph\">long</span>,\n        <span class=\"hue-doc-codeph\">float</span>, <span class=\"hue-doc-codeph\">double</span>, and <span class=\"hue-doc-codeph\">string</span>, or unions of these types with\n        null; for example, <span class=\"hue-doc-codeph\">[\"string\", \"null\"]</span>. Unions with <span class=\"hue-doc-codeph\">null</span> essentially\n        create a nullable type.\n      </p></div></div><div id=\"avro_json\"><div class=\"hue-doc-title\">Specifying the Avro Schema through JSON</div><div><p>\n        While you can embed a schema directly in your <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement, as shown above,\n        column width restrictions in the Hive metastore limit the length of schema you can specify. If you\n        encounter problems with long schema literals, try storing your schema as a <span class=\"hue-doc-codeph\">JSON</span> file in\n        HDFS instead. Specify your schema in HDFS using table properties similar to the following:\n      </p><div class=\"hue-doc-codeblock\">tblproperties ('avro.schema.url'='hdfs//your-name-node:port/path/to/schema.json');</div></div></div><div id=\"avro_load_data\"><div class=\"hue-doc-title\">Loading Data into an Avro Table</div><div><p>\n        Currently, Impala cannot write Avro data files. Therefore, an Avro table cannot be used as the destination\n        of an Impala <span class=\"hue-doc-codeph\">INSERT</span> statement or <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span>.\n      </p><p>\n        To copy data from another table, issue any <span class=\"hue-doc-codeph\">INSERT</span> statements through Hive. For information\n        about loading data into Avro tables through Hive, see\n        <a class=\"hue-doc-external-link\" href=\"https://cwiki.apache.org/confluence/display/Hive/AvroSerDe\" target=\"_blank\">Avro\n        page on the Hive wiki</a>.\n      </p><p>\n        If you already have data files in Avro format, you can also issue <span class=\"hue-doc-codeph\">LOAD DATA</span> in either\n        Impala or Hive. Impala can move existing Avro data files into an Avro table, it just cannot create new\n        Avro data files.\n      </p></div></div><div id=\"avro_compression\"><div class=\"hue-doc-title\">Enabling Compression for Avro Tables</div><div><p>\n        To enable compression for Avro tables, specify settings in the Hive shell to enable compression and to\n        specify a codec, then issue a <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement as in the preceding examples. Impala\n        supports the <span class=\"hue-doc-codeph\">snappy</span> and <span class=\"hue-doc-codeph\">deflate</span> codecs for Avro tables.\n      </p><p>\n        For example:\n      </p><div class=\"hue-doc-codeblock\">hive&gt; set hive.exec.compress.output=true;\nhive&gt; set avro.output.codec=snappy;</div></div></div><div id=\"avro_schema_evolution\"><div class=\"hue-doc-title\">How Impala Handles Avro Schema Evolution</div><div><p>\n        Starting in Impala 1.1, Impala can deal with Avro data files that employ <span class=\"hue-doc-term\">schema evolution</span>,\n        where different data files within the same table use slightly different type definitions. (You would\n        perform the schema evolution operation by issuing an <span class=\"hue-doc-codeph\">ALTER TABLE</span> statement in the Hive\n        shell.) The old and new types for any changed columns must be compatible, for example a column might start\n        as an <span class=\"hue-doc-codeph\">int</span> and later change to a <span class=\"hue-doc-codeph\">bigint</span> or <span class=\"hue-doc-codeph\">float</span>.\n      </p><p>\n        As with any other tables where the definitions are changed or data is added outside of the current\n        <span class=\"hue-doc-cmdname\">impalad</span> node, ensure that Impala loads the latest metadata for the table if the Avro\n        schema is modified through Hive. Issue a <span class=\"hue-doc-codeph\">REFRESH <span class=\"hue-doc-varname\">table_name</span></span> or\n        <span class=\"hue-doc-codeph\">INVALIDATE METADATA <span class=\"hue-doc-varname\">table_name</span></span> statement. <span class=\"hue-doc-codeph\">REFRESH</span>\n        reloads the metadata immediately, <span class=\"hue-doc-codeph\">INVALIDATE METADATA</span> reloads the metadata the next time\n        the table is accessed.\n      </p><p>\n        When Avro data files or columns are not consulted during a query, Impala does not check for consistency.\n        Thus, if you issue <span class=\"hue-doc-codeph\">SELECT c1, c2 FROM t1</span>, Impala does not return any error if the column\n        <span class=\"hue-doc-codeph\">c3</span> changed in an incompatible way. If a query retrieves data from some partitions but not\n        others, Impala does not check the data files for the unused partitions.\n      </p><p>\n        In the Hive DDL statements, you can specify an <span class=\"hue-doc-codeph\">avro.schema.literal</span> table property (if the\n        schema definition is short) or an <span class=\"hue-doc-codeph\">avro.schema.url</span> property (if the schema definition is\n        long, or to allow convenient editing for the definition).\n      </p><p>\n        For example, running the following SQL code in the Hive shell creates a table using the Avro file format\n        and puts some sample data into it:\n      </p><div class=\"hue-doc-codeblock\">CREATE TABLE avro_table (a string, b string)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nTBLPROPERTIES (\n  'avro.schema.literal'='{\n    \"type\": \"record\",\n    \"name\": \"my_record\",\n    \"fields\": [\n      {\"name\": \"a\", \"type\": \"int\"},\n      {\"name\": \"b\", \"type\": \"string\"}\n    ]}');\n\nINSERT OVERWRITE TABLE avro_table SELECT 1, \"avro\" FROM functional.alltypes LIMIT 1;\n</div><p>\n        Once the Avro table is created and contains data, you can query it through the\n        <span class=\"hue-doc-cmdname\">impala-shell</span> command:\n      </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; select * from avro_table;\n+---+------+\n| a | b    |\n+---+------+\n| 1 | avro |\n+---+------+\n</div><p>\n        Now in the Hive shell, you change the type of a column and add a new column with a default value:\n      </p><div class=\"hue-doc-codeblock\">-- Promote column \"a\" from INT to FLOAT (no need to update Avro schema)\nALTER TABLE avro_table CHANGE A A FLOAT;\n\n-- Add column \"c\" with default\nALTER TABLE avro_table ADD COLUMNS (c int);\nALTER TABLE avro_table SET TBLPROPERTIES (\n  'avro.schema.literal'='{\n    \"type\": \"record\",\n    \"name\": \"my_record\",\n    \"fields\": [\n      {\"name\": \"a\", \"type\": \"int\"},\n      {\"name\": \"b\", \"type\": \"string\"},\n      {\"name\": \"c\", \"type\": \"int\", \"default\": 10}\n    ]}');\n</div><p>\n        Once again in <span class=\"hue-doc-cmdname\">impala-shell</span>, you can query the Avro table based on its latest schema\n        definition. Because the table metadata was changed outside of Impala, you issue a <span class=\"hue-doc-codeph\">REFRESH</span>\n        statement first so that Impala has up-to-date metadata for the table.\n      </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; refresh avro_table;\n[localhost:21000] &gt; select * from avro_table;\n+---+------+----+\n| a | b    | c  |\n+---+------+----+\n| 1 | avro | 10 |\n+---+------+----+\n</div></div></div><div id=\"avro_data_types\"><div class=\"hue-doc-title\">Data Type Considerations for Avro Tables</div><div><p> The Avro format defines a set of data types whose names differ from\n        the names of the corresponding Impala data types. If you are preparing\n        Avro files using other Hadoop components such as Pig or MapReduce, you\n        might need to work with the type names defined by Avro. The following\n        figure lists the Avro-defined types and the equivalent types in Impala. </p><p><b>Primitive types:</b></p><table id=\"table_uvv_plj_gjb\"><thead><tr><td>Avro type</td><td>Impala type</td></tr></thead><tbody><tr><td>STRING</td><td>STRING</td></tr><tr><td>STRING</td><td>CHAR</td></tr><tr><td>STRING</td><td>VARCHAR</td></tr><tr><td>INT</td><td>INT</td></tr><tr><td>BOOLEAN</td><td>BOOLEAN</td></tr><tr><td>LONG</td><td>BIGINT</td></tr><tr><td>FLOAT</td><td>FLOAT</td></tr><tr><td>DOUBLE</td><td>DOUBLE</td></tr></tbody></table><p>The Avro specification allows string values up to 2**64 bytes in\n        length. Impala queries for Avro tables use 32-bit integers to hold\n        string lengths. </p><p>In Impala 2.5 and higher, Impala truncates\n          <span class=\"hue-doc-codeph\">CHAR</span> and <span class=\"hue-doc-codeph\">VARCHAR</span> values in Avro\n        tables to (2**31)-1 bytes. If a query encounters a\n          <span class=\"hue-doc-codeph\">STRING</span> value longer than (2**31)-1 bytes in an Avro\n        table, the query fails. In earlier releases, encountering such long\n        values in an Avro table could cause a crash.</p><p><b>Logical types:</b></p><table id=\"table_ch2_1mj_gjb\"><thead><tr><td>Avro type</td><td>Impala type</td></tr></thead><tbody><tr><td>BYTES annotated</td><td>DECIMAL</td></tr><tr><td>INT32 annotated</td><td>DATE</td></tr></tbody></table><p>Impala does not support the following Avro data types: RECORD, MAP,\n        ARRAY, UNION,  ENUM, FIXED, NULL</p></div></div><div id=\"avro_performance\"><div class=\"hue-doc-title\">Query Performance for Impala Avro Tables</div><div><p>\n        In general, expect query performance with Avro tables to be\n        faster than with tables using text data, but slower than with\n        Parquet tables. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet\">Using the Parquet File Format with Impala Tables</a>\n        for information about using the Parquet file format for\n        high-performance analytic queries.\n      </p><p id=\"s3_block_splitting\">\n        In Impala 2.6 and higher, Impala queries are optimized for files\n        stored in Amazon S3. For Impala tables that use the file formats Parquet, ORC, RCFile,\n        SequenceFile, Avro, and uncompressed text, the setting\n        <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> in the <span class=\"hue-doc-filepath\">core-site.xml</span>\n        configuration file determines how Impala divides the I/O work of reading the data files.\n        This configuration setting is specified in bytes. By default, this value is 33554432 (32\n        MB), meaning that Impala parallelizes S3 read operations on the files as if they were\n        made up of 32 MB blocks. For example, if your S3 queries primarily access Parquet files\n        written by MapReduce or Hive, increase <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 134217728\n        (128 MB) to match the row group size of those files. If most S3 queries involve Parquet\n        files written by Impala, increase <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 268435456 (256\n        MB) to match the row group size produced by Impala.\n      </p></div></div></div></div>","title":"Using the Avro File Format with Impala Tables"}