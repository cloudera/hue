{"body":"<div><div id=\"admission_control\"><div class=\"hue-doc-title\">Admission Control and Query Queuing</div><div><p id=\"admission_control_intro\"> Admission control is an Impala feature that\n      imposes limits on concurrent SQL queries, to avoid resource usage spikes\n      and out-of-memory conditions on busy clusters. The admission control\n      feature lets you set an upper limit on the number of concurrent Impala\n      queries and on the memory used by those queries. Any additional queries\n      are queued until the earlier ones finish, rather than being cancelled or\n      running slowly and causing contention. As other queries finish, the queued\n      queries are allowed to proceed. </p><p> In Impala 2.5 and higher, you can\n      specify these limits and thresholds for each pool rather than globally.\n      That way, you can balance the resource usage and throughput between steady\n      well-defined workloads, rare resource-intensive queries, and ad-hoc\n      exploratory queries. </p><p> In addition to the threshold values for currently executing queries, you\n      can place limits on the maximum number of queries that are queued\n      (waiting) and a limit on the amount of time they might wait before\n      returning with an error. These queue settings let you ensure that queries\n      do not wait indefinitely so that you can detect and correct\n        <q>starvation</q> scenarios. </p><p> Queries, DML statements, and some DDL statements, including\n        <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span> and <span class=\"hue-doc-codeph\">COMPUTE\n        STATS</span> are affected by admission control. </p><p> On a busy cluster, you might find there is an optimal number of Impala\n      queries that run concurrently. For example, when the I/O capacity is fully\n      utilized by I/O-intensive queries, you might not find any throughput\n      benefit in running more concurrent queries. By allowing some queries to\n      run at full speed while others wait, rather than having all queries\n      contend for resources and run slowly, admission control can result in\n      higher overall throughput. </p><p> For another example, consider a memory-bound workload such as many large\n      joins or aggregation queries. Each such query could briefly use many\n      gigabytes of memory to process intermediate results. Because Impala by\n      default cancels queries that exceed the specified memory limit, running\n      multiple large-scale queries at once might require re-running some queries\n      that are cancelled. In this case, admission control improves the\n      reliability and stability of the overall workload by only allowing as many\n      concurrent queries as the overall memory of the cluster can accommodate. </p><p/></div><div id=\"admission_concurrency\"><div class=\"hue-doc-title\">Concurrent Queries and Admission Control</div><div><p> One way to limit resource usage through admission control is to set an\n        upper limit on the number of concurrent queries. This is the initial\n        technique you might use when you do not have extensive information about\n        memory usage for your workload. The settings can be specified separately\n        for each dynamic resource pool. </p><dl><dt> Max Running Queries </dt><dd><p>Maximum number of concurrently running queries in this pool.\n              The default value is unlimited for Impala 2.5 or higher.\n              (optional)</p> The maximum number of queries that can run\n            concurrently in this pool. The default value is unlimited. Any\n            queries for this pool that exceed <div class=\"hue-doc-uicontrol\">Max Running\n              Queries</div> are added to the admission control queue until\n            other queries finish. You can use <div class=\"hue-doc-uicontrol\">Max Running\n              Queries</div> in the early stages of resource management,\n            when you do not have extensive data about query memory usage, to\n            determine if the cluster performs better overall if throttling is\n            applied to Impala queries. <p> For a workload with many small\n              queries, you typically specify a high value for this setting, or\n              leave the default setting of <q>unlimited</q>. For a workload with\n              expensive queries, where some number of concurrent queries\n              saturate the memory, I/O, CPU, or network capacity of the cluster,\n              set the value low enough that the cluster resources are not\n              overcommitted for Impala. </p><p>Once you have enabled\n              memory-based admission control using other pool settings, you can\n              still use <div class=\"hue-doc-uicontrol\">Max Running Queries</div> as a\n              safeguard. If queries exceed either the total estimated memory or\n              the maximum number of concurrent queries, they are added to the\n              queue. </p><p>If <div class=\"hue-doc-uicontrol\">Max Running Queries\n                Multiple</div> is set, the <div class=\"hue-doc-uicontrol\">Max Running\n                Queries</div> setting is ignored.</p></dd><dt>Max Running Queries Multiple</dt><dd>This floating point number is multiplied by the current total\n            number of executors at runtime to give the maximum number of\n            concurrently running queries allowed in the pool. The effect of this\n            setting scales with the number of executors in the resource\n              pool.<p>This calculation is rounded up to the nearest integer, so\n              the result will always be at least one. </p><p>If set to zero or a\n              negative number, the setting is ignored.</p></dd><dt> Max Queued Queries </dt><dd> Maximum number of queries that can be queued in this pool. The\n            default value is 200 for Impala 2.1 or higher and 50 for previous\n            versions of Impala. (optional)<p>If <div class=\"hue-doc-uicontrol\">Max Queued Queries\n                Multiple</div> is set, the <div class=\"hue-doc-uicontrol\">Max Queued\n                Queries</div> setting is ignored.</p></dd><dt>Max Queued Queries Multiple</dt><dd>This floating point number is multiplied by the current total\n            number of executors at runtime to give the maximum number of queries\n            that can be queued in the pool. The effect of this setting scales\n            with the number of executors in the resource pool.<p>This\n              calculation is rounded up to the nearest integer, so the result\n              will always be at least one. </p><p>If set to zero or a negative\n              number, the setting is ignored.</p></dd><dt> Queue Timeout </dt><dd> The amount of time, in milliseconds, that a query waits in the\n            admission control queue for this pool before being canceled. The\n            default value is 60,000 milliseconds. <p>In the following cases,\n                <div class=\"hue-doc-uicontrol\">Queue Timeout</div> is not significant, and you\n              can specify a high value to avoid canceling queries\n                unexpectedly:<ul id=\"ul_kzr_rbg_gw\"><li>In a low-concurrency workload where few or no queries are\n                  queued</li><li>In an environment without a strict SLA, where it does not\n                  matter if queries occasionally take longer than usual because\n                  they are held in admission control</li></ul>You might also need to increase the value to use Impala with\n              some business intelligence tools that have their own timeout\n              intervals for queries. </p><p>In a high-concurrency workload,\n              especially for queries with a tight SLA, long wait times in\n              admission control can cause a serious problem. For example, if a\n              query needs to run in 10 seconds, and you have tuned it so that it\n              runs in 8 seconds, it violates its SLA if it waits in the\n              admission control queue longer than 2 seconds. In a case like\n              this, set a low timeout value and monitor how many queries are\n              cancelled because of timeouts. This technique helps you to\n              discover capacity, tuning, and scaling problems early, and helps\n              avoid wasting resources by running expensive queries that have\n              already missed their SLA. </p><p> If you identify some queries\n              that can have a high timeout value, and others that benefit from a\n              low timeout value, you can create separate pools with different\n              values for this setting. </p></dd></dl><p> You can combine these settings with the memory-based approach\n        described in <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_admission.xml\" data-doc-anchor-id=\"admission_memory\">Memory Limits and Admission Control</a>. If\n        either the maximum number of or the expected memory usage of the\n        concurrent queries is exceeded, subsequent queries are queued until the\n        concurrent workload falls below the threshold again. </p></div></div><div id=\"admission_memory\"><div class=\"hue-doc-title\">Memory Limits and Admission Control</div><div><p>\n        Each dynamic resource pool can have an upper limit on the cluster-wide memory used by queries executing in that pool.\n        This is the technique to use once you have a stable workload with well-understood memory requirements.\n      </p><p>Use the following settings to manage memory-based admission\n        control.</p><dl><dt>Max Memory</dt><dd><p>\n              The maximum amount of aggregate memory available across the\n              cluster to all queries executing in this pool. This should be a\n              portion of the aggregate configured memory for Impala daemons,\n              which will be shown in the settings dialog next to this option for\n              convenience. Setting this to a non-zero value enables memory based\n              admission control.\n            </p><p>\n              Impala determines the expected maximum memory used by all\n              queries in the pool and holds back any further queries that would\n              result in Max Memory being exceeded.\n            </p><p>\n              You set Max Memory in <span class=\"hue-doc-codeph\">fair-scheduler.xml</span> file\n              with the <span class=\"hue-doc-codeph\">maxResources</span> tag. For example:\n                <span class=\"hue-doc-codeph\">&lt;maxResources&gt;2500 mb&lt;/maxResources&gt;</span></p><p>\n              If you specify Max Memory, you should specify the amount of\n              memory to allocate to each query in this pool. You can do this in\n              two ways:\n            </p><ul><li>By setting Maximum Query Memory Limit and Minimum Query Memory\n                Limit. This is preferred in Impala 3.1\n                and greater and gives Impala flexibility to set aside more\n                memory to queries that are expected to be memory-hungry.</li><li>By setting Default Query Memory Limit to the exact amount of\n                memory that Impala should set aside for queries in that\n                pool.</li></ul><p>\n              Note that in the following cases, Impala will rely entirely on\n              memory estimates to determine how much memory to set aside for\n              each query. This is not recommended because it can result in\n              queries not running or being starved for memory if the estimates\n              are inaccurate. And it can affect other queries running on the\n              same node.\n              <ul><li>Max Memory, Maximum Query Memory Limit, and Minimum Query\n                  Memory Limit are not set, and the <span class=\"hue-doc-codeph\">MEM_LIMIT</span>\n                  query option is not set for the query.</li><li>Default Query Memory Limit is set to 0, and the\n                    <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option is not set for the\n                  query.</li></ul></p><p>If <div class=\"hue-doc-uicontrol\">Max Memory Multiple</div> is set, the\n                <div class=\"hue-doc-uicontrol\">Max Memory</div> setting is ignored.</p></dd><dt>Max Memory Multiple</dt><dd> This number of bytes is multiplied by the current total number of\n            executors at runtime to give the maximum memory available across the\n            cluster for the pool. The effect of this setting scales with the\n            number of executors in the resource pool.<p>If set to zero or a\n              negative number, the setting is ignored.</p></dd><dt>Minimum Query Memory Limit and Maximum Query Memory Limit</dt><dd><p>These two options determine the minimum and maximum per-host\n              memory limit that will be chosen by Impala Admission control for\n              queries in this resource pool. If set, Impala Admission Control\n              will choose a memory limit between the minimum and maximum values\n              based on the per-host memory estimate for the query. The memory\n              limit chosen determines the amount of memory that Impala Admission\n              control will set aside for this query on each host that the query\n              is running on. The aggregate memory across all of the hosts that\n              the query is running on is counted against the pool’s Max\n              Memory.</p><p>Minimum Query Memory Limit must be less than or equal to Maximum\n              Query Memory Limit and Max Memory.</p><p>A user can override Impala’s choice of memory limit by setting\n              the <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option. If the Clamp\n              MEM_LIMIT Query Option setting is set to <span class=\"hue-doc-codeph\">TRUE</span> and\n              the user sets <span class=\"hue-doc-codeph\">MEM_LIMIT</span> to a value that is\n              outside of the range specified by these two options, then the\n              effective memory limit will be either the minimum or maximum,\n              depending on whether <span class=\"hue-doc-codeph\">MEM_LIMIT</span> is lower than or\n              higher the range.</p><p>For example, assume a resource pool with the following parameters\n              set: <ul><li>Minimum Query Memory Limit = 2GB</li><li>Maximum Query Memory Limit = 10GB</li></ul>If a user tries to submit a query with the\n                <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option set to 14 GB, the\n              following would happen:<ul><li>If Clamp MEM_LIMIT Query Option = true, admission controller\n                  would override <span class=\"hue-doc-codeph\">MEM_LIMIT</span> with 10 GB and\n                  attempt admission using that value.</li><li>If Clamp MEM_LIMIT Query Option = false, the admission\n                  controller will retain the <span class=\"hue-doc-codeph\">MEM_LIMIT</span> of 14 GB\n                  set by the user and will attempt admission using the\n                  value.</li></ul></p></dd><dt>Default Query Memory Limit</dt><dd>The default memory limit applied to queries executing in this pool\n            when no explicit <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option is set. The\n            memory limit chosen determines the amount of memory that Impala\n            Admission control will set aside for this query on each host that\n            the query is running on. The aggregate memory across all of the\n            hosts that the query is running on is counted against the pool’s Max\n            Memory. This option is deprecated in Impala 3.1 and higher and is replaced by Maximum Query Memory Limit and\n            Minimum Query Memory Limit. Do not set this if either Maximum Query\n            Memory Limit or Minimum Query Memory Limit is set.</dd></dl><dl><dt> Clamp MEM_LIMIT Query Option</dt><dd>If this field is not selected, the <span class=\"hue-doc-codeph\">MEM_LIMIT</span>\n            query option will not be bounded by the <b>Maximum Query Memory\n              Limit</b> and the <b>Minimum Query Memory Limit</b> values\n            specified for this resource pool. By default, this field is selected\n            in Impala 3.1 and higher. The field is disabled if both <b>Minimum\n              Query Memory Limit</b> and <b>Maximum Query Memory Limit</b> are\n            not set.</dd></dl><p id=\"admission_control_mem_limit_interaction\">\n        For example, consider the following scenario:\n        <ul><li>\n            The cluster is running <span class=\"hue-doc-codeph\">impalad</span> daemons on five hosts.\n          </li><li>\n            A dynamic resource pool has Max Memory set to 100 GB.\n          </li><li>\n            The Maximum Query Memory Limit for the pool is 10 GB and Minimum Query Memory Limit\n            is 2 GB. Therefore, any query running in this pool could use up to 50 GB of memory\n            (Maximum Query Memory Limit * number of Impala nodes).\n          </li><li>\n            Impala will execute varying numbers of queries concurrently because queries may be\n            given memory limits anywhere between 2 GB and 10 GB, depending on the estimated\n            memory requirements. For example, Impala may execute up to 10 small queries with 2\n            GB memory limits or two large queries with 10 GB memory limits because that is what\n            will fit in the 100 GB cluster-wide limit when executing on five hosts.\n          </li><li>\n            The executing queries may use less memory than the per-host memory limit or the Max\n            Memory cluster-wide limit if they do not need that much memory. In general this is\n            not a problem so long as you are able to execute enough queries concurrently to meet\n            your needs.\n          </li></ul></p><p>\n        You can combine the memory-based settings with the upper limit on concurrent queries described in\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_admission.xml\" data-doc-anchor-id=\"admission_concurrency\">Concurrent Queries and Admission Control</a>. If either the maximum number of\n        or the expected memory usage of the concurrent queries is exceeded, subsequent queries\n        are queued until the concurrent workload falls below the threshold again.\n      </p></div></div><div id=\"set_per_query_memory_limits\"><div class=\"hue-doc-title\">Setting Per-query Memory Limits</div><div><p>Use per-query memory limits to prevent queries from consuming excessive\n        memory resources that impact other queries. We recommends that you set\n        the query memory limits whenever possible.</p><p>If you set the <b>Max Memory</b> for a resource pool, Impala attempts\n        to throttle queries if there is not enough memory to run them within the\n        specified resources.</p><p>Only use admission control with maximum memory resources if you can\n        ensure there are query memory limits. Set the pool <b>Maximum Query\n          Memory Limit</b> to be certain. You can override this setting with the\n          <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option, if necessary.</p><p>Typically, you set query memory limits using the <span class=\"hue-doc-codeph\">set\n          MEM_LIMIT=Xg;</span> query option. When you find the right value for\n        your business case, memory-based admission control works well. The\n        potential downside is that queries that attempt to use more memory might\n        perform poorly or even be cancelled.</p></div></div><div id=\"admission_yarn\"><div class=\"hue-doc-title\">How Impala Admission Control Relates to Other Resource Management Tools</div><div><p>\n        The admission control feature is similar in some ways to the YARN resource management framework. These features\n        can be used separately or together. This section describes some similarities and differences, to help you\n        decide which combination of resource management features to use for Impala.\n      </p><p>\n        Admission control is a lightweight, decentralized system that is suitable for workloads consisting\n        primarily of Impala queries and other SQL statements. It sets <q>soft</q> limits that smooth out Impala\n        memory usage during times of heavy load, rather than taking an all-or-nothing approach that cancels jobs\n        that are too resource-intensive.\n      </p><p>\n        Because the admission control system does not interact with other Hadoop workloads such as MapReduce jobs, you\n        might use YARN with static service pools on clusters where resources are shared between\n        Impala and other Hadoop components. This configuration is recommended when using Impala in a\n        <span class=\"hue-doc-term\">multitenant</span> cluster. Devote a percentage of cluster resources to Impala, and allocate another\n        percentage for MapReduce and other batch-style workloads. Let admission control handle the concurrency and\n        memory usage for the Impala work within the cluster, and let YARN manage the work for other components within the\n        cluster. In this scenario, Impala's resources are not managed by YARN.\n      </p><p>\n        The Impala admission control feature uses the same configuration mechanism as the YARN resource manager to map users to\n        pools and authenticate them.\n      </p><p>\n        Although the Impala admission control feature uses a <span class=\"hue-doc-codeph\">fair-scheduler.xml</span> configuration file\n        behind the scenes, this file does not depend on which scheduler is used for YARN. You still use this file\n        even when YARN is using the capacity scheduler.\n      </p></div></div><div id=\"admission_architecture\"><div class=\"hue-doc-title\">How Impala Schedules and Enforces Limits on Concurrent Queries</div><div><p>\n        The admission control system is decentralized, embedded in each Impala daemon and communicating through the\n        statestore mechanism. Although the limits you set for memory usage and number of concurrent queries apply\n        cluster-wide, each Impala daemon makes its own decisions about whether to allow each query to run\n        immediately or to queue it for a less-busy time. These decisions are fast, meaning the admission control\n        mechanism is low-overhead, but might be imprecise during times of heavy load across many coordinators. There could be times when the\n        more queries were queued (in aggregate across the cluster) than the specified limit, or when number of admitted queries\n        exceeds the expected number. Thus, you typically err on the\n        high side for the size of the queue, because there is not a big penalty for having a large number of queued\n        queries; and you typically err on the low side for configuring memory resources, to leave some headroom in case more\n        queries are admitted than expected, without running out of memory and being cancelled as a result.\n      </p><p>\n        To avoid a large backlog of queued requests, you can set an upper limit on the size of the queue for\n        queries that are queued. When the number of queued queries exceeds this limit, further queries are\n        cancelled rather than being queued. You can also configure a timeout period per pool, after which queued queries are\n        cancelled, to avoid indefinite waits. If a cluster reaches this state where queries are cancelled due to\n        too many concurrent requests or long waits for query execution to begin, that is a signal for an\n        administrator to take action, either by provisioning more resources, scheduling work on the cluster to\n        smooth out the load, or by doing <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_performance.xml\" data-doc-anchor-id=\"performance\">Impala performance\n        tuning</a> to enable higher throughput.\n      </p></div></div><div id=\"admission_jdbc_odbc\"><div class=\"hue-doc-title\">How Admission Control works with Impala Clients (JDBC, ODBC, HiveServer2)</div><div><p>\n        Most aspects of admission control work transparently with client interfaces such as JDBC and ODBC:\n      </p><ul><li>\n          If a SQL statement is put into a queue rather than running immediately, the API call blocks until the\n          statement is dequeued and begins execution. At that point, the client program can request to fetch\n          results, which might also block until results become available.\n        </li><li>\n          If a SQL statement is cancelled because it has been queued for too long or because it exceeded the memory\n          limit during execution, the error is returned to the client program with a descriptive error message.\n        </li></ul><p> In Impala 2.0 and higher, you can submit a SQL\n          <span class=\"hue-doc-codeph\">SET</span> statement from the client application to change\n        the <span class=\"hue-doc-codeph\">REQUEST_POOL</span> query option. This option lets you\n        submit queries to different resource pools, as described in <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_request_pool.xml\" data-doc-anchor-id=\"request_pool\">REQUEST_POOL Query Option</a>.  </p><p>\n        At any time, the set of queued queries could include queries submitted through multiple different Impala\n        daemon hosts. All the queries submitted through a particular host will be executed in order, so a\n        <span class=\"hue-doc-codeph\">CREATE TABLE</span> followed by an <span class=\"hue-doc-codeph\">INSERT</span> on the same table would succeed.\n        Queries submitted through different hosts are not guaranteed to be executed in the order they were\n        received. Therefore, if you are using load-balancing or other round-robin scheduling where different\n        statements are submitted through different hosts, set up all table structures ahead of time so that the\n        statements controlled by the queuing system are primarily queries, where order is not significant. Or, if a\n        sequence of statements needs to happen in strict order (such as an <span class=\"hue-doc-codeph\">INSERT</span> followed by a\n        <span class=\"hue-doc-codeph\">SELECT</span>), submit all those statements through a single session, while connected to the same\n        Impala daemon host.\n      </p><p>\n        Admission control has the following limitations or special behavior when used with JDBC or ODBC\n        applications:\n      </p><ul><li>\n          The other resource-related query options,\n          <span class=\"hue-doc-codeph\">RESERVATION_REQUEST_TIMEOUT</span> and <span class=\"hue-doc-codeph\">V_CPU_CORES</span>, are no longer used. Those query options only\n          applied to using Impala with Llama, which is no longer supported.\n        </li></ul></div></div><div id=\"admission_schema_config\"><div class=\"hue-doc-title\">SQL and Schema Considerations for Admission Control</div><div><p>\n        When queries complete quickly and are tuned for optimal memory usage, there is less chance of\n        performance or capacity problems during times of heavy load. Before setting up admission control,\n        tune your Impala queries to ensure that the query plans are efficient and the memory estimates\n        are accurate. Understanding the nature of your workload, and which queries are the most\n        resource-intensive, helps you to plan how to divide the queries into different pools and\n        decide what limits to define for each pool.\n      </p><p>\n        For large tables, especially those involved in join queries, keep their statistics up to date\n        after loading substantial amounts of new data or adding new partitions.\n        Use the <span class=\"hue-doc-codeph\">COMPUTE STATS</span> statement for unpartitioned tables, and\n        <span class=\"hue-doc-codeph\">COMPUTE INCREMENTAL STATS</span> for partitioned tables.\n      </p><p>\n        When you use dynamic resource pools with a <div class=\"hue-doc-uicontrol\">Max Memory</div> setting enabled,\n        you typically override the memory estimates that Impala makes based on the statistics from the\n        <span class=\"hue-doc-codeph\">COMPUTE STATS</span> statement.\n        You either set the <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option within a particular session to\n        set an upper memory limit for queries within that session, or a default <span class=\"hue-doc-codeph\">MEM_LIMIT</span>\n        setting for all queries processed by the <span class=\"hue-doc-cmdname\">impalad</span> instance, or\n        a default <span class=\"hue-doc-codeph\">MEM_LIMIT</span> setting for all queries assigned to a particular\n        dynamic resource pool. By designating a consistent memory limit for a set of similar queries\n        that use the same resource pool, you avoid unnecessary query queuing or out-of-memory conditions\n        that can arise during high-concurrency workloads when memory estimates for some queries are inaccurate.\n      </p><p>\n        Follow other steps from <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_performance.xml\" data-doc-anchor-id=\"performance\">Tuning Impala for Performance</a> to tune your queries.\n      </p></div></div><div id=\"admission_guidelines\"><div class=\"hue-doc-title\">Guidelines for Using Admission Control</div><div><p> The limits imposed by admission control are de-centrally managed\n          <q>soft</q> limits. Each Impala coordinator node makes its own\n        decisions about whether to allow queries to run immediately or to queue\n        them. These decisions rely on information passed back and forth between\n        nodes by the StateStore service. If a sudden surge in requests causes\n        more queries than anticipated to run concurrently, then the throughput\n        could decrease due to queries spilling to disk or contending for\n        resources. Or queries could be cancelled if they exceed the\n          <span class=\"hue-doc-codeph\">MEM_LIMIT</span> setting while running. </p><p> In <span class=\"hue-doc-cmdname\">impala-shell</span>, you can also specify which\n        resource pool to direct queries to by setting the\n          <span class=\"hue-doc-codeph\">REQUEST_POOL</span> query option. </p><p> To see how admission control works for particular queries, examine the\n        profile output or the summary output for the query. <ul><li>Profile<p>The information is available through the\n                <span class=\"hue-doc-codeph\">PROFILE</span> statement in\n                <span class=\"hue-doc-cmdname\">impala-shell</span> immediately after running a\n              query in the shell, on the <div class=\"hue-doc-uicontrol\">queries</div> page of\n              the Impala debug web UI, or in the Impala log file (basic\n              information at log level 1, more detailed information at log level\n              2). </p><p>The profile output contains details about the admission\n              decision, such as whether the query was queued or not and which\n              resource pool it was assigned to. It also includes the estimated\n              and actual memory usage for the query, so you can fine-tune the\n              configuration for the memory limits of the resource pools.\n            </p></li><li>Summary<p>Starting in Impala 3.1, the\n              information is available in <span class=\"hue-doc-cmdname\">impala-shell</span> when\n              the <span class=\"hue-doc-codeph\">LIVE_PROGRESS</span> or\n                <span class=\"hue-doc-codeph\">LIVE_SUMMARY</span> query option is set to\n                <span class=\"hue-doc-codeph\">TRUE</span>.</p><p>You can also start an\n                <span class=\"hue-doc-codeph\">impala-shell</span> session with the\n                <span class=\"hue-doc-codeph\">--live_progress</span> or\n                <span class=\"hue-doc-codeph\">--live_summary</span> flags to monitor all queries in\n              that <span class=\"hue-doc-codeph\">impala-shell</span> session.</p><p>The summary\n              output includes the queuing status consisting of whether the query\n              was queued and what was the latest queuing reason.</p></li></ul></p><p> For details about all the Fair Scheduler configuration settings, see\n          <a class=\"hue-doc-external-link\" href=\"http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/FairScheduler.html#Configuration\" target=\"_blank\">Fair Scheduler Configuration</a>, in\n        particular the tags such as <span class=\"hue-doc-codeph\">&lt;queue&gt;</span> and\n          <span class=\"hue-doc-codeph\">&lt;aclSubmitApps&gt;</span> to map users and groups to\n        particular resource pools (queues). </p></div></div></div></div>","title":"Admission Control and Query Queuing"}