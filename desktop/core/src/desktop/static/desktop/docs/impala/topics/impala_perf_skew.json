{"body":"<div><div id=\"perf_skew\"><div class=\"hue-doc-title\">Detecting and Correcting HDFS Block Skew Conditions</div><div><p>\n      For best performance of Impala parallel queries, the work is divided equally across hosts in the cluster, and\n      all hosts take approximately equal time to finish their work. If one host takes substantially longer than\n      others, the extra time needed for the slow host can become the dominant factor in query performance.\n      Therefore, one of the first steps in performance tuning for Impala is to detect and correct such conditions.\n    </p><p>\n      The main cause of uneven performance that you can correct within Impala is <span class=\"hue-doc-term\">skew</span> in the number of\n      HDFS data blocks processed by each host, where some hosts process substantially more data blocks than others.\n      This condition can occur because of uneven distribution of the data values themselves, for example causing\n      certain data files or partitions to be large while others are very small. (Although it is possible to have\n      unevenly distributed data without any problems with the distribution of HDFS blocks.) Block skew could also\n      be due to the underlying block allocation policies within HDFS, the replication factor of the data files, and\n      the way that Impala chooses the host to process each data block.\n    </p><p>\n      The most convenient way to detect block skew, or slow-host issues in general, is to examine the <q>executive\n      summary</q> information from the query profile after running a query:\n    </p><ul><li><p>\n          In <span class=\"hue-doc-cmdname\">impala-shell</span>, issue the <span class=\"hue-doc-codeph\">SUMMARY</span> command immediately after the\n          query is complete, to see just the summary information. If you detect issues involving skew, you might\n          switch to issuing the <span class=\"hue-doc-codeph\">PROFILE</span> command, which displays the summary information followed\n          by a detailed performance analysis.\n        </p></li><li><p>\n          In the Impala debug web UI, click on the <div class=\"hue-doc-uicontrol\">Profile</div> link associated with the query after it is\n          complete. The executive summary information is displayed early in the profile output.\n        </p></li></ul><p>\n      For each phase of the query, you see an <div class=\"hue-doc-uicontrol\">Avg Time</div> and a <div class=\"hue-doc-uicontrol\">Max Time</div>\n      value, along with <div class=\"hue-doc-uicontrol\">#Hosts</div> indicating how many hosts are involved in that query phase.\n      For all the phases with <div class=\"hue-doc-uicontrol\">#Hosts</div> greater than one, look for cases where the maximum time\n      is substantially greater than the average time. Focus on the phases that took the longest, for example, those\n      taking multiple seconds rather than milliseconds or microseconds.\n    </p><p>\n      If you detect that some hosts take longer than others, first rule out non-Impala causes. One reason that some\n      hosts could be slower than others is if those hosts have less capacity than the others, or if they are\n      substantially busier due to unevenly distributed non-Impala workloads:\n    </p><ul><li><p>\n          For clusters running Impala, keep the relative capacities of all hosts roughly equal. Any cost savings\n          from including some underpowered hosts in the cluster will likely be outweighed by poor or uneven\n          performance, and the time spent diagnosing performance issues.\n        </p></li><li><p>\n          If non-Impala workloads cause slowdowns on some hosts but not others, use the appropriate load-balancing\n          techniques for the non-Impala components to smooth out the load across the cluster.\n        </p></li></ul><p>\n      If the hosts on your cluster are evenly powered and evenly loaded, examine the detailed profile output to\n      determine which host is taking longer than others for the query phase in question. Examine how many bytes are\n      processed during that phase on that host, how much memory is used, and how many bytes are transmitted across\n      the network.\n    </p><p>\n      The most common symptom is a higher number of bytes read on one host than others, due to one host being\n      requested to process a higher number of HDFS data blocks. This condition is more likely to occur when the\n      number of blocks accessed by the query is relatively small. For example, if you have a 10-node cluster and\n      the query processes 10 HDFS blocks, each node might not process exactly one block. If one node sits idle\n      while another node processes two blocks, the query could take twice as long as if the data was perfectly\n      distributed.\n    </p><p>\n      Possible solutions in this case include:\n    </p><ul><li><p>\n          If the query is artificially small, perhaps for benchmarking purposes, scale it up to process a larger\n          data set. For example, if some nodes read 10 HDFS data blocks while others read 11, the overall effect of\n          the uneven distribution is much lower than when some nodes did twice as much work as others. As a\n          guideline, aim for a <q>sweet spot</q> where each node reads 2 GB or more from HDFS per query. Queries\n          that process lower volumes than that could experience inconsistent performance that smooths out as\n          queries become more data-intensive.\n        </p></li><li><p>\n          If the query processes only a few large blocks, so that many nodes sit idle and cannot help to\n          parallelize the query, consider reducing the overall block size. For example, you might adjust the\n          <span class=\"hue-doc-codeph\">PARQUET_FILE_SIZE</span> query option before copying or converting data into a Parquet table.\n          Or you might adjust the granularity of data files produced earlier in the ETL pipeline by non-Impala\n          components. In Impala 2.0 and later, the default Parquet block size is 256 MB, reduced from 1 GB, to\n          improve parallelism for common cluster sizes and data volumes.\n        </p></li><li><p>\n          Reduce the amount of compression applied to the data. For text data files, the highest degree of\n          compression (gzip) produces unsplittable files that are more difficult for Impala to process in parallel,\n          and require extra memory during processing to hold the compressed and uncompressed data simultaneously.\n          For binary formats such as Parquet and Avro, compression can result in fewer data blocks overall, but\n          remember that when queries process relatively few blocks, there is less opportunity for parallel\n          execution and many nodes in the cluster might sit idle. Note that when Impala writes Parquet data with\n          the query option <span class=\"hue-doc-codeph\">COMPRESSION_CODEC=NONE</span> enabled, the data is still typically compact due\n          to the encoding schemes used by Parquet, independent of the final compression step.\n        </p></li></ul></div></div></div>","title":"Detecting and Correcting HDFS Block Skew Conditions"}