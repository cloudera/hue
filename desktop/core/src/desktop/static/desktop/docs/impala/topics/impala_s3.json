{"body":"<div><div id=\"s3\"><div class=\"hue-doc-title\">Using Impala with Amazon S3 Object Store</div><div><p> You can use Impala to query data residing on the Amazon S3\n      object store. This capability allows convenient access to a storage system\n      that is remotely managed, accessible from anywhere, and integrated with\n      various cloud-based services. Impala can query files in any supported file\n      format from S3. The S3 storage location can be for an entire table, or\n      individual partitions in a partitioned table. </p><p/></div><div id=\"s3_best_practices\"><div class=\"hue-doc-title\">Best Practices for Using Impala with S3</div><div><p> The following guidelines summarize the best practices described in the\n        rest of this topic: </p><ul><li><p> Any reference to an S3 location must be fully qualified when S3 is\n            not designated as the default storage, for example,\n              <span class=\"hue-doc-codeph\">s3a:://[s3-bucket-name]</span>.</p></li><li><p> Set <span class=\"hue-doc-codeph\">fs.s3a.connection.maximum</span> to 1500 for\n              <span class=\"hue-doc-cmdname\">impalad</span>. </p></li><li><p> Set <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 134217728 (128 MB in\n            bytes) if most Parquet files queried by Impala were written by Hive\n            or ParquetMR jobs. </p><p>Set the block size to 268435456 (256 MB in bytes) if most Parquet\n            files queried by Impala were written by Impala. </p><p>Starting in Impala 3.4.0, instead of\n              <span class=\"hue-doc-codeph\">fs.s3a.block.size</span>, the\n              <span class=\"hue-doc-codeph\">PARQUET_OBJECT_STORE_SPLIT_SIZE</span> query option\n            controls the Parquet-specific split size. The default value is 256\n            MB.</p></li><li><p><span class=\"hue-doc-codeph\">DROP TABLE .. PURGE</span> is much faster than the default\n              <span class=\"hue-doc-codeph\">DROP TABLE</span>. The same applies to <span class=\"hue-doc-codeph\">ALTER\n              TABLE ... DROP PARTITION PURGE</span> versus the default\n              <span class=\"hue-doc-codeph\">DROP PARTITION</span> operation. Due to the eventually\n            consistent nature of S3, the files for that table or partition could\n            remain for some unbounded time when using <span class=\"hue-doc-codeph\">PURGE</span>.\n            The default <span class=\"hue-doc-codeph\">DROP TABLE/PARTITION</span> is slow because\n            Impala copies the files to the S3A trash folder, and Impala waits\n            until all the data is moved. <span class=\"hue-doc-codeph\">DROP TABLE/PARTITION ..\n              PURGE</span> is a fast delete operation, and the Impala\n            statement finishes quickly even though the change might not have\n            propagated fully throughout S3. </p></li><li><p><span class=\"hue-doc-codeph\">INSERT</span> statements are faster than <span class=\"hue-doc-codeph\">INSERT\n              OVERWRITE</span> for S3. The query option\n              <span class=\"hue-doc-codeph\">S3_SKIP_INSERT_STAGING</span>, which is set to\n              <span class=\"hue-doc-codeph\">true</span> by default, skips the staging step for\n            regular <span class=\"hue-doc-codeph\">INSERT</span> (but not <span class=\"hue-doc-codeph\">INSERT\n              OVERWRITE</span>). This makes the operation much faster, but\n            consistency is not guaranteed: if a node fails during execution, the\n            table could end up with inconsistent data. Set this option to\n              <span class=\"hue-doc-codeph\">false</span> if stronger consistency is required,\n            however, this setting will make the <span class=\"hue-doc-codeph\">INSERT</span>\n            operations slower. </p><ul><li><p> For Impala-ACID tables, both <span class=\"hue-doc-codeph\">INSERT</span> and\n                  <span class=\"hue-doc-codeph\">INSERT OVERWRITE</span> tables for S3 are fast,\n                regardless of the setting of\n                  <span class=\"hue-doc-codeph\">S3_SKIP_INSERT_STAGING</span>. Plus, consistency is\n                guaranteed with ACID tables.</p></li></ul></li><li>Enable <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_data_cache.xml\" data-doc-anchor-id=\"data_cache\">data cache for\n            remote reads</a>.</li><li>Enable <a class=\"hue-doc-external-link\" href=\"https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/s3guard.html\" target=\"_blank\">S3Guard</a> in your cluster for\n          data consistency.</li><li><p> Too many files in a table can make metadata load and update slow\n            in S3. If too many requests are made to S3, S3 has a back-off\n            mechanism and responds slower than usual.</p><ul><li>If you have many small files due to over-granular partitioning,\n              configure partitions with many megabytes of data so that even a\n              query against a single partition can be parallelized effectively. </li><li>If you have many small files because of many small\n                <span class=\"hue-doc-codeph\">INSERT</span> queries, use bulk\n                <span class=\"hue-doc-codeph\">INSERT</span>s so that more data is written to fewer\n              files. </li></ul></li></ul></div></div><div id=\"s3_sql\"><div class=\"hue-doc-title\">How Impala SQL Statements Work with S3</div><div><p> Impala SQL statements work with data in S3 as follows: </p><ul><li><p> The <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_create_table.xml\" data-doc-anchor-id=\"create_table\">CREATE\n              TABLE</a> or <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_alter_table.xml\" data-doc-anchor-id=\"alter_table\">ALTER TABLE</a> statement can specify that a table resides in\n            the S3 object store by encoding an <span class=\"hue-doc-codeph\">s3a://</span> prefix\n            for the <span class=\"hue-doc-codeph\">LOCATION</span> property. <span class=\"hue-doc-codeph\">ALTER\n              TABLE</span> can also set the <span class=\"hue-doc-codeph\">LOCATION</span> property\n            for an individual partition so that some data in a table resides in\n            S3 and other data in the same table resides on HDFS. </p></li><li><p> Once a table or partition is designated as residing in S3, the\n              <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_select.xml\" data-doc-anchor-id=\"select\">SELECT Statement</a> statement transparently\n            accesses the data files from the appropriate storage layer. </p></li><li><p>\n            If the S3 table is an internal table, the <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_drop_table.xml\" data-doc-anchor-id=\"drop_table\">DROP TABLE</a> statement\n            removes the corresponding data files from S3 when the table is dropped.\n          </p></li><li><p> The <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_truncate_table.xml\" data-doc-anchor-id=\"truncate_table\">TRUNCATE\n              TABLE</a> statement always removes the corresponding\n            data files from S3 when the table is truncated. </p></li><li><p>\n            The <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_load_data.xml\" data-doc-anchor-id=\"load_data\">LOAD DATA</a>\n            statement can move data files residing in HDFS into\n            an S3 table.\n          </p></li><li><p>\n            The <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_insert.xml\" data-doc-anchor-id=\"insert\">INSERT</a> statement, or the <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span>\n            form of the <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement, can copy data from an HDFS table or another S3\n            table into an S3 table. The <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_s3_skip_insert_staging.xml\" data-doc-anchor-id=\"s3_skip_insert_staging\">S3_SKIP_INSERT_STAGING</a>\n            query option chooses whether or not to use a fast code path for these write operations to S3,\n            with the tradeoff of potential inconsistency in the case of a failure during the statement.\n          </p></li></ul><p>\n        For usage information about Impala SQL statements with S3 tables, see <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_s3.xml\" data-doc-anchor-id=\"s3_ddl\">Creating Impala Databases, Tables, and Partitions for Data Stored in\n      S3</a>\n        and <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_s3.xml\" data-doc-anchor-id=\"s3_dml\">Using Impala DML Statements for S3 Data</a>.\n      </p></div></div><div id=\"s3_creds\"><div class=\"hue-doc-title\">Specifying Impala Credentials to Access Data in S3</div><div><p> To allow Impala to access data in S3, specify values for the following\n        configuration settings in your <span class=\"hue-doc-filepath\">core-site.xml</span> file: </p><div class=\"hue-doc-codeblock\">&lt;property&gt;\n&lt;name&gt;fs.s3a.access.key&lt;/name&gt;\n&lt;value&gt;<span class=\"hue-doc-varname\">your_access_key</span>&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n&lt;name&gt;fs.s3a.secret.key&lt;/name&gt;\n&lt;value&gt;<span class=\"hue-doc-varname\">your_secret_key</span>&lt;/value&gt;\n&lt;/property&gt;\n</div><p> After specifying the credentials, restart both the Impala and Hive\n        services. Restarting Hive is required because Impala statements, such as\n          <span class=\"hue-doc-codeph\">CREATE TABLE</span>, go through the Hive Metastore. </p><div class=\"hue-doc-note\"><p>\n            Although you can specify the access key ID and secret key as part of the <span class=\"hue-doc-codeph\">s3a://</span> URL in the\n            <span class=\"hue-doc-codeph\">LOCATION</span> attribute, doing so makes this sensitive information visible in many places, such\n            as <span class=\"hue-doc-codeph\">DESCRIBE FORMATTED</span> output and Impala log files. Therefore, specify this information\n            centrally in the <span class=\"hue-doc-filepath\">core-site.xml</span> file, and restrict read access to that file to only\n            trusted users.\n          </p></div><p>See <a class=\"hue-doc-external-link\" href=\"https://www.google.com/url?q=https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html%23Authenticating_with_S3&amp;sa=D&amp;ust=1572980027740000&amp;usg=AFQjCNFnzPSfNBMVRgJZRenvhLblezHbdw\" target=\"_blank\">Authenticating with S3</a> for\n        additional authentication mechanisms to access S3.</p></div></div><div id=\"s3_etl\"><div class=\"hue-doc-title\">Loading Data into S3 for Impala Queries</div><div><p>\n        If your ETL pipeline involves moving data into S3 and then querying through Impala,\n        you can either use Impala DML statements to create, move, or copy the data, or\n        use the same data loading techniques as you would for non-Impala data.\n      </p></div><div id=\"s3_dml\"><div class=\"hue-doc-title\">Using Impala DML Statements for S3 Data</div><div><p>The Impala DML statements (<span class=\"hue-doc-codeph\">INSERT</span>, <span class=\"hue-doc-codeph\">LOAD\n            DATA</span>, and <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span>) can\n          write data into a table or partition that resides in S3. The syntax of\n          the DML statements is the same as for any other tables because the S3\n          location for tables and partitions is specified by an\n            <span class=\"hue-doc-codeph\">s3a://</span> prefix in the <span class=\"hue-doc-codeph\">LOCATION</span>\n          attribute of <span class=\"hue-doc-codeph\">CREATE TABLE</span> or <span class=\"hue-doc-codeph\">ALTER\n            TABLE</span> statements. If you bring data into S3 using the\n          normal S3 transfer mechanisms instead of Impala DML statements, issue\n          a <span class=\"hue-doc-codeph\">REFRESH</span> statement for the table before using Impala\n          to query the S3 data.</p><p id=\"s3_dml_performance\"> Because of differences\n        between S3 and traditional filesystems, DML operations for S3 tables can\n        take longer than for tables on HDFS. For example, both the <span class=\"hue-doc-codeph\">LOAD\n          DATA</span> statement and the final stage of the\n          <span class=\"hue-doc-codeph\">INSERT</span> and <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span>\n        statements involve moving files from one directory to another. (In the\n        case of <span class=\"hue-doc-codeph\">INSERT</span> and <span class=\"hue-doc-codeph\">CREATE TABLE AS\n          SELECT</span>, the files are moved from a temporary staging\n        directory to the final destination directory.) Because S3 does not\n        support a <q>rename</q> operation for existing objects, in these cases\n        Impala actually copies the data files from one location to another and\n        then removes the original files. In Impala 2.6,\n        the <span class=\"hue-doc-codeph\">S3_SKIP_INSERT_STAGING</span> query option provides a way\n        to speed up <span class=\"hue-doc-codeph\">INSERT</span> statements for S3 tables and\n        partitions, with the tradeoff that a problem during statement execution\n        could leave data in an inconsistent state. It does not apply to\n          <span class=\"hue-doc-codeph\">INSERT OVERWRITE</span> or <span class=\"hue-doc-codeph\">LOAD DATA</span>\n        statements. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_s3_skip_insert_staging.xml\" data-doc-anchor-id=\"s3_skip_insert_staging\">S3_SKIP_INSERT_STAGING Query Option</a> for details. </p></div></div><div id=\"s3_manual_etl\"><div class=\"hue-doc-title\">Manually Loading Data into Impala Tables in S3</div><div><p>\n          As an alternative, or on earlier Impala releases without DML support for S3,\n          you can use the Amazon-provided methods to bring data files into S3 for querying through Impala. See\n          <a class=\"hue-doc-external-link\" href=\"http://aws.amazon.com/s3/\" target=\"_blank\">the Amazon S3 web site</a> for\n          details.\n        </p><div class=\"hue-doc-note\"><p id=\"s3_drop_table_purge\"> For best\n        compatibility with the S3 write support in Impala 2.6 and higher: <ul><li> Use native Hadoop techniques to create data files in S3 for\n            querying through Impala. </li><li> Use the <span class=\"hue-doc-codeph\">PURGE</span> clause of <span class=\"hue-doc-codeph\">DROP\n              TABLE</span> when dropping internal (managed) tables. </li></ul> By default, when you drop an internal (managed) table, the data\n        files are moved to the HDFS trashcan. This operation is expensive for\n        tables that reside on the Amazon S3 object store. Therefore, for S3\n        tables, prefer to use <span class=\"hue-doc-codeph\">DROP TABLE <span class=\"hue-doc-varname\">table_name</span>\n          PURGE</span> rather than the default <span class=\"hue-doc-codeph\">DROP TABLE</span>\n        statement. The <span class=\"hue-doc-codeph\">PURGE</span> clause makes Impala delete the\n        data files immediately, skipping the HDFS trashcan. For the\n          <span class=\"hue-doc-codeph\">PURGE</span> clause to work effectively, you must originally\n        create the data files on S3 using one of the tools from the Hadoop\n        ecosystem, such as <span class=\"hue-doc-codeph\">hadoop fs -cp</span>, or\n          <span class=\"hue-doc-codeph\">INSERT</span> in Impala or Hive. </p></div><p> After you upload data files to a location already mapped to an\n          Impala table or partition, or if you delete files in S3 from such a\n          location, issue the <span class=\"hue-doc-codeph\">REFRESH</span> statement to make Impala\n          aware of the new set of data files. </p></div></div></div><div id=\"s3_ddl\"><div class=\"hue-doc-title\">Creating Impala Databases, Tables, and Partitions for Data Stored in\n      S3</div><div><p>To create a table that resides in S3, run the <span class=\"hue-doc-codeph\">CREATE\n          TABLE</span> or <span class=\"hue-doc-codeph\">ALTER TABLE</span> statement with the\n          <span class=\"hue-doc-codeph\">LOCATION</span> clause. </p><p><span class=\"hue-doc-codeph\">ALTER TABLE</span> can set the <span class=\"hue-doc-codeph\">LOCATION</span>\n        property for an individual partition, so that some data in a table\n        resides in S3 and other data in the same table resides on HDFS.</p><p>The syntax for the <span class=\"hue-doc-codeph\">LOCATION</span> clause is:</p><div class=\"hue-doc-codeblock\">LOCATION 's3a://<span class=\"hue-doc-varname\">bucket_name</span>/<span class=\"hue-doc-varname\">path</span>/<span class=\"hue-doc-varname\">to</span>/<span class=\"hue-doc-varname\">file</span>'</div><p>The file system prefix is always <span class=\"hue-doc-codeph\">s3a://</span>. Impala does\n        not support the <span class=\"hue-doc-codeph\">s3://</span> or <span class=\"hue-doc-codeph\">s3n://</span>\n        prefixes. </p><p> For a partitioned table, either specify a separate\n          <span class=\"hue-doc-codeph\">LOCATION</span> clause for each new partition, or specify a\n        base <span class=\"hue-doc-codeph\">LOCATION</span> for the table and set up a directory\n        structure in S3 to mirror the way Impala partitioned tables are\n        structured in S3. </p><p> You point a nonpartitioned table or an individual partition at S3 by\n        specifying a single directory path in S3, which could be any arbitrary\n        directory. To replicate the structure of an entire Impala partitioned\n        table or database in S3 requires more care, with directories and\n        subdirectories nested and named to match the equivalent directory tree\n        in HDFS. Consider setting up an empty staging area if necessary in HDFS,\n        and recording the complete directory structure so that you can replicate\n        it in S3.  </p><p> When working with multiple tables with data files stored in S3, you can\n        create a database with a <span class=\"hue-doc-codeph\">LOCATION</span> attribute pointing to\n        an S3 path. Specify a URL of the form\n            <span class=\"hue-doc-codeph\">s3a://<span class=\"hue-doc-varname\">bucket</span>/<span class=\"hue-doc-varname\">root</span>/<span class=\"hue-doc-varname\">path</span>/<span class=\"hue-doc-varname\">for</span>/<span class=\"hue-doc-varname\">database</span></span>\n        for the <span class=\"hue-doc-codeph\">LOCATION</span> attribute of the database. Any tables\n        created inside that database automatically create directories underneath\n        the one specified by the database <span class=\"hue-doc-codeph\">LOCATION</span> attribute. </p><p>The following example creates a table with one partition for the year\n        2017 resides on HDFS and one partition for the year 2018 resides in\n        S3.</p><p>The partition for year 2018 includes a <span class=\"hue-doc-codeph\">LOCATION</span>\n        attribute with an <span class=\"hue-doc-codeph\">s3a://</span> URL, and so refers to data\n        residing in S3, under a specific path underneath the bucket\n          <span class=\"hue-doc-codeph\">impala-demo</span>. </p><div class=\"hue-doc-codeblock\">CREATE TABLE mostly_on_hdfs (x int) PARTITIONED BY (year INT);\nALTER TABLE mostly_on_hdfs ADD PARTITION (year=2017);\nALTER TABLE mostly_on_hdfs ADD PARTITION (year=2018) \n   LOCATION 's3a://impala-demo/dir1/dir2/dir3/t1';\n</div><p> The following session creates a database and two partitioned tables\n        residing entirely in S3, one partitioned by a single column and the\n        other partitioned by multiple columns. </p><ul><li>Because a <span class=\"hue-doc-codeph\">LOCATION</span> attribute with an\n            <span class=\"hue-doc-codeph\">s3a://</span> URL is specified for the database, the\n          tables inside that database are automatically created in S3 underneath\n          the database directory. </li><li>To see the names of the associated subdirectories, including the\n          partition key values, use an S3 client tool to examine how the\n          directory structure is organized in S3. </li></ul><div class=\"hue-doc-codeblock\">CREATE DATABASE db_on_s3 LOCATION 's3a://impala-demo/dir1/dir2/dir3';\nCREATE TABLE partitioned_multiple_keys (x INT)\n   PARTITIONED BY (year SMALLINT, month TINYINT, day TINYINT);\n\nALTER TABLE partitioned_multiple_keys\n   ADD PARTITION (year=2015,month=1,day=1);\nALTER TABLE partitioned_multiple_keys\n   ADD PARTITION (year=2015,month=1,day=31);\n\n!hdfs dfs -ls -R s3a://impala-demo/dir1/dir2/dir3\n2015-03-17 13:56:34          0 dir1/dir2/dir3/\n2015-03-17 16:47:13          0 dir1/dir2/dir3/partitioned_multiple_keys/\n2015-03-17 16:47:44          0 dir1/dir2/dir3/partitioned_multiple_keys/year=2015/month=1/day=1/\n2015-03-17 16:47:50          0 dir1/dir2/dir3/partitioned_multiple_keys/year=2015/month=1/day=31/</div><p>\n        The <span class=\"hue-doc-codeph\">CREATE DATABASE</span> and <span class=\"hue-doc-codeph\">CREATE TABLE</span> statements create the associated\n        directory paths if they do not already exist. You can specify multiple levels of directories, and the\n        <span class=\"hue-doc-codeph\">CREATE</span> statement creates all appropriate levels, similar to using <span class=\"hue-doc-codeph\">mkdir\n        -p</span>.\n      </p><p> Use the standard S3 file upload methods to put the actual data files\n        into the right locations. You can also put the directory paths and data\n        files in place before creating the associated Impala databases or\n        tables, and Impala automatically uses the data from the appropriate\n        location after the associated databases and tables are created. </p><p>Use the <span class=\"hue-doc-codeph\">ALTER TABLE</span> statement with the\n          <span class=\"hue-doc-codeph\">LOCATION</span> clause to switch whether an existing table\n        or partition points to data in HDFS or S3. For example, if you have an\n        Impala table or partition pointing to data files in HDFS or S3, and you\n        later transfer those data files to the other filesystem, use the\n          <span class=\"hue-doc-codeph\">ALTER TABLE</span> statement to adjust the\n          <span class=\"hue-doc-codeph\">LOCATION</span> attribute of the corresponding table or\n        partition to reflect that change. </p></div></div><div id=\"s3_internal_external\"><div class=\"hue-doc-title\">Internal and External Tables Located in S3</div><div><p> Just as with tables located on HDFS storage, you can designate\n        S3-based tables as either internal (managed by Impala) or external, by\n        using the syntax <span class=\"hue-doc-codeph\">CREATE TABLE</span> or <span class=\"hue-doc-codeph\">CREATE\n          EXTERNAL TABLE</span> respectively. </p><p>When you drop an internal table, the files associated with the table\n        are removed, even if they are in S3 storage. When you drop an external\n        table, the files associated with the table are left alone, and are still\n        available for access by other tools or components.</p><p> If the data in S3 is intended to be long-lived and accessed by other\n        tools in addition to Impala, create any associated S3 tables with the\n          <span class=\"hue-doc-codeph\">CREATE EXTERNAL TABLE</span> syntax, so that the files are\n        not deleted from S3 when the table is dropped. </p><p> If the data in S3 is only needed for querying by Impala and can be\n        safely discarded once the Impala workflow is complete, create the\n        associated S3 tables using the <span class=\"hue-doc-codeph\">CREATE TABLE</span> syntax, so\n        that dropping the table also deletes the corresponding data files in S3. </p></div></div><div id=\"s3_queries\"><div class=\"hue-doc-title\">Running and Tuning Impala Queries for Data Stored in S3</div><div><p> Once a table or partition is designated as residing in S3, the\n          <span class=\"hue-doc-codeph\">SELECT</span> statement transparently accesses the data\n        files from the appropriate storage layer. </p><ul><li>\n          Queries against S3 data support all the same file formats as for HDFS data.\n        </li><li>\n          Tables can be unpartitioned or partitioned. For partitioned tables, either manually construct paths in S3\n          corresponding to the HDFS directories representing partition key values, or use <span class=\"hue-doc-codeph\">ALTER TABLE ...\n          ADD PARTITION</span> to set up the appropriate paths in S3.\n        </li><li>\n          HDFS and HBase tables can be joined to S3 tables, or S3 tables can be joined with each other.\n        </li><li> Authorization to control access to databases, tables, or columns\n          works the same whether the data is in HDFS or in S3. </li><li> The Catalog Server (<span class=\"hue-doc-cmdname\">catalogd</span>) daemon caches\n          metadata for both HDFS and S3 tables.</li><li>\n          Queries against S3 tables are subject to the same kinds of admission control and resource management as\n          HDFS tables.\n        </li><li> Metadata about S3 tables is stored in the same Metastore database\n          as for HDFS tables. </li><li>\n          You can set up views referring to S3 tables, the same as for HDFS tables.\n        </li><li> The <span class=\"hue-doc-codeph\">COMPUTE STATS</span>, <span class=\"hue-doc-codeph\">SHOW TABLE\n            STATS</span>, and <span class=\"hue-doc-codeph\">SHOW COLUMN STATS</span> statements\n          work for S3 tables. </li></ul></div><div id=\"s3_performance\"><div class=\"hue-doc-title\">Understanding and Tuning Impala Query Performance for S3 Data</div><div><p>Here are techniques you can use to interpret explain plans and\n          profiles for queries against S3 data, and tips to achieve the best\n          performance possible for such queries. </p><p> All else being equal, performance is expected to be lower for\n          queries running against data in S3 rather than HDFS. The actual\n          mechanics of the <span class=\"hue-doc-codeph\">SELECT</span> statement are somewhat\n          different when the data is in S3. Although the work is still\n          distributed across the DataNodes of the cluster, Impala might\n          parallelize the work for a distributed query differently for data on\n          HDFS and S3.</p><p>S3 does not have the same block notion as HDFS, so Impala uses\n          heuristics to determine how to split up large S3 files for processing\n          in parallel. Because all hosts can access any S3 data file with equal\n          efficiency, the distribution of work might be different than for HDFS\n          data, where the data blocks are physically read using short-circuit\n          local reads by hosts that contain the appropriate block replicas.\n          Although the I/O to read the S3 data might be spread evenly across the\n          hosts of the cluster, the fact that all data is initially retrieved\n          across the network means that the overall query performance is likely\n          to be lower for S3 data than for HDFS data. </p><p>Use the <span class=\"hue-doc-codeph\">PARQUET_OBJECT_STORE_SPLIT_SIZE</span> query option\n          to control the Parquet-specific split size. The default value is 256\n          MB.</p><p> When optimizing aspects of complex queries, such as the join order,\n          Impala treats tables on HDFS and S3 the same way. Therefore, follow\n          all the same tuning recommendations for S3 tables as for HDFS ones,\n          such as using the <span class=\"hue-doc-codeph\">COMPUTE STATS</span> statement to help\n          Impala construct accurate estimates of row counts and cardinality. See\n            <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_performance.xml\" data-doc-anchor-id=\"performance\">Tuning Impala for Performance</a> for details. </p><p> In query profile reports, the numbers for\n            <span class=\"hue-doc-codeph\">BytesReadLocal</span>,\n            <span class=\"hue-doc-codeph\">BytesReadShortCircuit</span>,\n            <span class=\"hue-doc-codeph\">BytesReadDataNodeCached</span>, and\n            <span class=\"hue-doc-codeph\">BytesReadRemoteUnexpected</span> are blank because those\n          metrics come from HDFS. By definition, all the I/O for S3 tables\n          involves remote reads. </p></div></div></div><div id=\"s3_restrictions\"><div class=\"hue-doc-title\">Restrictions on Impala Support for S3</div><div><p>The following restrictions apply when using Impala with S3:</p><ul><li> Impala does not support the old <span class=\"hue-doc-codeph\">s3://</span> block-based\n          and <span class=\"hue-doc-codeph\">s3n://</span> filesystem schemes, and it only supports\n            <span class=\"hue-doc-codeph\">s3a://</span>. </li><li>Although S3 is often used to store JSON-formatted data, the current\n          Impala support for S3 does not include directly querying JSON data.\n          For Impala queries, use data files in one of the file formats listed\n          in <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_file_formats.xml\" data-doc-anchor-id=\"file_formats\">How Impala Works with Hadoop File Formats</a>. If you have\n          data in JSON format, you can prepare a flattened version of that data\n          for querying by Impala as part of your ETL cycle. </li><li>You cannot use the <span class=\"hue-doc-codeph\">ALTER TABLE ... SET CACHED</span>\n          statement for tables or partitions that are located in S3. </li></ul></div></div></div></div>","title":"Using Impala with Amazon S3 Object Store"}