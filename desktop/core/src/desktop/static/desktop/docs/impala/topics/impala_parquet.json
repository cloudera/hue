{"body":"<div><div id=\"parquet\"><div class=\"hue-doc-title\">Using the Parquet File Format with Impala Tables</div><div><p>\n      Impala allows you to create, manage, and query Parquet tables. Parquet is a\n      column-oriented binary file format intended to be highly efficient for the types of\n      large-scale queries that Impala is best at. Parquet is especially good for queries\n      scanning particular columns within a table, for example, to query <q>wide</q> tables with\n      many columns, or to perform aggregation operations such as <span class=\"hue-doc-codeph\">SUM()</span> and\n      <span class=\"hue-doc-codeph\">AVG()</span> that need to process most or all of the values from a column. Each\n      Parquet data file written by Impala contains the values for a set of rows (referred to as\n      the <q>row group</q>). Within a data file, the values from each column are organized so\n      that they are all adjacent, enabling good compression for the values from that column.\n      Queries against a Parquet table can retrieve and analyze these values from any column\n      quickly and with minimal I/O.\n    </p><p>\n      See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_file_formats.xml\" data-doc-anchor-id=\"file_formats\">How Impala Works with Hadoop File Formats</a> for the summary of Parquet format\n      support.\n    </p><p/></div><div id=\"parquet_ddl\"><div class=\"hue-doc-title\">Creating Parquet Tables in Impala</div><div><p>\n        To create a table named <span class=\"hue-doc-codeph\">PARQUET_TABLE</span> that uses the Parquet format, you\n        would use a command like the following, substituting your own table name, column names,\n        and data types:\n      </p><div class=\"hue-doc-codeblock\">[impala-host:21000] &gt; create table <span class=\"hue-doc-varname\">parquet_table_name</span> (x INT, y STRING) STORED AS PARQUET;</div><p>\n        Or, to clone the column names and data types of an existing table:\n      </p><div class=\"hue-doc-codeblock\">[impala-host:21000] &gt; create table <span class=\"hue-doc-varname\">parquet_table_name</span> LIKE <span class=\"hue-doc-varname\">other_table_name</span> STORED AS PARQUET;</div><p>\n        In Impala 1.4.0 and higher, you can derive column definitions from a raw Parquet data\n        file, even without an existing Impala table. For example, you can create an external\n        table pointing to an HDFS directory, and base the column definitions on one of the files\n        in that directory:\n      </p><div class=\"hue-doc-codeblock\">CREATE EXTERNAL TABLE ingest_existing_files LIKE PARQUET '/user/etl/destination/datafile1.dat'\n  STORED AS PARQUET\n  LOCATION '/user/etl/destination';\n</div><p>\n        Or, you can refer to an existing data file and create a new empty table with suitable\n        column definitions. Then you can use <span class=\"hue-doc-codeph\">INSERT</span> to create new data files or\n        <span class=\"hue-doc-codeph\">LOAD DATA</span> to transfer existing data files into the new table.\n      </p><div class=\"hue-doc-codeblock\">CREATE TABLE columns_from_data_file LIKE PARQUET '/user/etl/destination/datafile1.dat'\n  STORED AS PARQUET;\n</div><p>\n        The default properties of the newly created table are the same as for any other\n        <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement. For example, the default file format is text;\n        if you want the new table to use the Parquet file format, include the <span class=\"hue-doc-codeph\">STORED AS\n        PARQUET</span> file also.\n      </p><p>\n        In this example, the new table is partitioned by year, month, and day. These partition\n        key columns are not part of the data file, so you specify them in the <span class=\"hue-doc-codeph\">CREATE\n        TABLE</span> statement:\n      </p><div class=\"hue-doc-codeblock\">CREATE TABLE columns_from_data_file LIKE PARQUET '/user/etl/destination/datafile1.dat'\n  PARTITION (year INT, month TINYINT, day TINYINT)\n  STORED AS PARQUET;\n</div><p>\n        See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_create_table.xml\" data-doc-anchor-id=\"create_table\">CREATE TABLE Statement</a> for more details about the\n        <span class=\"hue-doc-codeph\">CREATE TABLE LIKE PARQUET</span> syntax.\n      </p><p>\n        Once you have created a table, to insert data into that table, use a command similar to\n        the following, again with your own table names:\n      </p><div class=\"hue-doc-codeblock\">[impala-host:21000] &gt; insert overwrite table <span class=\"hue-doc-varname\">parquet_table_name</span> select * from <span class=\"hue-doc-varname\">other_table_name</span>;</div><p>\n        If the Parquet table has a different number of columns or different column names than\n        the other table, specify the names of columns from the other table rather than\n        <span class=\"hue-doc-codeph\">*</span> in the <span class=\"hue-doc-codeph\">SELECT</span> statement.\n      </p></div></div><div id=\"parquet_etl\"><div class=\"hue-doc-title\">Loading Data into Parquet Tables</div><div><p>\n        Choose from the following techniques for loading data into Parquet tables, depending on\n        whether the original data is already in an Impala table, or exists as raw data files\n        outside Impala.\n      </p><p>\n        If you already have data in an Impala or Hive table, perhaps in a different file format\n        or partitioning scheme, you can transfer the data to a Parquet table using the Impala\n        <span class=\"hue-doc-codeph\">INSERT...SELECT</span> syntax. You can convert, filter, repartition, and do\n        other things to the data as part of this same <span class=\"hue-doc-codeph\">INSERT</span> statement. See\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet_compression\">Compressions for Parquet Data Files</a> for some examples showing how to insert\n        data into Parquet tables.\n      </p><p>\n        When inserting into partitioned tables, especially using the Parquet file format, you\n        can include a hint in the <span class=\"hue-doc-codeph\">INSERT</span> statement to fine-tune the overall\n        performance of the operation and its resource usage. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_hints.xml\" data-doc-anchor-id=\"hints\">Optimizer Hints</a> for\n        using hints in the <span class=\"hue-doc-codeph\">INSERT</span> statements.\n      </p><p id=\"insert_parquet_blocksize\">\n        Any <span class=\"hue-doc-codeph\">INSERT</span> statement for a Parquet table requires enough free space in\n        the HDFS filesystem to write one block. Because Parquet data files use a block size of 1\n        GB by default, an <span class=\"hue-doc-codeph\">INSERT</span> might fail (even for a very small amount of\n        data) if your HDFS is running low on space.\n      </p><p>\n        Avoid the <span class=\"hue-doc-codeph\">INSERT...VALUES</span> syntax for Parquet tables, because\n        <span class=\"hue-doc-codeph\">INSERT...VALUES</span> produces a separate tiny data file for each\n        <span class=\"hue-doc-codeph\">INSERT...VALUES</span> statement, and the strength of Parquet is in its\n        handling of data (compressing, parallelizing, and so on) in\n        <span class=\"hue-doc-ph\">large</span> chunks.\n      </p><p>\n        If you have one or more Parquet data files produced outside of Impala, you can quickly\n        make the data queryable through Impala by one of the following methods:\n      </p><ul><li>\n          The <span class=\"hue-doc-codeph\">LOAD DATA</span> statement moves a single data file or a directory full\n          of data files into the data directory for an Impala table. It does no validation or\n          conversion of the data. The original data files must be somewhere in HDFS, not the\n          local filesystem.\n        </li><li>\n          The <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement with the <span class=\"hue-doc-codeph\">LOCATION</span> clause\n          creates a table where the data continues to reside outside the Impala data directory.\n          The original data files must be somewhere in HDFS, not the local filesystem. For extra\n          safety, if the data is intended to be long-lived and reused by other applications, you\n          can use the <span class=\"hue-doc-codeph\">CREATE EXTERNAL TABLE</span> syntax so that the data files are\n          not deleted by an Impala <span class=\"hue-doc-codeph\">DROP TABLE</span> statement.\n        </li><li>\n          If the Parquet table already exists, you can copy Parquet data files directly into it,\n          then use the <span class=\"hue-doc-codeph\">REFRESH</span> statement to make Impala recognize the newly\n          added data. Remember to preserve the block size of the Parquet data files by using the\n          <span class=\"hue-doc-codeph\">hadoop distcp -pb</span> command rather than a <span class=\"hue-doc-codeph\">-put</span> or\n          <span class=\"hue-doc-codeph\">-cp</span> operation on the Parquet files. See\n          <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet_compression_multiple\">Example of Copying Parquet Data Files</a> for an example of this kind of operation.\n        </li></ul><div class=\"hue-doc-note\" id=\"restrictions_nonimpala_parquet\"><p>\n          Currently, Impala always decodes the column data in Parquet files based on the ordinal\n          position of the columns, not by looking up the position of each column based on its\n          name. Parquet files produced outside of Impala must write column data in the same\n          order as the columns are declared in the Impala table. Any optional columns that are\n          omitted from the data files must be the rightmost columns in the Impala table\n          definition.\n        </p><p>\n          If you created compressed Parquet files through some tool other than Impala, make sure\n          that any compression codecs are supported in Parquet by Impala. For example, Impala\n          does not currently support LZO compression in Parquet files. Also doublecheck that you\n          used any recommended compatibility settings in the other tool, such as\n          <span class=\"hue-doc-codeph\">spark.sql.parquet.binaryAsString</span> when writing Parquet files through\n          Spark.\n        </p></div><p>\n        Recent versions of Sqoop can produce Parquet output files using the\n        <span class=\"hue-doc-codeph\">--as-parquetfile</span> option.\n      </p><p>\n        If the data exists outside Impala and is in some other format, combine both of the\n        preceding techniques. First, use a <span class=\"hue-doc-codeph\">LOAD DATA</span> or <span class=\"hue-doc-codeph\">CREATE EXTERNAL\n        TABLE ... LOCATION</span> statement to bring the data into an Impala table that uses\n        the appropriate file format. Then, use an <span class=\"hue-doc-codeph\">INSERT...SELECT</span> statement to\n        copy the data to the Parquet table, converting to Parquet format as part of the process.\n      </p><p>\n        Loading data into Parquet tables is a memory-intensive operation, because the incoming\n        data is buffered until it reaches <span class=\"hue-doc-ph\">one data\n        block</span> in size, then that chunk of data is organized and compressed in memory before\n        being written out. The memory consumption can be larger when inserting data into\n        partitioned Parquet tables, because a separate data file is written for each combination\n        of partition key column values, potentially requiring several\n        <span class=\"hue-doc-ph\">large</span> chunks to be manipulated in memory at once.\n      </p><p>\n        When inserting into a partitioned Parquet table, Impala redistributes the data among the\n        nodes to reduce memory consumption. You might still need to temporarily increase the\n        memory dedicated to Impala during the insert operation, or break up the load operation\n        into several <span class=\"hue-doc-codeph\">INSERT</span> statements, or both.\n      </p><div class=\"hue-doc-note\">        All the preceding techniques assume that the data you are loading matches the structure\n        of the destination table, including column order, column names, and partition layout. To\n        transform or reorganize the data, start by loading the data into a Parquet table that\n        matches the underlying structure of the data, then use one of the table-copying\n        techniques such as <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span> or <span class=\"hue-doc-codeph\">INSERT ...\n        SELECT</span> to reorder or rename columns, divide the data among multiple partitions,\n        and so on. For example to take a single comprehensive Parquet data file and load it into\n        a partitioned table, you would use an <span class=\"hue-doc-codeph\">INSERT ... SELECT</span> statement with\n        dynamic partitioning to let Impala create separate data files with the appropriate\n        partition values; for an example, see <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_insert.xml\" data-doc-anchor-id=\"insert\">INSERT Statement</a>.\n      </div></div></div><div id=\"parquet_performance\"><div class=\"hue-doc-title\">Query Performance for Impala Parquet Tables</div><div><p>\n        Query performance for Parquet tables depends on the number of columns needed to process\n        the <span class=\"hue-doc-codeph\">SELECT</span> list and <span class=\"hue-doc-codeph\">WHERE</span> clauses of the query, the\n        way data is divided into <span class=\"hue-doc-ph\">large data files with block size\n        equal to file size</span>, the reduction in I/O by reading the data for each column in\n        compressed format, which data files can be skipped (for partitioned tables), and the CPU\n        overhead of decompressing the data for each column.\n      </p><p>\n        For example, the following is an efficient query for a Parquet table:\n<div class=\"hue-doc-codeblock\">select avg(income) from census_data where state = 'CA';</div>\n        The query processes only 2 columns out of a large number of total columns. If the table\n        is partitioned by the <span class=\"hue-doc-codeph\">STATE</span> column, it is even more efficient because\n        the query only has to read and decode 1 column from each data file, and it can read only\n        the data files in the partition directory for the state <span class=\"hue-doc-codeph\">'CA'</span>, skipping\n        the data files for all the other states, which will be physically located in other\n        directories.\n      </p><p>\n        The following is a relatively inefficient query for a Parquet table:\n<div class=\"hue-doc-codeblock\">select * from census_data;</div>\n        Impala would have to read the entire contents of each\n        <span class=\"hue-doc-ph\">large</span> data file, and decompress the contents of each\n        column for each row group, negating the I/O optimizations of the column-oriented format.\n        This query might still be faster for a Parquet table than a table with some other file\n        format, but it does not take advantage of the unique strengths of Parquet data files.\n      </p><p>\n        Impala can optimize queries on Parquet tables, especially join queries, better when\n        statistics are available for all the tables. Issue the <span class=\"hue-doc-codeph\">COMPUTE STATS</span>\n        statement for each table after substantial amounts of data are loaded into or appended\n        to it. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_compute_stats.xml\" data-doc-anchor-id=\"compute_stats\">COMPUTE STATS Statement</a> for details.\n      </p><p>\n        The runtime filtering feature, available in Impala 2.5 and\n        higher, works best with Parquet tables. The per-row filtering aspect only applies to\n        Parquet tables. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_runtime_filtering.xml\" data-doc-anchor-id=\"runtime_filtering\">Runtime Filtering for Impala Queries (Impala 2.5 or higher only)</a> for\n        details.\n      </p><p id=\"s3_block_splitting\">\n        In Impala 2.6 and higher, Impala queries are optimized for files\n        stored in Amazon S3. For Impala tables that use the file formats Parquet, ORC, RCFile,\n        SequenceFile, Avro, and uncompressed text, the setting\n        <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> in the <span class=\"hue-doc-filepath\">core-site.xml</span>\n        configuration file determines how Impala divides the I/O work of reading the data files.\n        This configuration setting is specified in bytes. By default, this value is 33554432 (32\n        MB), meaning that Impala parallelizes S3 read operations on the files as if they were\n        made up of 32 MB blocks. For example, if your S3 queries primarily access Parquet files\n        written by MapReduce or Hive, increase <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 134217728\n        (128 MB) to match the row group size of those files. If most S3 queries involve Parquet\n        files written by Impala, increase <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 268435456 (256\n        MB) to match the row group size produced by Impala.\n      </p><p>Starting in Impala 3.4.0, use the query option\n          <span class=\"hue-doc-codeph\">PARQUET_OBJECT_STORE_SPLIT_SIZE</span> to control the\n        Parquet split size for non-block stores (e.g. S3, ADLS, etc.). The\n        default value is 256 MB.</p><p>\n        In Impala 2.9 and higher, Parquet files written by Impala include\n        embedded metadata specifying the minimum and maximum values for each column, within each\n        row group and each data page within the row group. Impala-written Parquet files\n        typically contain a single row group; a row group can contain many data pages. Impala\n        uses this information (currently, only the metadata for each row group) when reading\n        each Parquet data file during a query, to quickly determine whether each row group\n        within the file potentially includes any rows that match the conditions in the\n        <span class=\"hue-doc-codeph\">WHERE</span> clause. For example, if the column <span class=\"hue-doc-codeph\">X</span> within a\n        particular Parquet file has a minimum value of 1 and a maximum value of 100, then a\n        query including the clause <span class=\"hue-doc-codeph\">WHERE x &gt; 200</span> can quickly determine that\n        it is safe to skip that particular file, instead of scanning all the associated column\n        values. This optimization technique is especially effective for tables that use the\n        <span class=\"hue-doc-codeph\">SORT BY</span> clause for the columns most frequently checked in\n        <span class=\"hue-doc-codeph\">WHERE</span> clauses, because any <span class=\"hue-doc-codeph\">INSERT</span> operation on such\n        tables produces Parquet data files with relatively narrow ranges of column values within\n        each file.\n      </p><p>To disable Impala from writing the Parquet page index when creating\n        Parquet files, set the <span class=\"hue-doc-codeph\">PARQUET_WRITE_PAGE_INDEX</span> query\n        option to <span class=\"hue-doc-codeph\">FALSE</span>.</p></div><div id=\"parquet_partitioning\"><div class=\"hue-doc-title\">Partitioning for Parquet Tables</div><div><p>\n          As explained in <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_partitioning.xml\" data-doc-anchor-id=\"partitioning\">Partitioning for Impala Tables</a>, partitioning is\n          an important performance technique for Impala generally. This section explains some of\n          the performance considerations for partitioned Parquet tables.\n        </p><p>\n          The Parquet file format is ideal for tables containing many columns, where most\n          queries only refer to a small subset of the columns. As explained in\n          <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet_data_files\">How Parquet Data Files Are Organized</a>, the physical layout of Parquet data files lets\n          Impala read only a small fraction of the data for many queries. The performance\n          benefits of this approach are amplified when you use Parquet tables in combination\n          with partitioning. Impala can skip the data files for certain partitions entirely,\n          based on the comparisons in the <span class=\"hue-doc-codeph\">WHERE</span> clause that refer to the\n          partition key columns. For example, queries on partitioned tables often analyze data\n          for time intervals based on columns such as <span class=\"hue-doc-codeph\">YEAR</span>,\n          <span class=\"hue-doc-codeph\">MONTH</span>, and/or <span class=\"hue-doc-codeph\">DAY</span>, or for geographic regions.\n          Remember that Parquet data files use a <span class=\"hue-doc-ph\">large</span> block\n          size, so when deciding how finely to partition the data, try to find a granularity\n          where each partition contains <span class=\"hue-doc-ph\">256 MB</span> or more of\n          data, rather than creating a large number of smaller files split among many\n          partitions.\n        </p><p>\n          Inserting into a partitioned Parquet table can be a resource-intensive operation,\n          because each Impala node could potentially be writing a separate data file to HDFS for\n          each combination of different values for the partition key columns. The large number\n          of simultaneous open files could exceed the HDFS <q>transceivers</q> limit. To avoid\n          exceeding this limit, consider the following techniques:\n        </p><ul><li>\n            Load different subsets of data using separate <span class=\"hue-doc-codeph\">INSERT</span> statements\n            with specific values for the <span class=\"hue-doc-codeph\">PARTITION</span> clause, such as\n            <span class=\"hue-doc-codeph\">PARTITION (year=2010)</span>.\n          </li><li>\n            Increase the <q>transceivers</q> value for HDFS, sometimes spelled <q>xcievers</q>\n            (sic). The property value in the <span class=\"hue-doc-filepath\">hdfs-site.xml</span> configuration\n            file is <span class=\"hue-doc-codeph\">dfs.datanode.max.transfer.threads</span>. For example, if you were\n            loading 12 years of data partitioned by year, month, and day, even a value of 4096\n            might not be high enough. This\n            <a class=\"hue-doc-external-link\" href=\"http://blog.cloudera.com/blog/2012/03/hbase-hadoop-xceivers/\" target=\"_blank\">blog post</a> explores the\n            considerations for setting this value higher or lower, using HBase examples for\n            illustration.\n          </li><li>\n            Use the <span class=\"hue-doc-codeph\">COMPUTE STATS</span> statement to collect\n            <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_perf_stats.xml\" data-doc-anchor-id=\"perf_column_stats\">column statistics</a> on the\n            source table from which data is being copied, so that the Impala query can estimate\n            the number of different values in the partition key columns and distribute the work\n            accordingly.\n          </li></ul></div></div></div><div id=\"parquet_compression\"><div class=\"hue-doc-title\">Compressions for Parquet Data Files</div><div><p>\n        When Impala writes Parquet data files using the <span class=\"hue-doc-codeph\">INSERT</span> statement, the\n        underlying compression is controlled by the <span class=\"hue-doc-codeph\">COMPRESSION_CODEC</span> query\n        option. (Prior to Impala 2.0, the query option name was\n        <span class=\"hue-doc-codeph\">PARQUET_COMPRESSION_CODEC</span>.) The allowed values for this query option\n        are <span class=\"hue-doc-codeph\">snappy</span> (the default), <span class=\"hue-doc-codeph\">gzip</span>, <span class=\"hue-doc-codeph\">zstd</span>,\n        <span class=\"hue-doc-codeph\">lz4</span>, and <span class=\"hue-doc-codeph\">none</span>. The option value is not case-sensitive.\n        If the option is set to an unrecognized value, all kinds of queries will fail due to\n        the invalid option setting, not just queries involving Parquet tables.\n      </p></div><div id=\"parquet_snappy\"><div class=\"hue-doc-title\">Example of Parquet Table with Snappy Compression</div><div><p>\n          By default, the underlying data files for a Parquet table are compressed with Snappy.\n          The combination of fast compression and decompression makes it a good choice for many\n          data sets. To ensure Snappy compression is used, for example after experimenting with\n          other compression codecs, set the <span class=\"hue-doc-codeph\">COMPRESSION_CODEC</span> query option to\n          <span class=\"hue-doc-codeph\">snappy</span> before inserting the data:\n        </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; create database parquet_compression;\n[localhost:21000] &gt; use parquet_compression;\n[localhost:21000] &gt; create table parquet_snappy like raw_text_data;\n[localhost:21000] &gt; set COMPRESSION_CODEC=snappy;\n[localhost:21000] &gt; insert into parquet_snappy select * from raw_text_data;\nInserted 1000000000 rows in 181.98s\n</div></div></div><div id=\"parquet_gzip\"><div class=\"hue-doc-title\">Example of Parquet Table with GZip Compression</div><div><p>\n          If you need more intensive compression (at the expense of more CPU cycles for\n          uncompressing during queries), set the <span class=\"hue-doc-codeph\">COMPRESSION_CODEC</span> query option\n          to <span class=\"hue-doc-codeph\">gzip</span> before inserting the data:\n        </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; create table parquet_gzip like raw_text_data;\n[localhost:21000] &gt; set COMPRESSION_CODEC=gzip;\n[localhost:21000] &gt; insert into parquet_gzip select * from raw_text_data;\nInserted 1000000000 rows in 1418.24s\n</div></div></div><div id=\"parquet_none\"><div class=\"hue-doc-title\">Example of Uncompressed Parquet Table</div><div><p>\n          If your data compresses very poorly, or you want to avoid the CPU overhead of\n          compression and decompression entirely, set the <span class=\"hue-doc-codeph\">COMPRESSION_CODEC</span>\n          query option to <span class=\"hue-doc-codeph\">none</span> before inserting the data:\n        </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; create table parquet_none like raw_text_data;\n[localhost:21000] &gt; set COMPRESSION_CODEC=none;\n[localhost:21000] &gt; insert into parquet_none select * from raw_text_data;\nInserted 1000000000 rows in 146.90s\n</div></div></div><div id=\"parquet_compression_examples\"><div class=\"hue-doc-title\">Examples of Sizes and Speeds for Compressed Parquet Tables</div><div><p>\n          Here are some examples showing differences in data sizes and query speeds for 1\n          billion rows of synthetic data, compressed with each kind of codec. As always, run\n          similar tests with realistic data sets of your own. The actual compression ratios, and\n          relative insert and query speeds, will vary depending on the characteristics of the\n          actual data.\n        </p><p>\n          In this case, switching from Snappy to GZip compression shrinks the data by an\n          additional 40% or so, while switching from Snappy compression to no compression\n          expands the data also by about 40%:\n        </p><div class=\"hue-doc-codeblock\">$ hdfs dfs -du -h /user/hive/warehouse/parquet_compression.db\n23.1 G  /user/hive/warehouse/parquet_compression.db/parquet_snappy\n13.5 G  /user/hive/warehouse/parquet_compression.db/parquet_gzip\n32.8 G  /user/hive/warehouse/parquet_compression.db/parquet_none\n</div><p>\n          Because Parquet data files are typically <span class=\"hue-doc-ph\">large</span>, each\n          directory will have a different number of data files and the row groups will be\n          arranged differently.\n        </p><p>\n          At the same time, the less agressive the compression, the faster the data can be\n          decompressed. In this case using a table with a billion rows, a query that evaluates\n          all the values for a particular column runs faster with no compression than with\n          Snappy compression, and faster with Snappy compression than with Gzip compression.\n          Query performance depends on several other factors, so as always, run your own\n          benchmarks with your own data to determine the ideal tradeoff between data size, CPU\n          efficiency, and speed of insert and query operations.\n        </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; desc parquet_snappy;\nQuery finished, fetching results ...\n+-----------+---------+---------+\n| name      | type    | comment |\n+-----------+---------+---------+\n| id        | int     |         |\n| val       | int     |         |\n| zfill     | string  |         |\n| name      | string  |         |\n| assertion | boolean |         |\n+-----------+---------+---------+\nReturned 5 row(s) in 0.14s\n[localhost:21000] &gt; select avg(val) from parquet_snappy;\nQuery finished, fetching results ...\n+-----------------+\n| _c0             |\n+-----------------+\n| 250000.93577915 |\n+-----------------+\nReturned 1 row(s) in 4.29s\n[localhost:21000] &gt; select avg(val) from parquet_gzip;\nQuery finished, fetching results ...\n+-----------------+\n| _c0             |\n+-----------------+\n| 250000.93577915 |\n+-----------------+\nReturned 1 row(s) in 6.97s\n[localhost:21000] &gt; select avg(val) from parquet_none;\nQuery finished, fetching results ...\n+-----------------+\n| _c0             |\n+-----------------+\n| 250000.93577915 |\n+-----------------+\nReturned 1 row(s) in 3.67s\n</div></div></div><div id=\"parquet_compression_multiple\"><div class=\"hue-doc-title\">Example of Copying Parquet Data Files</div><div><p>\n          Here is a final example, to illustrate how the data files using the various\n          compression codecs are all compatible with each other for read operations. The\n          metadata about the compression format is written into each data file, and can be\n          decoded during queries regardless of the <span class=\"hue-doc-codeph\">COMPRESSION_CODEC</span> setting in\n          effect at the time. In this example, we copy data files from the\n          <span class=\"hue-doc-codeph\">PARQUET_SNAPPY</span>, <span class=\"hue-doc-codeph\">PARQUET_GZIP</span>, and\n          <span class=\"hue-doc-codeph\">PARQUET_NONE</span> tables used in the previous examples, each containing 1\n          billion rows, all to the data directory of a new table\n          <span class=\"hue-doc-codeph\">PARQUET_EVERYTHING</span>. A couple of sample queries demonstrate that the\n          new table now contains 3 billion rows featuring a variety of compression codecs for\n          the data files.\n        </p><p>\n          First, we create the table in Impala so that there is a destination directory in HDFS\n          to put the data files:\n        </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; create table parquet_everything like parquet_snappy;\nQuery: create table parquet_everything like parquet_snappy\n</div><p>\n          Then in the shell, we copy the relevant data files into the data directory for this\n          new table. Rather than using <span class=\"hue-doc-codeph\">hdfs dfs -cp</span> as with typical files, we\n          use <span class=\"hue-doc-codeph\">hadoop distcp -pb</span> to ensure that the special\n          <span class=\"hue-doc-ph\"> block size</span> of the Parquet data files is preserved.\n        </p><div class=\"hue-doc-codeblock\">$ hadoop distcp -pb /user/hive/warehouse/parquet_compression.db/parquet_snappy \\\n  /user/hive/warehouse/parquet_compression.db/parquet_everything\n...<span class=\"hue-doc-varname\">MapReduce output</span>...\n$ hadoop distcp -pb /user/hive/warehouse/parquet_compression.db/parquet_gzip  \\\n  /user/hive/warehouse/parquet_compression.db/parquet_everything\n...<span class=\"hue-doc-varname\">MapReduce output</span>...\n$ hadoop distcp -pb /user/hive/warehouse/parquet_compression.db/parquet_none  \\\n  /user/hive/warehouse/parquet_compression.db/parquet_everything\n...<span class=\"hue-doc-varname\">MapReduce output</span>...\n</div><p>\n          Back in the <span class=\"hue-doc-cmdname\">impala-shell</span> interpreter, we use the\n          <span class=\"hue-doc-codeph\">REFRESH</span> statement to alert the Impala server to the new data files\n          for this table, then we can run queries demonstrating that the data files represent 3\n          billion rows, and the values for one of the numeric columns match what was in the\n          original smaller tables:\n        </p><div class=\"hue-doc-codeblock\">[localhost:21000] &gt; refresh parquet_everything;\nQuery finished, fetching results ...\n\nReturned 0 row(s) in 0.32s\n[localhost:21000] &gt; select count(*) from parquet_everything;\nQuery finished, fetching results ...\n+------------+\n| _c0        |\n+------------+\n| 3000000000 |\n+------------+\nReturned 1 row(s) in 8.18s\n[localhost:21000] &gt; select avg(val) from parquet_everything;\nQuery finished, fetching results ...\n+-----------------+\n| _c0             |\n+-----------------+\n| 250000.93577915 |\n+-----------------+\nReturned 1 row(s) in 13.35s\n</div></div></div></div><div id=\"parquet_complex_types\"><div class=\"hue-doc-title\">Parquet Tables for Impala Complex Types</div><div><p id=\"complex_types_short_intro\">\n        In Impala 2.3 and higher, Impala supports the complex types\n        <span class=\"hue-doc-codeph\">ARRAY</span>, <span class=\"hue-doc-codeph\">STRUCT</span>, and <span class=\"hue-doc-codeph\">MAP</span>. In\n        Impala 3.2 and higher, Impala also supports these\n        complex types in ORC. See\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_complex_types.xml\" data-doc-anchor-id=\"complex_types\">Complex Types (Impala 2.3 or higher only)</a> for details.\n        These Complex types are currently supported only for the Parquet or ORC file formats.\n        Because Impala has better performance on Parquet than ORC, if you plan to use complex\n        types, become familiar with the performance and storage aspects of Parquet first.\n      </p></div></div><div id=\"parquet_interop\"><div class=\"hue-doc-title\">Exchanging Parquet Data Files with Other Hadoop Components</div><div><p>\n        You can read and write Parquet data files from other Hadoop components. See\n        \n    the documentation for your Apache Hadoop distribution\n   for details.\n      </p><p>\n        Previously, it was not possible to create Parquet data through Impala and reuse that\n        table within Hive. Now that Parquet support is available for Hive, reusing existing\n        Impala Parquet data files in Hive requires updating the table metadata. Use the\n        following command if you are already running Impala 1.1.1 or higher:\n      </p><div class=\"hue-doc-codeblock\">ALTER TABLE <span class=\"hue-doc-varname\">table_name</span> SET FILEFORMAT PARQUET;\n</div><p>\n        If you are running a level of Impala that is older than 1.1.1, do the metadata update\n        through Hive:\n      </p><div class=\"hue-doc-codeblock\">ALTER TABLE <span class=\"hue-doc-varname\">table_name</span> SET SERDE 'parquet.hive.serde.ParquetHiveSerDe';\nALTER TABLE <span class=\"hue-doc-varname\">table_name</span> SET FILEFORMAT\n  INPUTFORMAT \"parquet.hive.DeprecatedParquetInputFormat\"\n  OUTPUTFORMAT \"parquet.hive.DeprecatedParquetOutputFormat\";\n</div><p>\n        Impala 1.1.1 and higher can reuse Parquet data files created by Hive, without any action\n        required.\n      </p><p>\n        Impala supports the scalar data types that you can encode in a Parquet data file, but\n        not composite or nested types such as maps or arrays. In\n        Impala 2.2 and higher, Impala can query Parquet data files that\n        include composite or nested types, as long as the query only refers to columns with\n        scalar types.\n</p><p>\n        If you copy Parquet data files between nodes, or even between different directories on\n        the same node, make sure to preserve the block size by using the command <span class=\"hue-doc-codeph\">hadoop\n        distcp -pb</span>. To verify that the block size was preserved, issue the command\n        <span class=\"hue-doc-codeph\">hdfs fsck -blocks <span class=\"hue-doc-varname\">HDFS_path_of_impala_table_dir</span></span> and\n        check that the average block size is at or near <span class=\"hue-doc-ph\">256 MB (or\n        whatever other size is defined by the <span class=\"hue-doc-codeph\">PARQUET_FILE_SIZE</span> query\n        option).</span>. (The <span class=\"hue-doc-codeph\">hadoop distcp</span> operation typically leaves some\n        directories behind, with names matching <span class=\"hue-doc-filepath\">_distcp_logs_*</span>, that you\n        can delete from the destination directory afterward.)\n\n        Issue the command <span class=\"hue-doc-cmdname\">hadoop distcp</span> for details about\n        <span class=\"hue-doc-cmdname\">distcp</span> command syntax.\n      </p><p id=\"impala_parquet_encodings_caveat\">\n        Impala can query Parquet files that use the <span class=\"hue-doc-codeph\">PLAIN</span>,\n        <span class=\"hue-doc-codeph\">PLAIN_DICTIONARY</span>, <span class=\"hue-doc-codeph\">BIT_PACKED</span>, <span class=\"hue-doc-codeph\">RLE</span>\n        and <span class=\"hue-doc-codeph\">RLE_DICTIONARY</span> encodings. <span class=\"hue-doc-codeph\">RLE_DICTIONARY</span> is supported\n        only in Impala 4.0 and up.\n        When creating files outside of Impala for use by Impala, make sure to use one of the\n        supported encodings. In particular, for MapReduce jobs,\n        <span class=\"hue-doc-codeph\">parquet.writer.version</span> must not be defined (especially as\n        <span class=\"hue-doc-codeph\">PARQUET_2_0</span>) for writing the configurations of Parquet MR jobs. Use the\n        default version (or format). The default format, 1.0, includes some enhancements that\n        are compatible with older versions. Data using the 2.0 format might not be consumable by\n        Impala, due to use of the <span class=\"hue-doc-codeph\">RLE_DICTIONARY</span> encoding.\n      </p><p id=\"parquet_tools_blurb\">\n        To examine the internal structure and data of Parquet files, you can use the\n        <span class=\"hue-doc-cmdname\">parquet-tools</span> command. Make sure this command is in your\n        <span class=\"hue-doc-codeph\">$PATH</span>. (Typically, it is symlinked from <span class=\"hue-doc-filepath\">/usr/bin</span>;\n        sometimes, depending on your installation setup, you might need to locate it under an\n        alternative <span class=\"hue-doc-codeph\">bin</span> directory.) The arguments to this command let you\n        perform operations such as:\n        <ul><li><span class=\"hue-doc-codeph\">cat</span>: Print a file's contents to standard out. In\n            Impala 2.3 and higher, you can use the <span class=\"hue-doc-codeph\">-j</span>\n            option to output JSON.\n          </li><li><span class=\"hue-doc-codeph\">head</span>: Print the first few records of a file to standard output.\n          </li><li><span class=\"hue-doc-codeph\">schema</span>: Print the Parquet schema for the file.\n          </li><li><span class=\"hue-doc-codeph\">meta</span>: Print the file footer metadata, including key-value\n            properties (like Avro schema), compression ratios, encodings, compression used, and\n            row group information.\n          </li><li><span class=\"hue-doc-codeph\">dump</span>: Print all data and metadata.\n          </li></ul>\n        Use <span class=\"hue-doc-codeph\">parquet-tools -h</span> to see usage information for all the arguments.\n        Here are some examples showing <span class=\"hue-doc-cmdname\">parquet-tools</span> usage:\n<div class=\"hue-doc-codeblock\">$ # Be careful doing this for a big file! Use parquet-tools head to be safe.\n$ parquet-tools cat sample.parq\nyear = 1992\nmonth = 1\nday = 2\ndayofweek = 4\ndep_time = 748\ncrs_dep_time = 750\narr_time = 851\ncrs_arr_time = 846\ncarrier = US\nflight_num = 53\nactual_elapsed_time = 63\ncrs_elapsed_time = 56\narrdelay = 5\ndepdelay = -2\norigin = CMH\ndest = IND\ndistance = 182\ncancelled = 0\ndiverted = 0\n\nyear = 1992\nmonth = 1\nday = 3\n...\n</div><div class=\"hue-doc-codeblock\">$ parquet-tools head -n 2 sample.parq\nyear = 1992\nmonth = 1\nday = 2\ndayofweek = 4\ndep_time = 748\ncrs_dep_time = 750\narr_time = 851\ncrs_arr_time = 846\ncarrier = US\nflight_num = 53\nactual_elapsed_time = 63\ncrs_elapsed_time = 56\narrdelay = 5\ndepdelay = -2\norigin = CMH\ndest = IND\ndistance = 182\ncancelled = 0\ndiverted = 0\n\nyear = 1992\nmonth = 1\nday = 3\n...\n</div><div class=\"hue-doc-codeblock\">$ parquet-tools schema sample.parq\nmessage schema {\n  optional int32 year;\n  optional int32 month;\n  optional int32 day;\n  optional int32 dayofweek;\n  optional int32 dep_time;\n  optional int32 crs_dep_time;\n  optional int32 arr_time;\n  optional int32 crs_arr_time;\n  optional binary carrier;\n  optional int32 flight_num;\n...\n</div><div class=\"hue-doc-codeblock\">$ parquet-tools meta sample.parq\ncreator:             impala version 2.2.0-...\n\nfile schema:         schema\n-------------------------------------------------------------------\nyear:                OPTIONAL INT32 R:0 D:1\nmonth:               OPTIONAL INT32 R:0 D:1\nday:                 OPTIONAL INT32 R:0 D:1\ndayofweek:           OPTIONAL INT32 R:0 D:1\ndep_time:            OPTIONAL INT32 R:0 D:1\ncrs_dep_time:        OPTIONAL INT32 R:0 D:1\narr_time:            OPTIONAL INT32 R:0 D:1\ncrs_arr_time:        OPTIONAL INT32 R:0 D:1\ncarrier:             OPTIONAL BINARY R:0 D:1\nflight_num:          OPTIONAL INT32 R:0 D:1\n...\n\nrow group 1:         RC:20636601 TS:265103674\n-------------------------------------------------------------------\nyear:                 INT32 SNAPPY DO:4 FPO:35 SZ:10103/49723/4.92 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\nmonth:                INT32 SNAPPY DO:10147 FPO:10210 SZ:11380/35732/3.14 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\nday:                  INT32 SNAPPY DO:21572 FPO:21714 SZ:3071658/9868452/3.21 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\ndayofweek:            INT32 SNAPPY DO:3093276 FPO:3093319 SZ:2274375/5941876/2.61 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\ndep_time:             INT32 SNAPPY DO:5367705 FPO:5373967 SZ:28281281/28573175/1.01 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\ncrs_dep_time:         INT32 SNAPPY DO:33649039 FPO:33654262 SZ:10220839/11574964/1.13 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\narr_time:             INT32 SNAPPY DO:43869935 FPO:43876489 SZ:28562410/28797767/1.01 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\ncrs_arr_time:         INT32 SNAPPY DO:72432398 FPO:72438151 SZ:10908972/12164626/1.12 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\ncarrier:              BINARY SNAPPY DO:83341427 FPO:83341558 SZ:114916/128611/1.12 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\nflight_num:           INT32 SNAPPY DO:83456393 FPO:83488603 SZ:10216514/11474301/1.12 VC:20636601 ENC:PLAIN_DICTIONARY,RLE,PLAIN\n...\n</div></p></div></div><div id=\"parquet_data_files\"><div class=\"hue-doc-title\">How Parquet Data Files Are Organized</div><div><p>\n        Although Parquet is a column-oriented file format, do not expect to find one data file\n        for each column. Parquet keeps all the data for a row within the same data file, to\n        ensure that the columns for a row are always available on the same node for processing.\n        What Parquet does is to set a large HDFS block size and a matching maximum data file\n        size, to ensure that I/O and network transfer requests apply to large batches of data.\n      </p><p>\n        Within that data file, the data for a set of rows is rearranged so that all the values\n        from the first column are organized in one contiguous block, then all the values from\n        the second column, and so on. Putting the values from the same column next to each other\n        lets Impala use effective compression techniques on the values in that column.\n      </p><div class=\"hue-doc-note\"><p>\n          Impala <span class=\"hue-doc-codeph\">INSERT</span> statements write Parquet data files using an HDFS block\n          size <span class=\"hue-doc-ph\">that matches the data file size</span>, to ensure that\n          each data file is represented by a single HDFS block, and the entire file can be\n          processed on a single node without requiring any remote reads.\n        </p><p>\n          If you create Parquet data files outside of Impala, such as through a MapReduce or Pig\n          job, ensure that the HDFS block size is greater than or equal to the file size, so\n          that the <q>one file per block</q> relationship is maintained. Set the\n          <span class=\"hue-doc-codeph\">dfs.block.size</span> or the <span class=\"hue-doc-codeph\">dfs.blocksize</span> property large\n          enough that each file fits within a single HDFS block, even if that size is larger\n          than the normal HDFS block size.\n        </p><p>\n          If the block size is reset to a lower value during a file copy, you will see lower\n          performance for queries involving those files, and the <span class=\"hue-doc-codeph\">PROFILE</span>\n          statement will reveal that some I/O is being done suboptimally, through remote reads.\n          See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet_compression_multiple\">Example of Copying Parquet Data Files</a> for an example\n          showing how to preserve the block size when copying Parquet data files.\n        </p></div><p>\n        When Impala retrieves or tests the data for a particular column, it opens all the data\n        files, but only reads the portion of each file containing the values for that column.\n        The column values are stored consecutively, minimizing the I/O required to process the\n        values within a single column. If other columns are named in the <span class=\"hue-doc-codeph\">SELECT</span>\n        list or <span class=\"hue-doc-codeph\">WHERE</span> clauses, the data for all columns in the same row is\n        available within that same data file.\n      </p><p>\n        If an <span class=\"hue-doc-codeph\">INSERT</span> statement brings in less than\n        <span class=\"hue-doc-ph\">one Parquet block's worth</span> of data, the resulting data\n        file is smaller than ideal. Thus, if you do split up an ETL job to use multiple\n        <span class=\"hue-doc-codeph\">INSERT</span> statements, try to keep the volume of data for each\n        <span class=\"hue-doc-codeph\">INSERT</span> statement to approximately <span class=\"hue-doc-ph\">256 MB,\n        or a multiple of 256 MB</span>.\n      </p></div><div id=\"parquet_encoding\"><div class=\"hue-doc-title\">RLE and Dictionary Encoding for Parquet Data Files</div><div><p>\n          Parquet uses some automatic compression techniques, such as run-length encoding (RLE)\n          and dictionary encoding, based on analysis of the actual data values. Once the data\n          values are encoded in a compact form, the encoded data can optionally be further\n          compressed using a compression algorithm. Parquet data files created by Impala can use\n          Snappy, GZip, or no compression; the Parquet spec also allows LZO compression, but\n          currently Impala does not support LZO-compressed Parquet files.\n        </p><p>\n          RLE and dictionary encoding are compression techniques that Impala applies\n          automatically to groups of Parquet data values, in addition to any Snappy or GZip\n          compression applied to the entire data files. These automatic optimizations can save\n          you time and planning that are normally needed for a traditional data warehouse. For\n          example, dictionary encoding reduces the need to create numeric IDs as abbreviations\n          for longer string values.\n        </p><p>\n          Run-length encoding condenses sequences of repeated data values. For example, if many\n          consecutive rows all contain the same value for a country code, those repeating values\n          can be represented by the value followed by a count of how many times it appears\n          consecutively.\n        </p><p>\n          Dictionary encoding takes the different values present in a column, and represents\n          each one in compact 2-byte form rather than the original value, which could be several\n          bytes. (Additional compression is applied to the compacted values, for extra space\n          savings.) This type of encoding applies when the number of different values for a\n          column is less than 2**16 (16,384). It does not apply to columns of data type\n          <span class=\"hue-doc-codeph\">BOOLEAN</span>, which are already very short. <span class=\"hue-doc-codeph\">TIMESTAMP</span>\n          columns sometimes have a unique value for each row, in which case they can quickly\n          exceed the 2**16 limit on distinct values. The 2**16 limit on different values within\n          a column is reset for each data file, so if several different data files each\n          contained 10,000 different city names, the city name column in each data file could\n          still be condensed using dictionary encoding.\n        </p></div></div></div><div id=\"parquet_compacting\"><div class=\"hue-doc-title\">Compacting Data Files for Parquet Tables</div><div><p>\n        If you reuse existing table structures or ETL processes for Parquet tables, you might\n        encounter a <q>many small files</q> situation, which is suboptimal for query efficiency.\n        For example, statements like these might produce inefficiently organized data files:\n      </p><div class=\"hue-doc-codeblock\">-- In an N-node cluster, each node produces a data file\n-- for the INSERT operation. If you have less than\n-- N GB of data to copy, some files are likely to be\n-- much smaller than the <span class=\"hue-doc-ph\">default Parquet</span> block size.\ninsert into parquet_table select * from text_table;\n\n-- Even if this operation involves an overall large amount of data,\n-- when split up by year/month/day, each partition might only\n-- receive a small amount of data. Then the data files for\n-- the partition might be divided between the N nodes in the cluster.\n-- A multi-gigabyte copy operation might produce files of only\n-- a few MB each.\ninsert into partitioned_parquet_table partition (year, month, day)\n  select year, month, day, url, referer, user_agent, http_code, response_time\n  from web_stats;\n</div><p>\n        Here are techniques to help you produce large data files in Parquet\n        <span class=\"hue-doc-codeph\">INSERT</span> operations, and to compact existing too-small data files:\n      </p><ul><li><p>\n            When inserting into a partitioned Parquet table, use statically partitioned\n            <span class=\"hue-doc-codeph\">INSERT</span> statements where the partition key values are specified as\n            constant values. Ideally, use a separate <span class=\"hue-doc-codeph\">INSERT</span> statement for each\n            partition.\n          </p></li><li><p id=\"num_nodes_tip\">\n        You might set the <span class=\"hue-doc-codeph\">NUM_NODES</span> option to 1 briefly, during\n        <span class=\"hue-doc-codeph\">INSERT</span> or <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span> statements. Normally,\n        those statements produce one or more data files per data node. If the write operation\n        involves small amounts of data, a Parquet table, and/or a partitioned table, the default\n        behavior could produce many small files when intuitively you might expect only a single\n        output file. <span class=\"hue-doc-codeph\">SET NUM_NODES=1</span> turns off the <q>distributed</q> aspect of\n        the write operation, making it more likely to produce only one or a few data files.\n      </p></li><li><p>\n            Be prepared to reduce the number of partition key columns from what you are used to\n            with traditional analytic database systems.\n          </p></li><li><p>\n            Do not expect Impala-written Parquet files to fill up the entire Parquet block size.\n            Impala estimates on the conservative side when figuring out how much data to write\n            to each Parquet file. Typically, the of uncompressed data in memory is substantially\n            reduced on disk by the compression and encoding techniques in the Parquet file\n            format.\n\n            The final data file size varies depending on the compressibility of the data.\n            Therefore, it is not an indication of a problem if <span class=\"hue-doc-ph\">256\n            MB</span> of text data is turned into 2 Parquet data files, each less than\n            <span class=\"hue-doc-ph\">256 MB</span>.\n          </p></li><li><p>\n            If you accidentally end up with a table with many small data files, consider using\n            one or more of the preceding techniques and copying all the data into a new Parquet\n            table, either through <span class=\"hue-doc-codeph\">CREATE TABLE AS SELECT</span> or <span class=\"hue-doc-codeph\">INSERT ...\n            SELECT</span> statements.\n          </p><p>\n            To avoid rewriting queries to change table names, you can adopt a convention of\n            always running important queries against a view. Changing the view definition\n            immediately switches any subsequent queries to use the new underlying tables:\n          </p><div class=\"hue-doc-codeblock\">create view production_table as select * from table_with_many_small_files;\n-- CTAS or INSERT...SELECT all the data into a more efficient layout...\nalter view production_table as select * from table_with_few_big_files;\nselect * from production_table where c1 = 100 and c2 &lt; 50 and ...;\n</div></li></ul></div></div><div id=\"parquet_schema_evolution\"><div class=\"hue-doc-title\">Schema Evolution for Parquet Tables</div><div><p>\n        Schema evolution refers to using the statement <span class=\"hue-doc-codeph\">ALTER TABLE ... REPLACE\n        COLUMNS</span> to change the names, data type, or number of columns in a table. You\n        can perform schema evolution for Parquet tables as follows:\n      </p><ul><li><p>\n            The Impala <span class=\"hue-doc-codeph\">ALTER TABLE</span> statement never changes any data files in\n            the tables. From the Impala side, schema evolution involves interpreting the same\n            data files in terms of a new table definition. Some types of schema changes make\n            sense and are represented correctly. Other types of changes cannot be represented in\n            a sensible way, and produce special result values or conversion errors during\n            queries.\n          </p></li><li><p>\n            The <span class=\"hue-doc-codeph\">INSERT</span> statement always creates data using the latest table\n            definition. You might end up with data files with different numbers of columns or\n            internal data representations if you do a sequence of <span class=\"hue-doc-codeph\">INSERT</span> and\n            <span class=\"hue-doc-codeph\">ALTER TABLE ... REPLACE COLUMNS</span> statements.\n          </p></li><li><p>\n            If you use <span class=\"hue-doc-codeph\">ALTER TABLE ... REPLACE COLUMNS</span> to define additional\n            columns at the end, when the original data files are used in a query, these final\n            columns are considered to be all <span class=\"hue-doc-codeph\">NULL</span> values.\n          </p></li><li><p>\n            If you use <span class=\"hue-doc-codeph\">ALTER TABLE ... REPLACE COLUMNS</span> to define fewer columns\n            than before, when the original data files are used in a query, the unused columns\n            still present in the data file are ignored.\n          </p></li><li><p>\n            Parquet represents the <span class=\"hue-doc-codeph\">TINYINT</span>, <span class=\"hue-doc-codeph\">SMALLINT</span>, and\n            <span class=\"hue-doc-codeph\">INT</span> types the same internally, all stored in 32-bit integers.\n          </p><ul><li>\n              That means it is easy to promote a <span class=\"hue-doc-codeph\">TINYINT</span> column to\n              <span class=\"hue-doc-codeph\">SMALLINT</span> or <span class=\"hue-doc-codeph\">INT</span>, or a <span class=\"hue-doc-codeph\">SMALLINT</span>\n              column to <span class=\"hue-doc-codeph\">INT</span>. The numbers are represented exactly the same in\n              the data file, and the columns being promoted would not contain any out-of-range\n              values.\n            </li><li><p>\n                If you change any of these column types to a smaller type, any values that are\n                out-of-range for the new type are returned incorrectly, typically as negative\n                numbers.\n              </p></li><li><p>\n                You cannot change a <span class=\"hue-doc-codeph\">TINYINT</span>, <span class=\"hue-doc-codeph\">SMALLINT</span>, or\n                <span class=\"hue-doc-codeph\">INT</span> column to <span class=\"hue-doc-codeph\">BIGINT</span>, or the other way around.\n                Although the <span class=\"hue-doc-codeph\">ALTER TABLE</span> succeeds, any attempt to query those\n                columns results in conversion errors.\n              </p></li><li><p>\n                Any other type conversion for columns produces a conversion error during\n                queries. For example, <span class=\"hue-doc-codeph\">INT</span> to <span class=\"hue-doc-codeph\">STRING</span>,\n                <span class=\"hue-doc-codeph\">FLOAT</span> to <span class=\"hue-doc-codeph\">DOUBLE</span>, <span class=\"hue-doc-codeph\">TIMESTAMP</span> to\n                <span class=\"hue-doc-codeph\">STRING</span>, <span class=\"hue-doc-codeph\">DECIMAL(9,0)</span> to\n                <span class=\"hue-doc-codeph\">DECIMAL(5,2)</span>, and so on.\n              </p></li></ul></li></ul><p>\n        You might find that you have Parquet files where the columns do not line up in the same\n        order as in your Impala table. For example, you might have a Parquet file that was part\n        of a table with columns <span class=\"hue-doc-codeph\">C1,C2,C3,C4</span>, and now you want to reuse the same\n        Parquet file in a table with columns <span class=\"hue-doc-codeph\">C4,C2</span>. By default, Impala expects\n        the columns in the data file to appear in the same order as the columns defined for the\n        table, making it impractical to do some kinds of file reuse or schema evolution. In\n        Impala 2.6 and higher, the query option\n        <span class=\"hue-doc-codeph\">PARQUET_FALLBACK_SCHEMA_RESOLUTION=name</span> lets Impala resolve columns by\n        name, and therefore handle out-of-order or extra columns in the data file. For example:\n<div class=\"hue-doc-codeblock\" id=\"parquet_fallback_schema_resolution_example\">create database schema_evolution;\nuse schema_evolution;\ncreate table t1 (c1 int, c2 boolean, c3 string, c4 timestamp)\n  stored as parquet;\ninsert into t1 values\n  (1, true, 'yes', now()),\n  (2, false, 'no', now() + interval 1 day);\n\nselect * from t1;\n+----+-------+-----+-------------------------------+\n| c1 | c2    | c3  | c4                            |\n+----+-------+-----+-------------------------------+\n| 1  | true  | yes | 2016-06-28 14:53:26.554369000 |\n| 2  | false | no  | 2016-06-29 14:53:26.554369000 |\n+----+-------+-----+-------------------------------+\n\ndesc formatted t1;\n...\n| Location:   | /user/hive/warehouse/schema_evolution.db/t1 |\n...\n\n-- Make T2 have the same data file as in T1, including 2\n-- unused columns and column order different than T2 expects.\nload data inpath '/user/hive/warehouse/schema_evolution.db/t1'\n  into table t2;\n+----------------------------------------------------------+\n| summary                                                  |\n+----------------------------------------------------------+\n| Loaded 1 file(s). Total files in destination location: 1 |\n+----------------------------------------------------------+\n\n-- 'position' is the default setting.\n-- Impala cannot read the Parquet file if the column order does not match.\nset PARQUET_FALLBACK_SCHEMA_RESOLUTION=position;\nPARQUET_FALLBACK_SCHEMA_RESOLUTION set to position\n\nselect * from t2;\nWARNINGS:\nFile 'schema_evolution.db/t2/45331705_data.0.parq'\nhas an incompatible Parquet schema for column 'schema_evolution.t2.c4'.\nColumn type: TIMESTAMP, Parquet schema: optional int32 c1 [i:0 d:1 r:0]\n\nFile 'schema_evolution.db/t2/45331705_data.0.parq'\nhas an incompatible Parquet schema for column 'schema_evolution.t2.c4'.\nColumn type: TIMESTAMP, Parquet schema: optional int32 c1 [i:0 d:1 r:0]\n\n-- With the 'name' setting, Impala can read the Parquet data files\n-- despite mismatching column order.\nset PARQUET_FALLBACK_SCHEMA_RESOLUTION=name;\nPARQUET_FALLBACK_SCHEMA_RESOLUTION set to name\n\nselect * from t2;\n+-------------------------------+-------+\n| c4                            | c2    |\n+-------------------------------+-------+\n| 2016-06-28 14:53:26.554369000 | true  |\n| 2016-06-29 14:53:26.554369000 | false |\n+-------------------------------+-------+\n</div>\n        See\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet_fallback_schema_resolution.xml\" data-doc-anchor-id=\"parquet_fallback_schema_resolution\">PARQUET_FALLBACK_SCHEMA_RESOLUTION Query Option (Impala 2.6 or higher only)</a>\n        for more details.\n      </p></div></div><div id=\"parquet_data_types\"><div class=\"hue-doc-title\">Data Type Considerations for Parquet Tables</div><div><p>\n        The Parquet format defines a set of data types whose names differ from the names of the\n        corresponding Impala data types. If you are preparing Parquet files using other Hadoop\n        components such as Pig or MapReduce, you might need to work with the type names defined\n        by Parquet. The following tables list the Parquet-defined types and the equivalent types\n        in Impala.\n      </p><p><b>Primitive types</b></p><table id=\"simpletable_am3_rxn_wgb\"><tr class=\"hue-doc-sthead\"><td>Parquet type</td><td>Impala type</td></tr><tr><td>BINARY</td><td>STRING</td></tr><tr><td>BOOLEAN</td><td>BOOLEAN</td></tr><tr><td>DOUBLE</td><td>DOUBLE</td></tr><tr><td>FLOAT</td><td>FLOAT</td></tr><tr><td>INT32</td><td>INT</td></tr><tr><td>INT64</td><td>BIGINT</td></tr><tr><td>INT96</td><td>TIMESTAMP</td></tr></table><p><b>Logical types</b></p><p>\n        Parquet uses type annotations to extend the types that it can store, by specifying how\n        the primitive types should be interpreted.\n      </p><table id=\"simpletable_az3_byn_wgb\"><tr class=\"hue-doc-sthead\"><td>Parquet primitive type and annotation</td><td>Impala type</td></tr><tr><td>BINARY annotated with the UTF8 OriginalType</td><td>STRING</td></tr><tr><td>BINARY annotated with the STRING LogicalType</td><td>STRING</td></tr><tr><td>BINARY annotated with the  ENUM OriginalType</td><td>STRING</td></tr><tr><td>BINARY annotated with the DECIMAL OriginalType</td><td>DECIMAL</td></tr><tr><td>INT64 annotated with the TIMESTAMP_MILLIS\n            OriginalType</td><td>TIMESTAMP (in Impala 3.2 or\n              higher)<p>\n              or\n            </p>BIGINT (for backward compatibility)</td></tr><tr><td>INT64 annotated with the TIMESTAMP_MICROS\n            OriginalType</td><td>TIMESTAMP (in Impala 3.2 or\n              higher)<p>\n              or\n            </p>BIGINT (for backward compatibility)</td></tr><tr><td>INT64 annotated with the  TIMESTAMP LogicalType</td><td>TIMESTAMP (in Impala 3.2 or\n              higher)<p>\n              or\n            </p>BIGINT (for backward compatibility)</td></tr></table><p><b>Complex types:</b></p><p>\n        For the complex types (<span class=\"hue-doc-codeph\">ARRAY</span>, <span class=\"hue-doc-codeph\">MAP</span>, and\n        <span class=\"hue-doc-codeph\">STRUCT</span>) available in Impala 2.3 and higher,\n        Impala only supports queries against those types in Parquet tables.\n      </p></div></div></div></div>","title":"Using the Parquet File Format with Impala Tables"}