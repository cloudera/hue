{"body":"<div><div id=\"hdfs_caching\"><div class=\"hue-doc-title\">Using HDFS Caching with Impala (Impala 2.1 or higher only)</div><div><p>\n      HDFS caching provides performance and scalability benefits in production environments where Impala queries\n      and other Hadoop jobs operate on quantities of data much larger than the physical RAM on the DataNodes,\n      making it impractical to rely on the Linux OS cache, which only keeps the most recently used data in memory.\n      Data read from the HDFS cache avoids the overhead of checksumming and memory-to-memory copying involved when\n      using data from the Linux OS cache.\n    </p><div class=\"hue-doc-note\"><p>\n        On a small or lightly loaded cluster, HDFS caching might not produce any speedup. It might even lead to\n        slower queries, if I/O read operations that were performed in parallel across the entire cluster are replaced by in-memory\n        operations operating on a smaller number of hosts. The hosts where the HDFS blocks are cached can become\n        bottlenecks because they experience high CPU load while processing the cached data blocks, while other hosts remain idle.\n        Therefore, always compare performance with and without this feature enabled, using a realistic workload.\n      </p><p>\n        In Impala 2.2 and higher, you can spread the CPU load more evenly by specifying the <span class=\"hue-doc-codeph\">WITH REPLICATION</span>\n        clause of the <span class=\"hue-doc-codeph\">CREATE TABLE</span> and <span class=\"hue-doc-codeph\">ALTER TABLE</span> statements.\n        This clause lets you control the replication factor for\n        HDFS caching for a specific table or partition. By default, each cached block is\n        only present on a single host, which can lead to CPU contention if the same host\n        processes each cached block. Increasing the replication factor lets Impala choose\n        different hosts to process different cached blocks, to better distribute the CPU load.\n        Always use a <span class=\"hue-doc-codeph\">WITH REPLICATION</span> setting of at least 3, and adjust upward\n        if necessary to match the replication factor for the underlying HDFS data files.\n      </p><p>\n        In Impala 2.5 and higher, Impala automatically randomizes which host processes\n        a cached HDFS block, to avoid CPU hotspots. For tables where HDFS caching is not applied,\n        Impala designates which host to process a data block using an algorithm that estimates\n        the load on each host. If CPU hotspots still arise during queries,\n        you can enable additional randomization for the scheduling algorithm for non-HDFS cached data\n        by setting the <span class=\"hue-doc-codeph\">SCHEDULE_RANDOM_REPLICA</span> query option.\n      </p></div><p/><p>\n      For background information about how to set up and manage HDFS caching for a CDH cluster, see\n      \n    the documentation for your Apache Hadoop distribution\n  .\n    </p></div><div id=\"hdfs_caching_overview\"><div class=\"hue-doc-title\">Overview of HDFS Caching for Impala</div><div><p>\n        In Impala 1.4 and higher, Impala can use the HDFS caching feature to make more effective use of RAM, so that\n        repeated queries can take advantage of data <q>pinned</q> in memory regardless of how much data is\n        processed overall. The HDFS caching feature lets you designate a subset of frequently accessed data to be\n        pinned permanently in memory, remaining in the cache across multiple queries and never being evicted. This\n        technique is suitable for tables or partitions that are frequently accessed and are small enough to fit\n        entirely within the HDFS memory cache. For example, you might designate several dimension tables to be\n        pinned in the cache, to speed up many different join queries that reference them. Or in a partitioned\n        table, you might pin a partition holding data from the most recent time period because that data will be\n        queried intensively; then when the next set of data arrives, you could unpin the previous partition and pin\n        the partition holding the new data.\n      </p><p>\n        Because this Impala performance feature relies on HDFS infrastructure, it only applies to Impala tables\n        that use HDFS data files. HDFS caching for Impala does not apply to HBase tables, S3 tables,\n        Kudu tables,\n        or Isilon tables.\n      </p></div></div><div id=\"hdfs_caching_prereqs\"><div class=\"hue-doc-title\">Setting Up HDFS Caching for Impala</div><div><p>\n        To use HDFS caching with Impala, first set up that feature for your CDH cluster:\n      </p><ul><li><p>\n          Decide how much memory to devote to the HDFS cache on each host. Remember that the total memory available\n          for cached data is the sum of the cache sizes on all the hosts. By default, any data block is only cached on one\n          host, although you can cache a block across multiple hosts by increasing the replication factor.\n          </p></li><li><p>\n          Issue <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> commands to set up one or more cache pools, owned by the same\n          user as the <span class=\"hue-doc-cmdname\">impalad</span> daemon (typically <span class=\"hue-doc-codeph\">impala</span>). For example:\n<div class=\"hue-doc-codeblock\">hdfs cacheadmin -addPool four_gig_pool -owner impala -limit 4000000000\n</div>\n          For details about the <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> command, see\n          \n    the documentation for your Apache Hadoop distribution\n  .\n          </p></li></ul><p>\n        Once HDFS caching is enabled and one or more pools are available, see\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_perf_hdfs_caching.xml\" data-doc-anchor-id=\"hdfs_caching_ddl\">Enabling HDFS Caching for Impala Tables and Partitions</a> for how to choose which Impala data to load\n        into the HDFS cache. On the Impala side, you specify the cache pool name defined by the <span class=\"hue-doc-codeph\">hdfs\n        cacheadmin</span> command in the Impala DDL statements that enable HDFS caching for a table or partition,\n        such as <span class=\"hue-doc-codeph\">CREATE TABLE ... CACHED IN <span class=\"hue-doc-varname\">pool</span></span> or <span class=\"hue-doc-codeph\">ALTER TABLE ... SET\n        CACHED IN <span class=\"hue-doc-varname\">pool</span></span>.\n      </p></div></div><div id=\"hdfs_caching_ddl\"><div class=\"hue-doc-title\">Enabling HDFS Caching for Impala Tables and Partitions</div><div><p>\n        Begin by choosing which tables or partitions to cache. For example, these might be lookup tables that are\n        accessed by many different join queries, or partitions corresponding to the most recent time period that\n        are analyzed by different reports or ad hoc queries.\n      </p><p>\n        In your SQL statements, you specify logical divisions such as tables and partitions to be cached. Impala\n        translates these requests into HDFS-level directives that apply to particular directories and files. For\n        example, given a partitioned table <span class=\"hue-doc-codeph\">CENSUS</span> with a partition key column\n        <span class=\"hue-doc-codeph\">YEAR</span>, you could choose to cache all or part of the data as follows:\n      </p><p id=\"impala_cache_replication_factor\">\n        In Impala 2.2 and higher, the optional <span class=\"hue-doc-codeph\">WITH\n        REPLICATION</span> clause for <span class=\"hue-doc-codeph\">CREATE TABLE</span> and <span class=\"hue-doc-codeph\">ALTER\n        TABLE</span> lets you specify a <span class=\"hue-doc-term\">replication factor</span>, the number of hosts\n        on which to cache the same data blocks. When Impala processes a cached data block, where\n        the cache replication factor is greater than 1, Impala randomly selects a host that has\n        a cached copy of that data block. This optimization avoids excessive CPU usage on a\n        single host when the same cached data block is processed multiple times. Where\n        practical, specify a value greater than or equal to the HDFS block replication factor.\n      </p><div class=\"hue-doc-codeblock\">-- Cache the entire table (all partitions).\nalter table census set cached in '<span class=\"hue-doc-varname\">pool_name</span>';\n\n-- Remove the entire table from the cache.\nalter table census set uncached;\n\n-- Cache a portion of the table (a single partition).\n-- If the table is partitioned by multiple columns (such as year, month, day),\n-- the ALTER TABLE command must specify values for all those columns.\nalter table census partition (year=1960) set cached in '<span class=\"hue-doc-varname\">pool_name</span>';\n\n<span class=\"hue-doc-ph\">-- Cache the data from one partition on up to 4 hosts, to minimize CPU load on any\n-- single host when the same data block is processed multiple times.\nalter table census partition (year=1970)\n  set cached in '<span class=\"hue-doc-varname\">pool_name</span>' with replication = 4;</span>\n\n-- At each stage, check the volume of cached data.\n-- For large tables or partitions, the background loading might take some time,\n-- so you might have to wait and reissue the statement until all the data\n-- has finished being loaded into the cache.\nshow table stats census;\n+-------+-------+--------+------+--------------+--------+\n| year  | #Rows | #Files | Size | Bytes Cached | Format |\n+-------+-------+--------+------+--------------+--------+\n| 1900  | -1    | 1      | 11B  | NOT CACHED   | TEXT   |\n| 1940  | -1    | 1      | 11B  | NOT CACHED   | TEXT   |\n| 1960  | -1    | 1      | 11B  | 11B          | TEXT   |\n| 1970  | -1    | 1      | 11B  | NOT CACHED   | TEXT   |\n| Total | -1    | 4      | 44B  | 11B          |        |\n+-------+-------+--------+------+--------------+--------+\n</div><p><b>CREATE TABLE considerations:</b></p><p>\n        The HDFS caching feature affects the Impala <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement as follows:\n      </p><ul><li><p>\n          You can put a <span class=\"hue-doc-codeph\">CACHED IN '<span class=\"hue-doc-varname\">pool_name</span>'</span> clause\n          <span class=\"hue-doc-ph\">and optionally a <span class=\"hue-doc-codeph\">WITH REPLICATION = <span class=\"hue-doc-varname\">number_of_hosts</span></span> clause</span>\n          at the end of a\n          <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement to automatically cache the entire contents of the table,\n          including any partitions added later. The <span class=\"hue-doc-varname\">pool_name</span> is a pool that you previously set\n          up with the <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> command.\n        </p></li><li><p>\n          Once a table is designated for HDFS caching through the <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement, if new\n          partitions are added later through <span class=\"hue-doc-codeph\">ALTER TABLE ... ADD PARTITION</span> statements, the data in\n          those new partitions is automatically cached in the same pool.\n        </p></li><li><p>\n          If you want to perform repetitive queries on a subset of data from a large table, and it is not practical\n          to designate the entire table or specific partitions for HDFS caching, you can create a new cached table\n          with just a subset of the data by using <span class=\"hue-doc-codeph\">CREATE TABLE ... CACHED IN '<span class=\"hue-doc-varname\">pool_name</span>'\n          AS SELECT ... WHERE ...</span>. When you are finished with generating reports from this subset of data,\n          drop the table and both the data files and the data cached in RAM are automatically deleted.\n        </p></li></ul><p>\n        See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_create_table.xml\" data-doc-anchor-id=\"create_table\">CREATE TABLE Statement</a> for the full syntax.\n      </p><p><b>Other memory considerations:</b></p><p>\n        Certain DDL operations, such as <span class=\"hue-doc-codeph\">ALTER TABLE ... SET LOCATION</span>, are blocked while the\n        underlying HDFS directories contain cached files. You must uncache the files first, before changing the\n        location, dropping the table, and so on.\n      </p><p> When data is requested to be pinned in memory, that process happens in\n        the background without blocking access to the data while the caching is\n        in progress. Loading the data from disk could take some time. Impala\n        reads each HDFS data block from memory if it has been pinned already, or\n        from disk if it has not been pinned yet.</p><p>\n        The amount of data that you can pin on each node through the HDFS caching mechanism is subject to a quota\n        that is enforced by the underlying HDFS service. Before requesting to pin an Impala table or partition in\n        memory, check that its size does not exceed this quota.\n      </p><div class=\"hue-doc-note\">        Because the HDFS cache consists of combined memory from all the DataNodes in the cluster, cached tables or\n        partitions can be bigger than the amount of HDFS cache memory on any single host.\n      </div></div></div><div id=\"hdfs_caching_etl\"><div class=\"hue-doc-title\">Loading and Removing Data with HDFS Caching Enabled</div><div><p>\n        When HDFS caching is enabled, extra processing happens in the background when you add or remove data\n        through statements such as <span class=\"hue-doc-codeph\">INSERT</span> and <span class=\"hue-doc-codeph\">DROP TABLE</span>.\n      </p><p><b>Inserting or loading data:</b></p><ul><li>\n          When Impala performs an <span class=\"hue-doc-codeph\"><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_insert.xml\" data-doc-anchor-id=\"insert\">INSERT</a></span> or\n          <span class=\"hue-doc-codeph\"><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_load_data.xml\" data-doc-anchor-id=\"load_data\">LOAD DATA</a></span> statement for a table or\n          partition that is cached, the new data files are automatically cached and Impala recognizes that fact\n          automatically.\n        </li><li>\n          If you perform an <span class=\"hue-doc-codeph\">INSERT</span> or <span class=\"hue-doc-codeph\">LOAD DATA</span> through Hive, as always, Impala\n          only recognizes the new data files after a <span class=\"hue-doc-codeph\">REFRESH <span class=\"hue-doc-varname\">table_name</span></span>\n          statement in Impala.\n        </li><li>\n          If the cache pool is entirely full, or becomes full before all the requested data can be cached, the\n          Impala DDL statement returns an error. This is to avoid situations where only some of the requested data\n          could be cached.\n        </li><li>\n          When HDFS caching is enabled for a table or partition, new data files are cached automatically when they\n          are added to the appropriate directory in HDFS, without the need for a <span class=\"hue-doc-codeph\">REFRESH</span> statement\n          in Impala. Impala automatically performs a <span class=\"hue-doc-codeph\">REFRESH</span> once the new data is loaded into the\n          HDFS cache.\n        </li></ul><p><b>Dropping tables, partitions, or cache pools:</b></p><p>\n        The HDFS caching feature interacts with the Impala\n        <span class=\"hue-doc-codeph\"><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_drop_table.xml\" data-doc-anchor-id=\"drop_table\">DROP TABLE</a></span> and\n        <span class=\"hue-doc-codeph\"><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_alter_table.xml\" data-doc-anchor-id=\"alter_table\">ALTER TABLE ... DROP PARTITION</a></span>\n        statements as follows:\n      </p><ul><li>\n          When you issue a <span class=\"hue-doc-codeph\">DROP TABLE</span> for a table that is entirely cached, or has some partitions\n          cached, the <span class=\"hue-doc-codeph\">DROP TABLE</span> succeeds and all the cache directives Impala submitted for that\n          table are removed from the HDFS cache system.\n        </li><li>\n          The same applies to <span class=\"hue-doc-codeph\">ALTER TABLE ... DROP PARTITION</span>. The operation succeeds and any cache\n          directives are removed.\n        </li><li>\n          As always, the underlying data files are removed if the dropped table is an internal table, or the\n          dropped partition is in its default location underneath an internal table. The data files are left alone\n          if the dropped table is an external table, or if the dropped partition is in a non-default location.\n        </li><li>\n          If you designated the data files as cached through the <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> command, and\n          the data files are left behind as described in the previous item, the data files remain cached. Impala\n          only removes the cache directives submitted by Impala through the <span class=\"hue-doc-codeph\">CREATE TABLE</span> or\n          <span class=\"hue-doc-codeph\">ALTER TABLE</span> statements. It is OK to have multiple redundant cache directives pertaining\n          to the same files; the directives all have unique IDs and owners so that the system can tell them apart.\n        </li><li>\n          If you drop an HDFS cache pool through the <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> command, all the Impala\n          data files are preserved, just no longer cached. After a subsequent <span class=\"hue-doc-codeph\">REFRESH</span>,\n          <span class=\"hue-doc-codeph\">SHOW TABLE STATS</span> reports 0 bytes cached for each associated Impala table or partition.\n        </li></ul><p><b>Relocating a table or partition:</b></p><p>\n        The HDFS caching feature interacts with the Impala\n        <span class=\"hue-doc-codeph\"><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_alter_table.xml\" data-doc-anchor-id=\"alter_table\">ALTER TABLE ... SET LOCATION</a></span>\n        statement as follows:\n      </p><ul><li>\n          If you have designated a table or partition as cached through the <span class=\"hue-doc-codeph\">CREATE TABLE</span> or\n          <span class=\"hue-doc-codeph\">ALTER TABLE</span> statements, subsequent attempts to relocate the table or partition through\n          an <span class=\"hue-doc-codeph\">ALTER TABLE ... SET LOCATION</span> statement will fail. You must issue an <span class=\"hue-doc-codeph\">ALTER\n          TABLE ... SET UNCACHED</span> statement for the table or partition first. Otherwise, Impala would lose\n          track of some cached data files and have no way to uncache them later.\n        </li></ul></div></div><div id=\"hdfs_caching_admin\"><div class=\"hue-doc-title\">Administration for HDFS Caching with Impala</div><div><p>\n        Here are the guidelines and steps to check or change the status of HDFS caching for Impala data:\n      </p><p><b>hdfs cacheadmin command:</b></p><ul><li>\n          If you drop a cache pool with the <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> command, Impala queries against the\n          associated data files will still work, by falling back to reading the files from disk. After performing a\n          <span class=\"hue-doc-codeph\">REFRESH</span> on the table, Impala reports the number of bytes cached as 0 for all associated\n          tables and partitions.\n        </li><li>\n          You might use <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> to get a list of existing cache pools, or detailed\n          information about the pools, as follows:\n<div class=\"hue-doc-codeblock\">hdfs cacheadmin -listDirectives         # Basic info\nFound 122 entries\n  ID POOL       REPL EXPIRY  PATH\n 123 testPool      1 never   /user/hive/warehouse/tpcds.store_sales\n 124 testPool      1 never   /user/hive/warehouse/tpcds.store_sales/ss_date=1998-01-15\n 125 testPool      1 never   /user/hive/warehouse/tpcds.store_sales/ss_date=1998-02-01\n...\n\nhdfs cacheadmin -listDirectives -stats  # More details\nFound 122 entries\n  ID POOL       REPL EXPIRY  PATH                                                        BYTES_NEEDED  BYTES_CACHED  FILES_NEEDED  FILES_CACHED\n 123 testPool      1 never   /user/hive/warehouse/tpcds.store_sales                                 0             0             0             0\n 124 testPool      1 never   /user/hive/warehouse/tpcds.store_sales/ss_date=1998-01-15         143169        143169             1             1\n 125 testPool      1 never   /user/hive/warehouse/tpcds.store_sales/ss_date=1998-02-01         112447        112447             1             1\n...\n</div></li></ul><p><b>Impala SHOW statement:</b></p><ul><li>\n          For each table or partition, the <span class=\"hue-doc-codeph\">SHOW TABLE STATS</span> or <span class=\"hue-doc-codeph\">SHOW PARTITIONS</span>\n          statement displays the number of bytes currently cached by the HDFS caching feature. If there are no\n          cache directives in place for that table or partition, the result set displays <span class=\"hue-doc-codeph\">NOT\n          CACHED</span>. A value of 0, or a smaller number than the overall size of the table or partition,\n          indicates that the cache request has been submitted but the data has not been entirely loaded into memory\n          yet. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_show.xml\" data-doc-anchor-id=\"show\">SHOW Statement</a> for details.\n        </li></ul><p><b>Impala memory limits:</b></p><p>\n        The Impala HDFS caching feature interacts with the Impala memory limits as follows:\n      </p><ul><li>\n          The maximum size of each HDFS cache pool is specified externally to Impala, through the <span class=\"hue-doc-cmdname\">hdfs\n          cacheadmin</span> command.\n        </li><li>\n          All the memory used for HDFS caching is separate from the <span class=\"hue-doc-cmdname\">impalad</span> daemon address space\n          and does not count towards the limits of the <span class=\"hue-doc-codeph\">--mem_limit</span> startup option,\n          <span class=\"hue-doc-codeph\">MEM_LIMIT</span> query option, or further limits imposed through YARN resource management or\n          the Linux <span class=\"hue-doc-codeph\">cgroups</span> mechanism.\n        </li><li>\n          Because accessing HDFS cached data avoids a memory-to-memory copy operation, queries involving cached\n          data require less memory on the Impala side than the equivalent queries on uncached data. In addition to\n          any performance benefits in a single-user environment, the reduced memory helps to improve scalability\n          under high-concurrency workloads.\n        </li></ul></div></div><div id=\"hdfs_caching_performance\"><div class=\"hue-doc-title\">Performance Considerations for HDFS Caching with Impala</div><div><p>\n        In Impala 1.4.0 and higher, Impala supports efficient reads from data that is pinned in memory through HDFS\n        caching. Impala takes advantage of the HDFS API and reads the data from memory rather than from disk\n        whether the data files are pinned using Impala DDL statements, or using the command-line mechanism where\n        you specify HDFS paths.\n      </p><p>\n        When you examine the output of the <span class=\"hue-doc-cmdname\">impala-shell</span><span class=\"hue-doc-cmdname\">SUMMARY</span> command, or\n        look in the metrics report for the <span class=\"hue-doc-cmdname\">impalad</span> daemon, you see how many bytes are read from\n        the HDFS cache. For example, this excerpt from a query profile illustrates that all the data read during a\n        particular phase of the query came from the HDFS cache, because the <span class=\"hue-doc-codeph\">BytesRead</span> and\n        <span class=\"hue-doc-codeph\">BytesReadDataNodeCache</span> values are identical.\n      </p><div class=\"hue-doc-codeblock\">HDFS_SCAN_NODE (id=0):(Total: 11s114ms, non-child: 11s114ms, % non-child: 100.00%)\n        - AverageHdfsReadThreadConcurrency: 0.00\n        - AverageScannerThreadConcurrency: 32.75\n<b>        - BytesRead: 10.47 GB (11240756479)\n        - BytesReadDataNodeCache: 10.47 GB (11240756479)</b>\n        - BytesReadLocal: 10.47 GB (11240756479)\n        - BytesReadShortCircuit: 10.47 GB (11240756479)\n        - DecompressionTime: 27s572ms\n</div><p>\n        For queries involving smaller amounts of data, or in single-user workloads, you might not notice a\n        significant difference in query response time with or without HDFS caching. Even with HDFS caching turned\n        off, the data for the query might still be in the Linux OS buffer cache. The benefits become clearer as\n        data volume increases, and especially as the system processes more concurrent queries. HDFS caching\n        improves the scalability of the overall system. That is, it prevents query performance from declining when\n        the workload outstrips the capacity of the Linux OS cache.\n      </p><p id=\"hdfs_caching_encryption_caveat\">\n        Due to a limitation of HDFS, zero-copy reads are not supported with encryption. Where\n        practical, avoid HDFS caching for Impala data files in encryption zones. The queries\n        fall back to the normal read path during query execution, which might cause some\n        performance overhead.\n      </p><p><b>SELECT considerations:</b></p><p>\n        The Impala HDFS caching feature interacts with the\n        <span class=\"hue-doc-codeph\"><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_select.xml\" data-doc-anchor-id=\"select\">SELECT</a></span> statement and query performance as\n        follows:\n      </p><ul><li>\n          Impala automatically reads from memory any data that has been designated as cached and actually loaded\n          into the HDFS cache. (It could take some time after the initial request to fully populate the cache for a\n          table with large size or many partitions.) The speedup comes from two aspects: reading from RAM instead\n          of disk, and accessing the data straight from the cache area instead of copying from one RAM area to\n          another. This second aspect yields further performance improvement over the standard OS caching\n          mechanism, which still results in memory-to-memory copying of cached data.\n        </li><li>\n          For small amounts of data, the query speedup might not be noticeable in terms of wall clock time. The\n          performance might be roughly the same with HDFS caching turned on or off, due to recently used data being\n          held in the Linux OS cache. The difference is more pronounced with:\n          <ul><li>\n              Data volumes (for all queries running concurrently) that exceed the size of the Linux OS cache.\n            </li><li>\n              A busy cluster running many concurrent queries, where the reduction in memory-to-memory copying and\n              overall memory usage during queries results in greater scalability and throughput.\n            </li><li>\n              Thus, to really exercise and benchmark this feature in a development environment, you might need to\n              simulate realistic workloads and concurrent queries that match your production environment.\n            </li><li>\n              One way to simulate a heavy workload on a lightly loaded system is to flush the OS buffer cache (on\n              each DataNode) between iterations of queries against the same tables or partitions:\n<div class=\"hue-doc-codeblock\">$ sync\n$ echo 1 &gt; /proc/sys/vm/drop_caches\n</div></li></ul></li><li>\n          Impala queries take advantage of HDFS cached data regardless of whether the cache directive was issued by\n          Impala or externally through the <span class=\"hue-doc-cmdname\">hdfs cacheadmin</span> command, for example for an external\n          table where the cached data files might be accessed by several different Hadoop components.\n        </li><li>\n          If your query returns a large result set, the time reported for the query could be dominated by the time\n          needed to print the results on the screen. To measure the time for the underlying query processing, query\n          the <span class=\"hue-doc-codeph\">COUNT()</span> of the big result set, which does all the same processing but only prints a\n          single line to the screen.\n        </li></ul></div></div></div></div>","title":"Using HDFS Caching with Impala (Impala 2.1 or higher only)"}