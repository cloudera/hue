{"body":"<div><div id=\"rcfile\"><div class=\"hue-doc-title\">Using the RCFile File Format with Impala Tables</div><div><p>\n      Impala supports using RCFile data files.\n    </p><table><div class=\"hue-doc-title\">RCFile Format Support in Impala</div><thead><tr><td>\n              File Type\n            </td><td>\n              Format\n            </td><td>\n              Compression Codecs\n            </td><td>\n              Impala Can CREATE?\n            </td><td>\n              Impala Can INSERT?\n            </td></tr></thead><tbody><tr id=\"rcfile_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_rcfile.xml\" data-doc-anchor-id=\"rcfile\">RCFile</a></td><td>\n              Structured\n            </td><td>\n              Snappy, gzip, deflate, bzip2\n            </td><td>\n              Yes.\n            </td><td>\n              No. Import data by using <span class=\"hue-doc-codeph\">LOAD DATA</span> on data files already in the\n              right format, or use <span class=\"hue-doc-codeph\">INSERT</span> in Hive followed by <span class=\"hue-doc-codeph\">REFRESH\n              <span class=\"hue-doc-varname\">table_name</span></span> in Impala.\n            </td></tr></tbody></table><p/></div><div id=\"rcfile_create\"><div class=\"hue-doc-title\">Creating RCFile Tables and Loading Data</div><div><p>\n        If you do not have an existing data file to use, begin by creating one in the appropriate format.\n      </p><p><b>To create an RCFile table:</b></p><p>\n        In the <span class=\"hue-doc-codeph\">impala-shell</span> interpreter, issue a command similar to:\n      </p><div class=\"hue-doc-codeblock\">create table rcfile_table (<span class=\"hue-doc-varname\">column_specs</span>) stored as rcfile;</div><p>\n        Because Impala can query some kinds of tables that it cannot currently write to, after creating tables of\n        certain file formats, you might use the Hive shell to load the data. See\n        <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_file_formats.xml\" data-doc-anchor-id=\"file_formats\">How Impala Works with Hadoop File Formats</a> for details. After loading data into a table through\n        Hive or other mechanism outside of Impala, issue a <span class=\"hue-doc-codeph\">REFRESH <span class=\"hue-doc-varname\">table_name</span></span>\n        statement the next time you connect to the Impala node, before querying the table, to make Impala recognize\n        the new data.\n      </p><div class=\"hue-doc-note\">        See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_known_issues.xml\" data-doc-anchor-id=\"known_issues\"><span class=\"hue-doc-ph\">Known Issues and Workarounds in Impala</span><span class=\"hue-doc-ph\">Apache Impala Known Issues</span></a> for potential compatibility issues with\n        RCFile tables created in Hive 0.12, due to a change in the default RCFile SerDe for Hive.\n      </div><p>\n        For example, here is how you might create some RCFile tables in Impala (by specifying the columns\n        explicitly, or cloning the structure of another table), load data through Hive, and query them through\n        Impala:\n      </p><div class=\"hue-doc-codeblock\">$ impala-shell -i localhost\n[localhost:21000] &gt; create table rcfile_table (x int) stored as rcfile;\n[localhost:21000] &gt; create table rcfile_clone like some_other_table stored as rcfile;\n[localhost:21000] &gt; quit;\n\n$ hive\nhive&gt; insert into table rcfile_table select x from some_other_table;\n3 Rows loaded to rcfile_table\nTime taken: 19.015 seconds\nhive&gt; quit;\n\n$ impala-shell -i localhost\n[localhost:21000] &gt; select * from rcfile_table;\nReturned 0 row(s) in 0.23s\n[localhost:21000] &gt; -- Make Impala recognize the data loaded through Hive;\n[localhost:21000] &gt; refresh rcfile_table;\n[localhost:21000] &gt; select * from rcfile_table;\n+---+\n| x |\n+---+\n| 1 |\n| 2 |\n| 3 |\n+---+\nReturned 3 row(s) in 0.23s</div><p id=\"complex_types_unsupported_filetype\"><b>Complex type considerations:</b> Although you can create tables in this file format\n        using the complex types (<span class=\"hue-doc-codeph\">ARRAY</span>, <span class=\"hue-doc-codeph\">STRUCT</span>, and\n        <span class=\"hue-doc-codeph\">MAP</span>) available in Impala 2.3 and higher,\n        currently, Impala can query these types only in Parquet tables. <span class=\"hue-doc-ph\">\n        The one exception to the preceding rule is <span class=\"hue-doc-codeph\">COUNT(*)</span> queries on RCFile\n        tables that include complex types. Such queries are allowed in\n        Impala 2.6 and higher. </span></p></div></div><div id=\"rcfile_compression\"><div class=\"hue-doc-title\">Enabling Compression for RCFile Tables</div><div><p>\n        You may want to enable compression on existing tables. Enabling compression provides performance gains in\n        most cases and is supported for RCFile tables. For example, to enable Snappy compression, you would specify\n        the following additional settings when loading data through the Hive shell:\n      </p><div class=\"hue-doc-codeblock\">hive&gt; SET hive.exec.compress.output=true;\nhive&gt; SET mapred.max.split.size=256000000;\nhive&gt; SET mapred.output.compression.type=BLOCK;\nhive&gt; SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nhive&gt; INSERT OVERWRITE TABLE <span class=\"hue-doc-varname\">new_table</span> SELECT * FROM <span class=\"hue-doc-varname\">old_table</span>;</div><p>\n        If you are converting partitioned tables, you must complete additional steps. In such a case, specify\n        additional settings similar to the following:\n      </p><div class=\"hue-doc-codeblock\">hive&gt; CREATE TABLE <span class=\"hue-doc-varname\">new_table</span> (<span class=\"hue-doc-varname\">your_cols</span>) PARTITIONED BY (<span class=\"hue-doc-varname\">partition_cols</span>) STORED AS <span class=\"hue-doc-varname\">new_format</span>;\nhive&gt; SET hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; SET hive.exec.dynamic.partition=true;\nhive&gt; INSERT OVERWRITE TABLE <span class=\"hue-doc-varname\">new_table</span> PARTITION(<span class=\"hue-doc-varname\">comma_separated_partition_cols</span>) SELECT * FROM <span class=\"hue-doc-varname\">old_table</span>;</div><p>\n        Remember that Hive does not require that you specify a source format for it. Consider the case of\n        converting a table with two partition columns called <span class=\"hue-doc-codeph\">year</span> and <span class=\"hue-doc-codeph\">month</span> to a\n        Snappy compressed RCFile. Combining the components outlined previously to complete this table conversion,\n        you would specify settings similar to the following:\n      </p><div class=\"hue-doc-codeblock\">hive&gt; CREATE TABLE tbl_rc (int_col INT, string_col STRING) STORED AS RCFILE;\nhive&gt; SET hive.exec.compress.output=true;\nhive&gt; SET mapred.max.split.size=256000000;\nhive&gt; SET mapred.output.compression.type=BLOCK;\nhive&gt; SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nhive&gt; SET hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; SET hive.exec.dynamic.partition=true;\nhive&gt; INSERT OVERWRITE TABLE tbl_rc SELECT * FROM tbl;</div><p>\n        To complete a similar process for a table that includes partitions, you would specify settings similar to\n        the following:\n      </p><div class=\"hue-doc-codeblock\">hive&gt; CREATE TABLE tbl_rc (int_col INT, string_col STRING) PARTITIONED BY (year INT) STORED AS RCFILE;\nhive&gt; SET hive.exec.compress.output=true;\nhive&gt; SET mapred.max.split.size=256000000;\nhive&gt; SET mapred.output.compression.type=BLOCK;\nhive&gt; SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;\nhive&gt; SET hive.exec.dynamic.partition.mode=nonstrict;\nhive&gt; SET hive.exec.dynamic.partition=true;\nhive&gt; INSERT OVERWRITE TABLE tbl_rc PARTITION(year) SELECT * FROM tbl;</div><div class=\"hue-doc-note\"><p>\n          The compression type is specified in the following command:\n        </p><div class=\"hue-doc-codeblock\">SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;</div><p>\n          You could elect to specify alternative codecs such as <span class=\"hue-doc-codeph\">GzipCodec</span> here.\n        </p></div></div></div><div id=\"rcfile_performance\"><div class=\"hue-doc-title\">Query Performance for Impala RCFile Tables</div><div><p>\n        In general, expect query performance with RCFile tables to be\n        faster than with tables using text data, but slower than with\n        Parquet tables. See <a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet\">Using the Parquet File Format with Impala Tables</a>\n        for information about using the Parquet file format for\n        high-performance analytic queries.\n      </p><p id=\"s3_block_splitting\">\n        In Impala 2.6 and higher, Impala queries are optimized for files\n        stored in Amazon S3. For Impala tables that use the file formats Parquet, ORC, RCFile,\n        SequenceFile, Avro, and uncompressed text, the setting\n        <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> in the <span class=\"hue-doc-filepath\">core-site.xml</span>\n        configuration file determines how Impala divides the I/O work of reading the data files.\n        This configuration setting is specified in bytes. By default, this value is 33554432 (32\n        MB), meaning that Impala parallelizes S3 read operations on the files as if they were\n        made up of 32 MB blocks. For example, if your S3 queries primarily access Parquet files\n        written by MapReduce or Hive, increase <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 134217728\n        (128 MB) to match the row group size of those files. If most S3 queries involve Parquet\n        files written by Impala, increase <span class=\"hue-doc-codeph\">fs.s3a.block.size</span> to 268435456 (256\n        MB) to match the row group size produced by Impala.\n      </p></div></div><div style=\"display:none;\" id=\"rcfile_data_types\"><div class=\"hue-doc-title\">Data Type Considerations for RCFile Tables</div><div><p/></div></div></div></div>","title":"Using the RCFile File Format with Impala Tables"}