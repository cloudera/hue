{"body":"<div><div id=\"file_formats\"><div class=\"hue-doc-title\">How Impala Works with Hadoop File Formats</div><div><p>\n      Impala supports several familiar file formats used in Apache Hadoop. Impala can load and\n      query data files produced by other Hadoop components such as Spark, and data files\n      produced by Impala can be used by other components also. The following sections discuss\n      the procedures, limitations, and performance considerations for using each file format\n      with Impala.\n    </p><p>\n      The file format used for an Impala table has significant performance consequences. Some\n      file formats include compression support that affects the size of data on the disk and,\n      consequently, the amount of I/O and CPU resources required to deserialize data. The\n      amounts of I/O and CPU resources required can be a limiting factor in query performance\n      since querying often begins with moving and decompressing data. To reduce the potential\n      impact of this part of the process, data is often compressed. By compressing data, a\n      smaller total number of bytes are transferred from disk to memory. This reduces the amount\n      of time taken to transfer the data, but a tradeoff occurs when the CPU decompresses the\n      content.\n    </p><p>\n      For the file formats that Impala cannot write to, create the table from within Impala\n      whenever possible and insert data using another component such as Hive or Spark. See the\n      table below for specific file formats.\n    </p><p>\n      The following table lists the file formats that Impala supports.\n    </p><table><thead><tr><td>\n              File Type\n            </td><td>\n              Format\n            </td><td>\n              Compression Codecs\n            </td><td>\n              Impala Can CREATE?\n            </td><td>\n              Impala Can INSERT?\n            </td></tr></thead><tbody><tr id=\"parquet_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_parquet.xml\" data-doc-anchor-id=\"parquet\">Parquet</a></td><td>\n              Structured\n            </td><td>Snappy, gzip, zstd, lz4; currently Snappy by default </td><td>\n              Yes.\n            </td><td>\n              Yes: <span class=\"hue-doc-codeph\">CREATE TABLE</span>, <span class=\"hue-doc-codeph\">INSERT</span>, <span class=\"hue-doc-codeph\">LOAD\n              DATA</span>, and query.\n            </td></tr><tr id=\"orc_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_orc.xml\" data-doc-anchor-id=\"orc\">ORC</a></td><td>\n              Structured\n            </td><td>\n              gzip, Snappy, LZO, LZ4; currently gzip by default\n            </td><td> Yes, in Impala 2.12.0 and higher. <p>By default, ORC reads are enabled in Impala\n              3.4.0 and higher. </p></td><td>\n              No. Import data by using <span class=\"hue-doc-codeph\">LOAD DATA</span> on data files already in the\n              right format, or use <span class=\"hue-doc-codeph\">INSERT</span> in Hive followed by <span class=\"hue-doc-codeph\">REFRESH\n              <span class=\"hue-doc-varname\">table_name</span></span> in Impala.\n            </td></tr><tr id=\"txtfile_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_txtfile.xml\" data-doc-anchor-id=\"txtfile\">Text</a></td><td>\n              Unstructured\n            </td><td>bzip2, deflate, gzip, LZO, Snappy, zstd</td><td>\n              Yes. For <span class=\"hue-doc-codeph\">CREATE TABLE</span> with no <span class=\"hue-doc-codeph\">STORED AS</span> clause,\n              the default file format is uncompressed text, with values separated by ASCII\n              <span class=\"hue-doc-codeph\">0x01</span> characters (typically represented as Ctrl-A).\n            </td><td> Yes if uncompressed.<p>No if compressed.</p><p>If LZO\n                compression is used, you must create the table and load data in\n                Hive.</p><p>If other kinds of compression are used, you must\n                load data through <span class=\"hue-doc-codeph\">LOAD DATA</span>, Hive, or manually\n                in HDFS. </p></td></tr><tr id=\"avro_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_avro.xml\" data-doc-anchor-id=\"avro\">Avro</a></td><td>\n              Structured\n            </td><td>\n              Snappy, gzip, deflate\n            </td><td>\n              Yes, in Impala 1.4.0 and higher. In lower versions, create the table using Hive.\n            </td><td>\n              No. Import data by using <span class=\"hue-doc-codeph\">LOAD DATA</span> on data files already in the\n              right format, or use <span class=\"hue-doc-codeph\">INSERT</span> in Hive followed by <span class=\"hue-doc-codeph\">REFRESH\n              <span class=\"hue-doc-varname\">table_name</span></span> in Impala.\n            </td></tr><tr id=\"hudi_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_hudi.xml\" data-doc-anchor-id=\"hudi\">Hudi</a></td><td>Structured</td><td>Snappy, gzip, zstd, lz4; currently Snappy by default </td><td>Yes, support for Read Optimized Queries is experimental.</td><td>No. Create an external table in Impala. Set the table location to the Hudi table\n              directory. Alternatively, create the Hudi table in Hive. </td></tr><tr id=\"rcfile_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_rcfile.xml\" data-doc-anchor-id=\"rcfile\">RCFile</a></td><td>\n              Structured\n            </td><td>\n              Snappy, gzip, deflate, bzip2\n            </td><td>\n              Yes.\n            </td><td>\n              No. Import data by using <span class=\"hue-doc-codeph\">LOAD DATA</span> on data files already in the\n              right format, or use <span class=\"hue-doc-codeph\">INSERT</span> in Hive followed by <span class=\"hue-doc-codeph\">REFRESH\n              <span class=\"hue-doc-varname\">table_name</span></span> in Impala.\n            </td></tr><tr id=\"sequencefile_support\"><td><a class=\"hue-doc-internal-link\" href=\"javascript:void(0);\" data-doc-ref=\"topics/impala_seqfile.xml\" data-doc-anchor-id=\"seqfile\">SequenceFile</a></td><td>\n              Structured\n            </td><td>\n              Snappy, gzip, deflate, bzip2\n            </td><td>\n              Yes.\n            </td><td>\n              No. Import data by using <span class=\"hue-doc-codeph\">LOAD DATA</span> on data files already in the\n              right format, or use <span class=\"hue-doc-codeph\">INSERT</span> in Hive followed by <span class=\"hue-doc-codeph\">REFRESH\n              <span class=\"hue-doc-varname\">table_name</span></span> in Impala.\n            </td></tr></tbody></table><p>\n      Impala supports the following compression codecs:\n    </p><dl><dt>\n          Snappy\n        </dt><dd><p>\n            Recommended for its effective balance between compression ratio and decompression\n            speed. Snappy compression is very fast, but gzip provides greater space savings.\n            Supported for text, RC, Sequence, and Avro files in Impala 2.0 and higher.\n          </p></dd><dt>\n          Gzip\n        </dt><dd><p>\n            Recommended when achieving the highest level of compression (and therefore greatest\n            disk-space savings) is desired. Supported for text, RC, Sequence and Avro files in\n            Impala 2.0 and higher.\n          </p></dd><dt>\n          Deflate\n        </dt><dd><p> Supported for AVRO, RC, Sequence, and text files. </p></dd><dt>\n          Bzip2\n        </dt><dd><p>\n            Supported for text, RC, and Sequence files in Impala 2.0 and higher.\n          </p></dd><dt>\n          LZO\n        </dt><dd><p>\n            For text files only. Impala can query LZO-compressed text tables, but currently\n            cannot create them or insert data into them. You need to perform these operations in\n            Hive.\n          </p></dd><dt>Zstd</dt><dd>For Parquet and text files only.</dd><dt>Lz4</dt><dd>For Parquet files only.</dd></dl></div><div id=\"file_format_choosing\"><div class=\"hue-doc-title\">Choosing the File Format for a Table</div><div><p>\n        Different file formats and compression codecs work better for different data sets.\n        Choosing the proper format for your data can yield performance improvements. Use the\n        following considerations to decide which combination of file format and compression to\n        use for a particular table:\n      </p><ul><li>\n          If you are working with existing files that are already in a supported file format,\n          use the same format for the Impala table if performance is acceptable. If the original\n          format does not yield acceptable query performance or resource usage, consider\n          creating a new Impala table with different file format or compression characteristics,\n          and doing a one-time conversion by rewriting the data to the new table.\n        </li><li>\n          Text files are convenient to produce through many different tools, and are\n          human-readable for ease of verification and debugging. Those characteristics are why\n          text is the default format for an Impala <span class=\"hue-doc-codeph\">CREATE TABLE</span> statement.\n          However, when performance and resource usage are the primary considerations, use one\n          of the structured file formats that include metadata and built-in compression.\n          <p>\n            A typical workflow might involve bringing data into an Impala table by copying CSV\n            or TSV files into the appropriate data directory, and then using the <span class=\"hue-doc-codeph\">INSERT\n            ... SELECT</span> syntax to rewrite the data into a table using a different, more\n            compact file format.\n          </p></li></ul></div></div></div></div>","title":"How Impala Works with Hadoop File Formats"}