# Lightweight Hue configuration file
# ==================================

[desktop]

  # Set this to a random string, the longer the better.
  secret_key=kasdlfjknasdfl3hbaksk3bwkasdfkasdfba23asdf

  # Webserver listens on this address and port
  http_host=0.0.0.0
  http_port=8888

  # Time zone name
  time_zone=America/Los_Angeles

  # Enable or disable debug mode.
  django_debug_mode=false

  # Enable or disable backtrace for server error
  http_500_debug_mode=false

  app_blacklist=search,hbase,security

  # Configuration options for specifying the Desktop Database. For more info,
  # see http://docs.djangoproject.com/en/1.11/ref/settings/#database-engine
  # ------------------------------------------------------------------------
  [[database]]
    # Database engine is typically one of:
    # postgresql_psycopg2, mysql, sqlite3 or oracle.
    #
    # Note that for sqlite3, 'name', below is a path to the filename. For other backends, it is the database name
    # Note for Oracle, options={"threaded":true} must be set in order to avoid crashes.
    # Note for Oracle, you can use the Oracle Service Name by setting "host=" and "port=" and then "name=<host>:<port>/<service_name>".
    # Note for MariaDB use the 'mysql' engine.

    # engine=postgresql_psycopg2
    # host=hue-postgres
    # port=5432
    # user=hue
    # password=hue
    # name=hue

    # engine=mysql
    # host=database
    # port=3306
    # user=root
    # password=secret
    # name=hue

###########################################################################
# Settings to configure the snippets available in the Notebook
###########################################################################

# [notebook]

  # One entry for each type of snippet.
  # [[interpreters]]
    # Define the name and how to connect and execute the language.
    # https://docs.gethue.com/administrator/configuration/editor/

    # Example for Docker compose
    # [[[mysql]]]
    #   name = MySQL
    #   interface=sqlalchemy
    #   ## https://docs.sqlalchemy.org/en/latest/dialects/mysql.html
    #   options='{"url": "mysql://root:secret@database:3306/hue"}'
    #   ## options='{"url": "mysql://${USER}:${PASSWORD}@localhost:3306/hue"}'

    # [[[hive]]]
    #   name=Hive
    #   interface=hiveserver2

    # [[[impala]]]
    #   name=Impala
    #   interface=hiveserver2

    # [[[sparksql]]]
    #   name = Spark Sql
    #   interface=sqlalchemy
    #   options='{"url": "hive://user:password@localhost:10000/database"}'

    # [[[sparksql]]]
    #   name=SparkSql
    #   interface=livy

    # [[[spark]]]
    #   name=Scala
    #   interface=livy

    # [[[pyspark]]]
    #   name=PySpark
    #   interface=livy

    # [[[r]]]
    #   name=R
    #   interface=livy

    # [[jar]]]
    #   name=Spark Submit Jar
    #   interface=livy-batch

    # [[[py]]]
    #   name=Spark Submit Python
    #   interface=livy-batch

    # [[[text]]]
    #   name=Text
    #   interface=text

    # [[[markdown]]]
    #   name=Markdown
    #   interface=text

    # [[[sqlite]]]
    #   name = SQLite
    #   interface=rdbms

    # [[[postgresql]]]
    #   name = PostgreSQL
    #   interface=rdbms

    # [[[oracle]]]
    #   name = Oracle
    #   interface=rdbms

    # [[[solr]]]
    #   name = Solr SQL
    #   interface=solr
    #   ## Name of the collection handler
    #   # options='{"collection": "default"}'

    # [[[pig]]]
    #   name=Pig
    #   interface=oozie

    # [[[java]]]
    #   name=Java
    #   interface=oozie

    # [[[spark2]]]
    #   name=Spark
    #   interface=oozie

    # [[[mapreduce]]]
    #   name=MapReduce
    #   interface=oozie

    # [[[sqoop1]]]
    #   name=Sqoop1
    #   interface=oozie

    # [[[distcp]]]
    #   name=Distcp
    #   interface=oozie

    # [[[shell]]]
    #   name=Shell
    #   interface=oozie

    # [[[presto]]]
    #   name=Presto SQL
    #   interface=presto
    #   ## Specific options for connecting to the Presto server.
    #   ## The JDBC driver presto-jdbc.jar need to be in the CLASSPATH environment variable.
    #   ## If 'user' and 'password' are omitted, they will be prompted in the UI.
    #   options='{"url": "jdbc:presto://localhost:8080/catalog/schema", "driver": "io.prestosql.jdbc.PrestoDriver", "user": "root", "password": "root"}'

    # [[[clickhouse]]]
    #   name=ClickHouse
    #   interface=jdbc
    #   ## Specific options for connecting to the ClickHouse server.
    #   ## The JDBC driver clickhouse-jdbc.jar and its related jars need to be in the CLASSPATH environment variable.
    #   options='{"url": "jdbc:clickhouse://localhost:8123", "driver": "ru.yandex.clickhouse.ClickHouseDriver", "user": "readonly", "password": ""}'


[dashboard]

  # Activate the SQL Dashboard (beta).
  has_sql_enabled=true


[hadoop]

  # Configuration for HDFS NameNode
  # ------------------------------------------------------------------------
  [[hdfs_clusters]]
    # HA support by using HttpFs

    # [[[default]]]
      # Enter the filesystem uri
      # fs_defaultfs=hdfs://localhost:8020

      # Use WebHdfs/HttpFs as the communication mechanism.
      # Domain should be the NameNode or HttpFs host.
      # Default port is 14000 for HttpFs.
      ## webhdfs_url=http://localhost:50070/webhdfs/v1

  # Configuration for YARN (MR2)
  # ------------------------------------------------------------------------
  [[yarn_clusters]]

    # [[[default]]]
      # Enter the host on which you are running the ResourceManager
      ## resourcemanager_host=localhost

      # The port where the ResourceManager IPC listens on
      ## resourcemanager_port=8032

      # URL of the ResourceManager API
      ## resourcemanager_api_url=http://localhost:8088

      # URL of the ProxyServer API
      ## proxy_api_url=http://localhost:8088

      # URL of the HistoryServer API
      ## history_server_api_url=http://localhost:19888

      # URL of the Spark History Server
      ## spark_history_server_url=http://localhost:18088


###########################################################################
# Settings to configure Beeswax with Hive
###########################################################################

[beeswax]

  # Host where HiveServer2 is running.
  # If Kerberos security is enabled, use fully-qualified domain name (FQDN).
  ## hive_server_host=localhost

  # Port where HiveServer2 Thrift server runs on.
  ## hive_server_port=10000


###########################################################################
# Settings to configure Impala
###########################################################################

[impala]
  # Host of the Impala Server (one of the Impalad)
  ## server_host=localhost

  # Port of the Impala Server
  ## server_port=21050


###########################################################################
# Settings to configure the Spark application.
###########################################################################

[spark]
  # The Livy Server URL.
  ## livy_server_url=http://localhost:8998

  # Configure Livy to start in local 'process' mode, or 'yarn' workers.
  ## livy_server_session_kind=yarn

  # Whether Livy requires client to perform Kerberos authentication.
  ## security_enabled=false

  # Host of the Sql Server
  ## sql_server_host=localhost

  # Port of the Sql Server
  ## sql_server_port=10000

  # Choose whether Hue should validate certificates received from the server.
  ## ssl_cert_ca_verify=true


###########################################################################
# Settings to configure HBase Browser
###########################################################################

[hbase]
  # Comma-separated list of HBase Thrift servers for clusters in the format of '(name|host:port)'.
  ## hbase_clusters=(Cluster|localhost:9090)


###########################################################################
# Settings to configure Solr Search
###########################################################################

[search]

  # URL of the Solr Server
  ## solr_url=http://localhost:8983/solr/


###########################################################################
# Settings to configure liboozie
###########################################################################

[liboozie]
  # The URL where the Oozie service runs on. This is required in order for
  # users to submit jobs. Empty value disables the config check.
  ## oozie_url=http://localhost:11000/oozie


###########################################################################
# Settings for the AWS lib
###########################################################################

[aws]
  [[aws_accounts]]
    # Default AWS account
    ## [[[default]]]
      # AWS credentials
      ## access_key_id=
      ## secret_access_key=
      ## security_token=

      # Execute this script to produce the AWS access key ID.
      ## access_key_id_script=/path/access_key_id.sh

      # Execute this script to produce the AWS secret access key.
      ## secret_access_key_script=/path/secret_access_key.sh

      # Allow to use either environment variables or
      # EC2 InstanceProfile to retrieve AWS credentials.
      ## allow_environment_credentials=yes

      # AWS region to use, if no region is specified, will attempt to connect to standard s3.amazonaws.com endpoint
      ## region=us-east-1

      # Endpoint overrides
      ## host=

      # Proxy address and port
      ## proxy_address=
      ## proxy_port=8080
      ## proxy_user=
      ## proxy_pass=

      # Secure connections are the default, but this can be explicitly overridden:
      ## is_secure=true


###########################################################################
# Settings for the Azure lib
###########################################################################
[azure]
  [[azure_accounts]]
    # Default Azure account
    [[[default]]]
      # Azure credentials
      ## client_id=
      # Execute this script to produce the ADLS client id.
      ## client_id_script=/path/client_id.sh
      ## client_secret=
      # Execute this script to produce the ADLS client secret.
      ## client_secret_script=/path/client_secret.sh
      ## tenant_id=
      # Execute this script to produce the ADLS tenant id.
      ## tenant_id_script=/path/tenant_id.sh

  [[adls_clusters]]
    # Default ADLS cluster
    [[[default]]]
      ## fs_defaultfs=adl://<account_name>.azuredatalakestore.net
      ## webhdfs_url=https://<account_name>.azuredatalakestore.net/webhdfs/v1


###########################################################################
# Settings to configure the ZooKeeper Lib
###########################################################################

[libzookeeper]
  # ZooKeeper ensemble. Comma separated list of Host/Port.
  # e.g. localhost:2181,localhost:2182,localhost:2183
  ## ensemble=localhost:2181


###########################################################################
# Settings to configure Kafka
###########################################################################

[kafka]

  [[kafka]]
    # Enable the Kafka integration.
    ## is_enabled=false

    # Base URL of Kafka REST API.
    ## api_url=http://localhost:8082


###########################################################################
# Settings to configure Metadata
###########################################################################

[metadata]

  [[navigator]]
    # Navigator API URL (without version suffix).
    ## api_url=http://localhost:7187/api

    # Which authentication to use: CM or external via LDAP or SAML.
    ## navmetadataserver_auth_type=CMDB

    # Username of the CM user used for authentication.
    ## navmetadataserver_cmdb_user=hue
    # CM password of the user used for authentication.
    ## navmetadataserver_cmdb_password=
    # Execute this script to produce the CM password. This will be used when the plain password is not set.
    # navmetadataserver_cmdb_password_script=
